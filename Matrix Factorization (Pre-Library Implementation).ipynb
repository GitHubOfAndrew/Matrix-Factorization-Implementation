{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f697f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit as t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e025c88",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "# Theory\n",
    "\n",
    "## Mathematical Problem\n",
    "\n",
    "We have a sparse matrix $A$, of size $m\\times n$. This sparse matrix is a table of interactions between **users** and **items**. We describe the **users** and **items** based on $k$ **latent features**. The **users** and **items** are both described by these **latent features**, and the matrix $A$ is the table of **interactions** based on these common **latent features**.\n",
    "\n",
    "Let **users** be described by an $m \\times k$ matrix $U$. Let **items** be described by an $n \\times k$ matrix $V$. We call $U$ and $V$ the user/item **embeddings**. It is our job to find $U$, $V$ such that:\n",
    "\n",
    "$$(A - U\\cdot V^{T})_{ij} < \\varepsilon \\quad \\forall i,j : A_{ij} \\neq 0$$\n",
    "\n",
    "Where $\\varepsilon$ is an arbitrary positive constant.\n",
    "\n",
    "Thus, our **goal** is to **train these embeddings**, $U$, $V$, in order to reliably predict the *unobserved interactions* between **user** and **item**.\n",
    "\n",
    "## Practical/Business Problem\n",
    "\n",
    "The practical problem here is that, given some interaction history between users and items, we can mathematically predict interactions between users and items based on their embeddings.\n",
    "\n",
    "# Implementation\n",
    "\n",
    "## Choice of Loss Function\n",
    "\n",
    "We must compute the loss here in an appropriate manner so that we may quantify the extent to which our prediction and real interactions disagree.\n",
    "\n",
    "An obvious choice is the MSE loss over all of our nonzero entries.\n",
    "\n",
    "Above, we stated that our objective is:\n",
    "\n",
    "$$\\left(A - UV^{T}\\right)_{ij} < \\varepsilon \\quad \\forall ij : A_{ij} \\neq 0$$\n",
    "\n",
    "### 1. Worst Implementation\n",
    "\n",
    "Therefore, in **this naive implementation, our loss would be**:\n",
    "\n",
    "$$L = \\left(A - UV^{T}\\right)_{ij} \\quad \\forall ij : A_{ij} \\neq 0$$\n",
    "\n",
    "However, we see here that this is only guaranteed to converge for **positive interactions** $A$ and **positive initializations** $U$ and $V$.\n",
    "\n",
    "### 2. Naive Implementation\n",
    "\n",
    "If we want to guarantee convergence while retaining the differentiability of our loss function, we should probably do this:\n",
    "\n",
    "$$L_{ij} = \\left(A - UV^{T}\\right)^{2}_{ij} \\quad \\forall ij : A_{ij} \\neq 0$$\n",
    "\n",
    "We can easily implement this and write some equations down to train this. But the issue here is that this is **prone to overfitting**. In the presence of extreme values, the neighboring entries surrounding an extreme interaction are prone to tending towards the extreme interaction rather than capturing the true nature of that interaction based on the features. \n",
    "\n",
    "### 3. Better Implementation\n",
    "\n",
    "Therefore, we should weight this in order to control this contribution!\n",
    "\n",
    "$$L_{ij} = \\lambda \\left(A - U\\cdot V^{T}\\right)^{2}_{ij} \\quad \\forall ij : A_{ij} \\neq 0$$\n",
    "\n",
    "However, there is one improvement we could make here. This is still not the most accurate loss possible because **we are assuming that the observed interactions completely determine the unobserved interactions**. In order to control for this, what do we do?\n",
    "\n",
    "### 4. Best Implementation\n",
    "\n",
    "We can **weight the unobserved entries**.\n",
    "\n",
    "$$L = \\begin{cases} \n",
    "\\lambda_{1}\\left(A - U\\cdot V^{T}\\right)^{2}_{ij} & \\forall ij : A_{ij} \\neq 0\\\\[5pt]\n",
    "\\lambda_{2}\\left(U\\cdot V^{T}\\right)^{2}_{ij} & \\forall ij : A_{ij} = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "This choice of $L$ takes contributions of both our observed interactions and unobserved interactions, while having the hyperparameters to control which contribution is more impactful.\n",
    "\n",
    "In practice, we will implement the 3rd method, and then we can get to doing the best possible implementation.\n",
    "\n",
    "## Initialize Embeddings\n",
    "\n",
    "We will initialize our user and item embeddings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8028a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our original matrix of observed interactions\n",
    "A = tf.constant([[5,0,2,0,0], [0,0,3,1,0], [0,0,0,1,5], [2,0,0,0,1], [0,0,0,0,5]], dtype=tf.float32)\n",
    "\n",
    "# our embeddings\n",
    "U, V = tf.Variable(tf.random.uniform(shape=(5,3))), tf.Variable(tf.random.uniform(shape=(5,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da90e805",
   "metadata": {},
   "source": [
    "## Implement Automatic Differentiation for our Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fda45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_1 = tf.constant(0.01)\n",
    "# with tf.GradientTape(persistent=True) as tape:\n",
    "#     # initialize our loss in the scope of the gradient tape\n",
    "#     loss_fn = tf.multiply(lambda_1, tf.pow(tf.where(A != 0, A - tf.matmul(U, tf.transpose(V)), 0.0), 2))\n",
    "\n",
    "# # backpropagate to compute gradient\n",
    "# dloss_dU = tape.gradient(loss_fn, U)\n",
    "# dloss_dV = tape.gradient(loss_fn, V)\n",
    "\n",
    "# dloss_dU.numpy(), dloss_dV.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114d5b9",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent\n",
    "\n",
    "We will take an existing gradient descent method from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c9f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-1\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79dc9b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.08122253, 0.757429  , 0.25588763],\n",
       "        [0.84419334, 0.01586115, 0.7617438 ],\n",
       "        [0.20530128, 0.848701  , 0.8023819 ],\n",
       "        [0.21759665, 0.18440318, 0.46592808],\n",
       "        [0.31103742, 0.07522655, 0.5374615 ]], dtype=float32),\n",
       " array([[0.22551465, 0.22709131, 0.7178328 ],\n",
       "        [0.68392634, 0.35441232, 0.99062216],\n",
       "        [0.82800114, 0.6106597 , 0.10556054],\n",
       "        [0.88696134, 0.5254947 , 0.3948456 ],\n",
       "        [0.05091584, 0.41112316, 0.65729904]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.numpy(), V.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f15f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0.37400696, 0.5774803 , 0.55679536, 0.57110226, 0.4837268 ],\n",
       "       [0.7407846 , 1.3375877 , 0.78908885, 1.057873  , 0.5501972 ],\n",
       "       [0.8150071 , 1.2360584 , 0.7729571 , 0.94489914, 0.88677853],\n",
       "       [0.42540607, 0.6757335 , 0.34196147, 0.47387233, 0.39314562],\n",
       "       [0.4730343 , 0.7718092 , 0.3602119 , 0.5276236 , 0.40003705]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(U, tf.transpose(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ee5bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.18121548, 0.8574208 , 0.3558831 ],\n",
       "        [0.9441845 , 0.11584917, 0.86166877],\n",
       "        [0.3052401 , 0.9486918 , 0.90237606],\n",
       "        [0.31755573, 0.28437713, 0.56591773],\n",
       "        [0.41096997, 0.1752182 , 0.6374563 ]], dtype=float32),\n",
       " array([[0.32549265, 0.32708716, 0.81782454],\n",
       "        [0.68392634, 0.35441232, 0.99062216],\n",
       "        [0.9279932 , 0.7106457 , 0.20555285],\n",
       "        [0.7873807 , 0.625151  , 0.43949082],\n",
       "        [0.15090927, 0.5111191 , 0.75729644]], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_1 = tf.constant(0.01)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # initialize our loss in the scope of the gradient tape\n",
    "    loss_fn = tf.multiply(lambda_1, tf.pow(tf.where(A != 0, A - tf.matmul(U, tf.transpose(V)), 0.0), 2))\n",
    "\n",
    "# backpropagate to compute gradient\n",
    "dloss_dU, dloss_dV = tape.gradient(loss_fn, U), tape.gradient(loss_fn, V)\n",
    "\n",
    "li_grads = [dloss_dU, dloss_dV]\n",
    "li_vars = [U, V]\n",
    "\n",
    "# now do the optimization\n",
    "optimizer.apply_gradients(zip(li_grads, li_vars))\n",
    "\n",
    "U.numpy(), V.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b0c0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0.63048553, 0.7803642 , 0.85064185, 0.8351104 , 0.73510027],\n",
       "       [1.0499117 , 1.5403992 , 1.1356429 , 1.1945513 , 0.8542376 ],\n",
       "       [1.1476436 , 1.4389035 , 1.1429304 , 1.2300017 , 1.2143242 ],\n",
       "       [0.6591996 , 0.8785821 , 0.6131069 , 0.67653155, 0.6218402 ],\n",
       "       [0.71240675, 0.974651  , 0.63692635, 0.71328384, 0.63431996]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(U, tf.transpose(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ddecf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[5., 0., 2., 0., 0.],\n",
       "       [0., 0., 3., 1., 0.],\n",
       "       [0., 0., 0., 1., 5.],\n",
       "       [2., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 5.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd636e",
   "metadata": {},
   "source": [
    "So our optimizer updated the embeddings, this was one pass so the differences are not huge so far, but it did update.\n",
    "\n",
    "## Implement Training Loop (Loss 3)\n",
    "\n",
    "We will now implement the training loop (i.e. the gradient descent for multiple epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05252006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our original matrix of observed interactions\n",
    "A = tf.constant([[5,0,2,0,0], [0,0,3,1,0], [0,0,0,1,5], [2,0,0,0,1], [0,0,0,0,5]], dtype=tf.float32)\n",
    "\n",
    "# our embeddings\n",
    "U_1, V_1 = tf.Variable(tf.random.uniform(shape=(5,3))), tf.Variable(tf.random.uniform(shape=(5,3)))\n",
    "\n",
    "lambda_1 = tf.constant(0.01)\n",
    "optimizer_1 = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935895b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[0.07933831, 0.59718215, 0.26859355],\n",
       "        [0.7451229 , 0.8870889 , 0.61328113],\n",
       "        [0.1465751 , 0.05487955, 0.45774043],\n",
       "        [0.94897795, 0.05587411, 0.11407959],\n",
       "        [0.3906908 , 0.8970146 , 0.7635932 ]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[0.47210371, 0.05594623, 0.10557187],\n",
       "        [0.82508147, 0.70971763, 0.92652845],\n",
       "        [0.507422  , 0.2751348 , 0.01749194],\n",
       "        [0.6188357 , 0.08820915, 0.30138838],\n",
       "        [0.5684279 , 0.8174298 , 0.4409219 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_1, V_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d146c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to implement the optimization of loss 3\n",
    "\n",
    "def train_loss3(U, V, A, epochs, optimizer, lambda_1):\n",
    "    total_time = 0\n",
    "    # run training loop for multiple epochs\n",
    "    for epoch in range(epochs):\n",
    "        start = t.default_timer()\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # initialize our loss in the scope of the gradient tape\n",
    "            loss_fn = tf.multiply(lambda_1, tf.pow(tf.where(A != 0, A - tf.matmul(U, tf.transpose(V)), 0.0), 2))\n",
    "\n",
    "        # backpropagate to compute gradient\n",
    "        dloss_dU, dloss_dV = tape.gradient(loss_fn, U), tape.gradient(loss_fn, V)\n",
    "\n",
    "        # get arrays of variables and gradients\n",
    "        li_grads = [dloss_dU, dloss_dV]\n",
    "        li_vars = [U, V]\n",
    "\n",
    "        # now do the optimization\n",
    "        optimizer.apply_gradients(zip(li_grads, li_vars))\n",
    "        \n",
    "        # track runtime and total loss\n",
    "        end = t.default_timer()\n",
    "        \n",
    "        total_time += (end - start)\n",
    "        # print the updates for every epoch\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Loss {tf.reduce_sum(loss_fn)} | Total Runtime {total_time} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f4bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.6647934913635254 | Total Runtime 0.3557698000000009 sec.\n",
      "Loss 0.6099028587341309 | Total Runtime 0.6507769000000039 sec.\n",
      "Loss 0.552852213382721 | Total Runtime 0.9248403999999972 sec.\n",
      "Loss 0.4948466122150421 | Total Runtime 1.2308782999999952 sec.\n",
      "Loss 0.43711239099502563 | Total Runtime 1.5156865999999933 sec.\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "\n",
    "train_loss3(U_1, V_1, A, epochs, optimizer_1, lambda_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c1d21",
   "metadata": {},
   "source": [
    "## Implement Training Loops (Loss 4)\n",
    "\n",
    "Loss 4 was the method 4 in which we added an extra term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f0fd4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our original matrix of observed interactions\n",
    "A = tf.constant([[5,0,2,0,0], [0,0,3,1,0], [0,0,0,1,5], [2,0,0,0,1], [0,0,0,0,5]], dtype=tf.float32)\n",
    "\n",
    "# our embeddings\n",
    "U_2, V_2 = tf.Variable(tf.random.uniform(shape=(5,3))), tf.Variable(tf.random.uniform(shape=(5,3)))\n",
    "\n",
    "lambda_1, lambda_2 = tf.constant(0.01), tf.constant(0.001) \n",
    "optimizer_1 = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bb8981b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[0.45406008, 0.6125567 , 0.6828482 ],\n",
       "        [0.3958645 , 0.27806878, 0.64831066],\n",
       "        [0.9756948 , 0.8801267 , 0.14891171],\n",
       "        [0.7785053 , 0.771783  , 0.1295476 ],\n",
       "        [0.48571408, 0.40935826, 0.9892758 ]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[0.5707859 , 0.11755216, 0.2963915 ],\n",
       "        [0.22033596, 0.09281969, 0.48017776],\n",
       "        [0.25119758, 0.78879523, 0.3585558 ],\n",
       "        [0.91686547, 0.99404526, 0.519649  ],\n",
       "        [0.69685435, 0.7268249 , 0.76492393]], dtype=float32)>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_2, V_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49f4abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to implement the optimization of loss 3\n",
    "\n",
    "def train_loss4(U, V, A, epochs, optimizer, lambda_1, lambda_2):\n",
    "    total_time = 0\n",
    "    # run training loop for multiple epochs\n",
    "    for epoch in range(epochs):\n",
    "        start = t.default_timer()\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # initialize our loss in the scope of the gradient tape\n",
    "            loss_fn = tf.where(A != 0, tf.multiply(lambda_1, tf.pow(A - tf.matmul(U, tf.transpose(V)), 2)), tf.multiply(lambda_2, tf.pow(-tf.matmul(U, tf.transpose(V)), 2)))\n",
    "\n",
    "        # backpropagate to compute gradient\n",
    "        dloss_dU, dloss_dV = tape.gradient(loss_fn, U), tape.gradient(loss_fn, V)\n",
    "\n",
    "        # get arrays of variables and gradients\n",
    "        li_grads = [dloss_dU, dloss_dV]\n",
    "        li_vars = [U, V]\n",
    "\n",
    "        # now do the optimization\n",
    "        optimizer.apply_gradients(zip(li_grads, li_vars))\n",
    "        \n",
    "        # track runtime and total loss\n",
    "        end = t.default_timer()\n",
    "        \n",
    "        total_time += (end - start)\n",
    "        # print the updates for every epoch\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Loss {tf.reduce_sum(loss_fn)} | Total Runtime {total_time} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f88e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.5190655589103699 | Total Runtime 0.4495614000000039 sec.\n",
      "Loss 0.46565189957618713 | Total Runtime 0.8992254000000042 sec.\n",
      "Loss 0.41350579261779785 | Total Runtime 1.3589951000000067 sec.\n",
      "Loss 0.3640349507331848 | Total Runtime 1.7045038000000172 sec.\n",
      "Loss 0.3185979127883911 | Total Runtime 2.305753300000033 sec.\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "\n",
    "train_loss4(U_2, V_2, A, epochs, optimizer_1, lambda_1, lambda_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5361ae05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4111626 ,  0.16100453,  1.7719631 ,  1.3313775 ,  2.453659  ],\n",
       "       [ 1.205685  ,  0.18614072,  1.3572495 ,  1.0194348 ,  2.0053127 ],\n",
       "       [ 1.629098  , -0.0031546 ,  1.9628751 ,  1.731515  ,  2.6354938 ],\n",
       "       [ 1.0395435 , -0.0117838 ,  1.0327182 ,  1.0324891 ,  1.4998994 ],\n",
       "       [ 1.5107505 ,  0.26055306,  1.7363986 ,  1.2583627 ,  2.5601194 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(U_2, tf.transpose(V_2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bd711e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 2., 0., 0.],\n",
       "       [0., 0., 3., 1., 0.],\n",
       "       [0., 0., 0., 1., 5.],\n",
       "       [2., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b46f6",
   "metadata": {},
   "source": [
    "## Method 3 vs Method 4\n",
    "\n",
    "Comparing method 3 and method 4, we see that method 4 is slightly more powerful at the cost of slightly longer training time. Maybe we can uncover better schemes to make our loss more accurate, but we must ensure that it does not overfit or improperly represent our observed interactions. This is essentially what it boils down to:\n",
    "\n",
    "Assume $\\mathrm{Observed} + \\mathrm{Unobserved} = \\mathrm{All\\;Interactions}$ (where $+$ here means disjoint union of sets).\n",
    "\n",
    "Here are the two losses we used:\n",
    "\n",
    "$$L_{3} = \\sum_{(i,j) \\in \\mathrm{Observed}} \\lambda_{1}\\left(A - U\\cdot V^{T}\\right)_{ij}$$\n",
    "\n",
    "$$L_{4} = \\sum_{(i,j) \\in \\mathrm{Observed}} \\lambda_{1}\\left(A - U\\cdot V^{T}\\right)_{ij} + \\sum_{(i,j) \\in \\mathrm{Unobserved}} \\lambda_{2}\\left(-U \\cdot V^{T}\\right)_{ij}$$\n",
    "\n",
    "We see that $L_{3}$ only trains the **observed interactions** (i.e. the entries corresponding to the i,j such that $A_{ij} \\neq 0$). This is fine, however, in cases where certain observations are extreme (there is a lot of interactions between certain item, user pairs), the nearby interactions (entries) will likely **overfit** to those values.\n",
    "\n",
    "To counteract this, we train the **unobserved interactions** as well (i.e. the entries corresponding to the i,j such that $A_{ij} = 0$). The purpose of doing this is to counteract the **overfitting** that comes with only training the **observed interactions**. So in some sense, the extra term in $L_{4}$ (the term with the $\\lambda_{2}$ on it) is a **regularization** term.\n",
    "\n",
    "**Note:** If we set $\\lambda_{2} = 0$ for $L_{4}$, then we obtain $L_{3}$, so it is better to implement $L_{4}$.\n",
    "\n",
    "## Create a Class\n",
    "\n",
    "We will now use object-oriented programming to consolidate the basic aspects of this model. This will be the basis of the library we are building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f932ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    \n",
    "    \"\"\"Class for a Matrix Factorization Model. A hybrid (content-based and collaborative filtering) recommender model \n",
    "    that takes user embeddings and item embeddings to predict unobserved interactions in a given interaction table\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, shape):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        - num_features: a python int, number of latent features for the user and item embeddings\n",
    "        - shape: a python tuple of the form (n,m) where (n,m) is the shape of the interaction table we are approximating\n",
    "        \n",
    "        Purpose:\n",
    "        Initializes the following instance attributes:\n",
    "        - user embeddings: U ---> of shape (n, num_features)\n",
    "        - item embeddings: V ---> of shape (m, num_features)\n",
    "        \"\"\"\n",
    "        n, m = shape\n",
    "        user_shape = (n, num_features)\n",
    "        item_shape = (m, num_features)\n",
    "        \n",
    "        self.U = tf.Variable(tf.random.uniform(shape=user_shape), trainable=True, dtype=tf.float32)\n",
    "        self.V = tf.Variable(tf.random.uniform(shape=item_shape), trainable=True, dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def loss(self, A, U, V, lambda_1, lambda_2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        - None\n",
    "        \n",
    "        Purpose:\n",
    "        Outputs the loss for both observed and unobserved interactions. In particular, it creates a loss graph for computation\n",
    "        in tensorflow's autograd\n",
    "        \n",
    "        2022-06-03 - The only loss available is the MSE loss, this will be changed as necessary as we discover more losses to use\n",
    "        \"\"\"\n",
    "        \n",
    "        # mean squared error loss with regularization\n",
    "        return tf.where(A != 0, tf.multiply(lambda_1, tf.pow(A - tf.matmul(U, tf.transpose(V)), 2)), tf.multiply(lambda_2, tf.pow(-tf.matmul(U, tf.transpose(V)), 2)))\n",
    "        \n",
    "    \n",
    "    def fit(self, A, epochs, optimizer, lambda_1=0.01, lambda_2=0.001, verbose=1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        - A: an array with 2 axes, an interaction table between users and items\n",
    "        - epochs: a python int, the number of times the model will train\n",
    "        - optimizer: a tf.keras.optimizers object, the optimization algorithm used in the minimization of the loss\n",
    "        - lambda_1: a python float, the hyperparamater that weights the contribution of observed entries in the training\n",
    "        - lambda_2: a python float, the hyperparameter that weights the contribution of unobserved entries in the training\n",
    "        - verbose: a python int, \n",
    "        --- 0: no information about training process printed\n",
    "        --- 1: epoch number, loss, cumulative training runtime printed\n",
    "        --- 2: epoch number printed\n",
    "        \n",
    "        Purpose:\n",
    "        Runs a training loop for a specified loss function using an optimization algorithm of choice.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get history of losses and training parameters\n",
    "        train_history, li_loss = {}, []\n",
    "        \n",
    "        # recast hyperparameters as tf.constant objects\n",
    "        lambda_1, lambda_2 = tf.constant(lambda_1), tf.constant(lambda_2)\n",
    "        \n",
    "        total_time = 0\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # start timer\n",
    "            start=t.default_timer()\n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                \n",
    "                # initialize loss to take gradients\n",
    "                loss_fn = self.loss(A, self.U, self.V, lambda_1, lambda_2)\n",
    "                \n",
    "            # backpropagate to compute gradient\n",
    "            dloss_dU, dloss_dV = tape.gradient(loss_fn, self.U), tape.gradient(loss_fn, self.V)\n",
    "\n",
    "            # get arrays of variables and gradients\n",
    "            li_grads = [dloss_dU, dloss_dV]\n",
    "            li_vars = [self.U, self.V]\n",
    "\n",
    "            # now do the optimization\n",
    "            optimizer.apply_gradients(zip(li_grads, li_vars))\n",
    "\n",
    "            # end timer\n",
    "            end = t.default_timer()\n",
    "\n",
    "            # put in key for train history, get loss from every epoch\n",
    "            li_loss.append(tf.reduce_sum(loss_fn).numpy())\n",
    "            train_history['Loss'] = li_loss\n",
    "\n",
    "            # verbose outputs\n",
    "            if verbose == 0:\n",
    "                pass\n",
    "            \n",
    "            if verbose == 1:\n",
    "                total_time += (end - start)\n",
    "                \n",
    "                # print the updates for every epoch\n",
    "                if (epoch + 1) % 50 == 0:\n",
    "                    print(f'Epoch {epoch + 1}/{epochs} | Loss {tf.reduce_sum(loss_fn)} | Total Runtime {total_time} sec.')\n",
    "            \n",
    "            if verbose == 2:\n",
    "                total_time += (end - start)\n",
    "                \n",
    "                # print the updates for every epoch\n",
    "                if (epoch + 1) % 50 == 0:\n",
    "                    print(f'Epoch {epoch + 1}/{epochs} | Total Runtime {total_time} sec.')\n",
    "                    \n",
    "        return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dc7b9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[0.687345  , 0.89686644, 0.2590859 ],\n",
       "        [0.20819509, 0.2115885 , 0.31697786],\n",
       "        [0.7831768 , 0.91765773, 0.8220005 ],\n",
       "        [0.9278258 , 0.7236351 , 0.9515606 ],\n",
       "        [0.74774766, 0.33941114, 0.49397886]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[0.7801782 , 0.5865929 , 0.1732508 ],\n",
       "        [0.8587339 , 0.295681  , 0.7646496 ],\n",
       "        [0.17215383, 0.82947946, 0.32177687],\n",
       "        [0.14014292, 0.8995613 , 0.34337568],\n",
       "        [0.53720796, 0.33877194, 0.6331147 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the class\n",
    "\n",
    "### initialize model\n",
    "model1 = MatrixFactorization(3, (5,5))\n",
    "\n",
    "model1.U, model1.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "282054a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant([[5,0,2,0,0], [0,0,3,1,0], [0,0,0,1,5], [2,0,0,0,1], [0,0,0,0,5]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75c1c747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/250 | Loss 0.5270388722419739 | Total Runtime 0.8224340000000012 sec.\n",
      "Epoch 100/250 | Loss 0.4757670760154724 | Total Runtime 1.1190998000000114 sec.\n",
      "Epoch 150/250 | Loss 0.42503368854522705 | Total Runtime 1.5748088000000102 sec.\n",
      "Epoch 200/250 | Loss 0.37621575593948364 | Total Runtime 2.1004636000000225 sec.\n",
      "Epoch 250/250 | Loss 0.3307192921638489 | Total Runtime 2.5274242000000235 sec.\n"
     ]
    }
   ],
   "source": [
    "# test the fitting\n",
    "\n",
    "optimizer_adam=tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "history_model1 = model1.fit(A, 250, optimizer_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8c38fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': [0.5769777,\n",
       "  0.57596195,\n",
       "  0.5749459,\n",
       "  0.5739299,\n",
       "  0.5729136,\n",
       "  0.57189727,\n",
       "  0.57088083,\n",
       "  0.5698642,\n",
       "  0.56884754,\n",
       "  0.56783074,\n",
       "  0.5668139,\n",
       "  0.56579685,\n",
       "  0.5647798,\n",
       "  0.56376266,\n",
       "  0.5627454,\n",
       "  0.561728,\n",
       "  0.56071055,\n",
       "  0.559693,\n",
       "  0.5586753,\n",
       "  0.55765754,\n",
       "  0.5566397,\n",
       "  0.5556216,\n",
       "  0.5546034,\n",
       "  0.5535852,\n",
       "  0.5525667,\n",
       "  0.5515481,\n",
       "  0.5505293,\n",
       "  0.54951024,\n",
       "  0.5484911,\n",
       "  0.54747164,\n",
       "  0.54645205,\n",
       "  0.54543227,\n",
       "  0.54441226,\n",
       "  0.543392,\n",
       "  0.5423716,\n",
       "  0.54135084,\n",
       "  0.54032993,\n",
       "  0.5393088,\n",
       "  0.53828734,\n",
       "  0.53726584,\n",
       "  0.53624403,\n",
       "  0.53522205,\n",
       "  0.5341999,\n",
       "  0.53317744,\n",
       "  0.5321548,\n",
       "  0.531132,\n",
       "  0.53010905,\n",
       "  0.5290859,\n",
       "  0.52806246,\n",
       "  0.5270389,\n",
       "  0.52601516,\n",
       "  0.5249913,\n",
       "  0.52396727,\n",
       "  0.522943,\n",
       "  0.52191865,\n",
       "  0.52089405,\n",
       "  0.51986945,\n",
       "  0.51884466,\n",
       "  0.51781964,\n",
       "  0.5167947,\n",
       "  0.5157694,\n",
       "  0.51474416,\n",
       "  0.5137188,\n",
       "  0.5126933,\n",
       "  0.51166767,\n",
       "  0.51064193,\n",
       "  0.50961626,\n",
       "  0.50859034,\n",
       "  0.5075644,\n",
       "  0.50653845,\n",
       "  0.50551236,\n",
       "  0.5044863,\n",
       "  0.5034602,\n",
       "  0.5024341,\n",
       "  0.50140786,\n",
       "  0.5003816,\n",
       "  0.49935538,\n",
       "  0.4983291,\n",
       "  0.4973029,\n",
       "  0.49627674,\n",
       "  0.4952505,\n",
       "  0.49422428,\n",
       "  0.49319813,\n",
       "  0.492172,\n",
       "  0.49114597,\n",
       "  0.49012,\n",
       "  0.48909402,\n",
       "  0.48806816,\n",
       "  0.48704243,\n",
       "  0.48601672,\n",
       "  0.4849911,\n",
       "  0.48396564,\n",
       "  0.48294032,\n",
       "  0.48191503,\n",
       "  0.48089,\n",
       "  0.47986504,\n",
       "  0.4788403,\n",
       "  0.47781572,\n",
       "  0.47679126,\n",
       "  0.47576708,\n",
       "  0.47474298,\n",
       "  0.47371912,\n",
       "  0.47269553,\n",
       "  0.47167212,\n",
       "  0.470649,\n",
       "  0.46962607,\n",
       "  0.46860346,\n",
       "  0.4675811,\n",
       "  0.46655893,\n",
       "  0.46553716,\n",
       "  0.4645157,\n",
       "  0.46349448,\n",
       "  0.4624736,\n",
       "  0.46145308,\n",
       "  0.4604329,\n",
       "  0.4594131,\n",
       "  0.45839363,\n",
       "  0.4573745,\n",
       "  0.45635587,\n",
       "  0.45533758,\n",
       "  0.45431972,\n",
       "  0.45330226,\n",
       "  0.4522853,\n",
       "  0.4512687,\n",
       "  0.4502526,\n",
       "  0.44923693,\n",
       "  0.4482218,\n",
       "  0.44720715,\n",
       "  0.44619298,\n",
       "  0.44517934,\n",
       "  0.4441663,\n",
       "  0.44315374,\n",
       "  0.4421417,\n",
       "  0.44113028,\n",
       "  0.4401194,\n",
       "  0.43910915,\n",
       "  0.43809947,\n",
       "  0.43709043,\n",
       "  0.43608195,\n",
       "  0.43507415,\n",
       "  0.434067,\n",
       "  0.43306053,\n",
       "  0.4320547,\n",
       "  0.43104956,\n",
       "  0.43004513,\n",
       "  0.42904136,\n",
       "  0.42803833,\n",
       "  0.42703605,\n",
       "  0.4260345,\n",
       "  0.4250337,\n",
       "  0.4240337,\n",
       "  0.42303443,\n",
       "  0.422036,\n",
       "  0.42103833,\n",
       "  0.42004144,\n",
       "  0.41904545,\n",
       "  0.4180503,\n",
       "  0.41705596,\n",
       "  0.41606247,\n",
       "  0.41506988,\n",
       "  0.41407818,\n",
       "  0.41308737,\n",
       "  0.41209748,\n",
       "  0.41110846,\n",
       "  0.41012046,\n",
       "  0.40913337,\n",
       "  0.40814722,\n",
       "  0.407162,\n",
       "  0.40617782,\n",
       "  0.4051946,\n",
       "  0.40421236,\n",
       "  0.4032312,\n",
       "  0.402251,\n",
       "  0.40127188,\n",
       "  0.40029377,\n",
       "  0.3993168,\n",
       "  0.39834085,\n",
       "  0.39736596,\n",
       "  0.39639223,\n",
       "  0.3954195,\n",
       "  0.39444798,\n",
       "  0.39347753,\n",
       "  0.39250824,\n",
       "  0.3915401,\n",
       "  0.39057317,\n",
       "  0.38960737,\n",
       "  0.3886428,\n",
       "  0.38767937,\n",
       "  0.38671714,\n",
       "  0.38575616,\n",
       "  0.38479644,\n",
       "  0.38383788,\n",
       "  0.3828806,\n",
       "  0.38192463,\n",
       "  0.3809699,\n",
       "  0.38001648,\n",
       "  0.37906438,\n",
       "  0.3781135,\n",
       "  0.37716398,\n",
       "  0.37621576,\n",
       "  0.37526894,\n",
       "  0.37432343,\n",
       "  0.37337926,\n",
       "  0.37243646,\n",
       "  0.37149507,\n",
       "  0.37055504,\n",
       "  0.36961642,\n",
       "  0.36867926,\n",
       "  0.36774343,\n",
       "  0.36680907,\n",
       "  0.3658762,\n",
       "  0.36494476,\n",
       "  0.36401474,\n",
       "  0.36308622,\n",
       "  0.36215916,\n",
       "  0.36123365,\n",
       "  0.3603096,\n",
       "  0.35938704,\n",
       "  0.35846603,\n",
       "  0.35754657,\n",
       "  0.35662863,\n",
       "  0.35571218,\n",
       "  0.35479736,\n",
       "  0.3538841,\n",
       "  0.35297245,\n",
       "  0.35206234,\n",
       "  0.35115385,\n",
       "  0.3502469,\n",
       "  0.34934166,\n",
       "  0.34843796,\n",
       "  0.34753597,\n",
       "  0.34663558,\n",
       "  0.34573683,\n",
       "  0.34483975,\n",
       "  0.34394437,\n",
       "  0.34305063,\n",
       "  0.34215856,\n",
       "  0.34126824,\n",
       "  0.3403796,\n",
       "  0.33949268,\n",
       "  0.33860746,\n",
       "  0.33772397,\n",
       "  0.3368422,\n",
       "  0.3359622,\n",
       "  0.33508396,\n",
       "  0.33420748,\n",
       "  0.33333278,\n",
       "  0.33245984,\n",
       "  0.3315887,\n",
       "  0.3307193]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc01b6",
   "metadata": {},
   "source": [
    "## Test Scalability\n",
    "\n",
    "Let us test how scalable this training is. Let us try it for large interaction tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "559ecf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/250 | Loss 48255.1953125 | Total Runtime 4.8321564999999715 sec.\n",
      "Epoch 100/250 | Loss 43383.28515625 | Total Runtime 9.029244199999983 sec.\n",
      "Epoch 150/250 | Loss 38848.359375 | Total Runtime 13.226628699999981 sec.\n",
      "Epoch 200/250 | Loss 34834.125 | Total Runtime 17.274489399999947 sec.\n",
      "Epoch 250/250 | Loss 31454.318359375 | Total Runtime 21.382511099999927 sec.\n"
     ]
    }
   ],
   "source": [
    "model2 = MatrixFactorization(3, (1000,1000))\n",
    "\n",
    "A_2 = tf.random.uniform(shape=(1000,1000), maxval=5)\n",
    "\n",
    "# test the fitting\n",
    "\n",
    "optimizer_adam=tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "history_model2 = model2.fit(A_2, 250, optimizer_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2efb128",
   "metadata": {},
   "source": [
    "For a $1000 \\times 1000$ matrix, this actually has acceptable speed. Although we did not train it for long. Let us test the limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cf4b914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/250 | Loss 4806552.0 | Total Runtime 708.1014444999997 sec.\n",
      "Epoch 100/250 | Loss 4318902.0 | Total Runtime 1418.5567218000006 sec.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# test the fitting\u001b[39;00m\n\u001b[0;32m      7\u001b[0m optimizer_adam\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m history_model3 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_adam\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mMatrixFactorization.fit\u001b[1;34m(self, A, epochs, optimizer, lambda_1, lambda_2, verbose)\u001b[0m\n\u001b[0;32m     74\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(A, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV, lambda_1, lambda_2)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# backpropagate to compute gradient\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m dloss_dU, dloss_dV \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m, tape\u001b[38;5;241m.\u001b[39mgradient(loss_fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# get arrays of variables and gradients\u001b[39;00m\n\u001b[0;32m     80\u001b[0m li_grads \u001b[38;5;241m=\u001b[39m [dloss_dU, dloss_dV]\n",
      "File \u001b[1;32m~\\Desktop\\Random Python Scripts\\blank_env\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1081\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_gradients \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1079\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(output_gradients)]\n\u001b[1;32m-> 1081\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1090\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\Desktop\\Random Python Scripts\\blank_env\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Random Python Scripts\\blank_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1207\u001b[0m, in \u001b[0;36m_TapeGradientFunctions._wrap_backward_function.<locals>._backward_function_wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   1205\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m backward_function_inputs:\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremapped_captures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Random Python Scripts\\blank_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Desktop\\Random Python Scripts\\blank_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\Desktop\\Random Python Scripts\\blank_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model3 = MatrixFactorization(3, (10000,10000))\n",
    "\n",
    "A_3 = tf.random.uniform(shape=(10000,10000), maxval=5)\n",
    "\n",
    "# test the fitting\n",
    "\n",
    "optimizer_adam=tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "history_model3 = model3.fit(A_3, 250, optimizer_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e599e",
   "metadata": {},
   "source": [
    "### Notes on Performance\n",
    "\n",
    "I did not finish training the embeddings on the $10000 \\times 10000$ interaction matrix, for the safety of my computer (it was overheating like crazy). However, here are some observations from the limited run above:\n",
    "\n",
    "- the output here tells us that on a CPU alone, the training is manageable. We have that the computation takes about 14 seconds per epoch\n",
    "\n",
    "- furthermore, we tested this on a VERY dense matrix (this interaction table had 100 million nonzero entries!), in practice, we are far more likely to have more users than items, and many of the interactions are likely to be zero. Thus, the computation time is not as severe for these more likely cases.\n",
    "\n",
    "- the loss is encouraging, however, in 50 epochs, we saw a difference of 300000 in mse loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blank_env",
   "language": "python",
   "name": "blank_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
