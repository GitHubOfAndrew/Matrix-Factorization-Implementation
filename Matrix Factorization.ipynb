{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2e48f8",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "We will implement matrix factorization here, both manually and maybe through TensorFlow.\n",
    "\n",
    "## Theory Behind Matrix Factorization: Problem Statement\n",
    "\n",
    "We will give a brief rundown, nothing too complicated.\n",
    "\n",
    "The main **mathematical** statement that we will tackle with matrix factorization is as follows:\n",
    "\n",
    "We have some matrix $A$ (this could be a tabular dataset or anything else). Now the *caveat* is that $A$ has **missing entries** that do not know about. We want to find two matrices $U$ and $V$, so that their product:\n",
    "\n",
    "$$U \\cdot V^{T} \\approx A$$\n",
    "\n",
    "So really, in terms of an optimization problem, we want to minimize each entry of the matrix $E$:\n",
    "\n",
    "$$E = A - U\\cdot V^{T}$$\n",
    "\n",
    "It really helps to write this in index notation to represent each component so that we are not dealing with matrices directly. Let $E_{ij} := e_{ij}$, then, writing out the above expression in matrix form, we see that:\n",
    "\n",
    "$$e_{ij} = a_{ij} - \\sum_{k}u_{ik}v_{jk}^{T} = a_{ij} - \\sum_{k}u_{ik}v_{kj}$$\n",
    "\n",
    "We are now only working with real numbers! We can now adopt the traditional MSE loss for each entry.\n",
    "\n",
    "$$e_{ij}^{2} = \\left(a_{ij} - \\sum_{k}u_{ik}v_{kj}\\right)^{2}$$\n",
    "\n",
    "or to make it look simpler, we can write $\\hat{a}_{ij} = \\sum_{k}u_{ik}v_{kj}$, and we now have:\n",
    "\n",
    "$$e_{ij}^{2} = (a_{ij} - \\hat{a}_{ij})^{2}$$\n",
    "\n",
    "We are now minimizing this expression for all $(i,j) \\in rows(A), cols(A))$. **For all intents and purposes**, this is all we really need to know to continue.\n",
    "\n",
    "## Theory of Matrix Factorization: Machine Learning\n",
    "\n",
    "However, we can continue our derivation of our equations, as this will help us in our implementation. We take the gradient (derivative) of our loss above, with respect to the nonzero components of both $U$ and $V$ (**Notice that we are not training weights and biases, but just the entries of the matrices $U$ and $V$!!!**):\n",
    "\n",
    "$$\\frac{\\partial e_{ij}^{2}}{\\partial u_{ik}} = \\frac{\\partial e_{ij}^{2}}{\\partial e_{ij}}\\frac{\\partial e_{ij}}{\\partial u_{ik}} = -2e_{ij}v_{kj} $$\n",
    "\n",
    "Likewise, for the other component:\n",
    "\n",
    "$$\\frac{\\partial e_{ij}^{2}}{\\partial v_{ik}} = -2e_{ij}u_{ik}$$\n",
    "\n",
    "We now do the **optimization step** (i.e. the *Gradient Descent Step*):\n",
    "\n",
    "$$u_{ik} \\longmapsto u_{ik} - \\lambda \\frac{\\partial e_{ij}^{2}}{\\partial u_{ik}} = u_{ik} + 2\\lambda e_{ij}v_{kj}$$\n",
    "\n",
    "$$v_{kj} \\longmapsto v_{kj} - \\lambda \\frac{\\partial e_{ij}^{2}}{\\partial v_{kj}} = v_{kj} + 2\\lambda e_{ij}u_{ik}$$\n",
    "\n",
    "## Business Use of Matrix Factorization\n",
    "\n",
    "The business problem we are trying to solve is the following:\n",
    "\n",
    "We are given a table of **customers** and **products** they interacted with, however, some customers do not leave ratings on the product that they interacted with. In this case, how do we find these missing ratings?\n",
    "\n",
    "Furthermore, let us say that we track certain features for the customers and products. For example:\n",
    "\n",
    "Let $U$ denote the customers, and let $V$ denote items bought at a food mart. Let us say that $U$, $V$ contains **2 features**, sweet and savory, and let $A$ be the rating of sweet or savory items (from 1-5) by a customer that purchased them. Sweet and savory are what we refer to as **latent features**, and they relate customers to products as they are keys that merge customers and products. \n",
    "\n",
    "Now assume $A$ is the following customer-product interaction table...\n",
    "\n",
    "$$A = \\begin{pmatrix} \n",
    "4 & 0 & 3 & 2 & 2 & 1\\\\[3pt]\n",
    "0 & 5 & 3 & 4 & 5 & 1\\\\[3pt]\n",
    "5 & 5 & 0 & 0 & 3 & 5\\\\[3pt]\n",
    "3 & 3 & 4 & 3 & 2 & 0\\\\[3pt]\n",
    "2 & 4 & 2 & 4 & 5 & 1\\\\[3pt]\n",
    "0 & 0 & 5 & 3 & 3 & 4\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then we want to find $U$ and $V$ such that:\n",
    "\n",
    "- $U$ encodes the latent features (sweet or savory) of the customers, and $V$ does the same for the products.\n",
    "- $U \\cdot V^{T} \\approx A$.\n",
    "\n",
    "Thus, we are looking for $U$, $V$ such that:\n",
    "\n",
    "$$A \\approx U\\cdot V^{T}$$\n",
    "\n",
    "Now refer to the previous section for the mathematical details of our machine learning problem.\n",
    "\n",
    "\n",
    "## Manual Implementation of Matrix Factorization\n",
    "\n",
    "By \"manual implementation\", we mean *implementation in python with only numpy*. This is possible because we are using the following loss:\n",
    "\n",
    "$$\\mathrm{Loss} = \\mathrm{MSE} + \\mathrm{Regularization}$$\n",
    "\n",
    "This loss goes component-by-component:\n",
    "\n",
    "$$e_{ij}^{2} = (a_{ij} - \\hat{a}_{ij})^{2}$$\n",
    "\n",
    "The total loss is\n",
    "\n",
    "$$\\hat{L} = \\sum_{i,j} e_{ij}^{2}$$\n",
    "\n",
    "We also add a regularization term:\n",
    "\n",
    "$$L = \\hat{L} + \\sum_{k=1}^{N}\\beta\\left(u_{ik}^{2} + v_{jk}^{2}\\right)$$\n",
    "\n",
    "For $N$ latent features. This is to prevent overfitting.\n",
    "\n",
    "So we have a closed-form expression for the gradient of the loss in this case. If we wanted to implement more complicated loss functions, we will likely need to use an automatic differentiation package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ab2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f4aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARGUMENTS\n",
    "# A - ground truth matrix\n",
    "# U - product 1\n",
    "# V - product 2\n",
    "# number of features\n",
    "# epochs - number of epochs to train\n",
    "# lambda - learning rate\n",
    "# beta - regularization parameter\n",
    "\n",
    "def matrix_factorization(A, U, V, k, steps, lmbda=0.0002, beta = 0.02):\n",
    "    V = V.T\n",
    "\n",
    "    for epoch in range(steps):\n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                # we are only looking to minimize the loss on the already-filled entries\n",
    "                if A[i][j] > 0:\n",
    "                    # error\n",
    "                    eij = A[i][j] - np.dot(U[i, :], V[:, j])\n",
    "                    \n",
    "                    for K in range(k):\n",
    "                        # gradient descent step\n",
    "                        U[i][K] = U[i][K] + lmbda * (2*eij * V[K][j] - beta*U[i][K])\n",
    "                        V[K][j] = V[K][j] + lmbda * (2*eij * U[i][K] - beta*V[K][j])\n",
    "        \n",
    "        # e is our loss (i.e. our e_{ij}^{2})\n",
    "        e = 0\n",
    "        \n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                if A[i][j] > 0:\n",
    "                    # update the loss\n",
    "                    e = e + (A[i][j] - np.dot(U[i,:], V[:,j]))**2\n",
    "                    \n",
    "                    for K in range(k):\n",
    "                        # put regularization term on loss so it doesn't overfit on training data\n",
    "                        e = e + (beta/2) * (U[i][K]**2 + V[K][j]**2)\n",
    "                     \n",
    "        # give print output of training (print only every 10 epochs)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} : Total Loss {e:.3f}')\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return U, V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0113a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.9600173  0.69951205]\n",
      " [0.99986729 0.2200673 ]\n",
      " [0.36105635 0.73984099]\n",
      " [0.99645573 0.31634698]\n",
      " [0.13654458 0.38398001]]\n",
      "V is:\n",
      " [[0.32051928 0.36641475]\n",
      " [0.70965156 0.90014243]\n",
      " [0.53411544 0.24729376]\n",
      " [0.67180656 0.56172911]\n",
      " [0.54255988 0.8934476 ]]\n",
      "Epoch 1 : Total Loss 99.275\n",
      "Epoch 11 : Total Loss 96.767\n",
      "Epoch 21 : Total Loss 94.202\n",
      "Epoch 31 : Total Loss 91.584\n",
      "Epoch 41 : Total Loss 88.918\n",
      "Epoch 51 : Total Loss 86.207\n",
      "Epoch 61 : Total Loss 83.459\n",
      "Epoch 71 : Total Loss 80.680\n",
      "Epoch 81 : Total Loss 77.878\n",
      "Epoch 91 : Total Loss 75.059\n",
      "Epoch 101 : Total Loss 72.233\n",
      "Epoch 111 : Total Loss 69.408\n",
      "Epoch 121 : Total Loss 66.595\n",
      "Epoch 131 : Total Loss 63.801\n",
      "Epoch 141 : Total Loss 61.036\n",
      "Epoch 151 : Total Loss 58.311\n",
      "Epoch 161 : Total Loss 55.633\n",
      "Epoch 171 : Total Loss 53.012\n",
      "Epoch 181 : Total Loss 50.455\n",
      "Epoch 191 : Total Loss 47.970\n",
      "U_hat is:\n",
      " [[1.2427563  0.99404657]\n",
      " [1.32958304 0.4449768 ]\n",
      " [0.53087359 0.91554011]\n",
      " [1.23935835 0.47920855]\n",
      " [0.51268872 0.8134178 ]]\n",
      "V_hat is:\n",
      " [[1.02502384 1.0589032 ]\n",
      " [0.87169684 1.08305621]\n",
      " [0.97105056 0.41299391]\n",
      " [0.95914034 0.64674232]\n",
      " [0.55186549 0.90495035]]\n",
      "A_hat is:\n",
      " [[2.32645393 2.15991506 1.61731439 1.83486969 1.58539711]\n",
      " [1.83404167 1.64092823 1.47486507 1.56304206 1.13643291]\n",
      " [1.51362644 1.45434224 0.89361759 1.10130081 1.12148916]\n",
      " [1.77780732 1.59935457 1.40138984 1.49864305 1.11761906]\n",
      " [1.38684887 1.32788634 0.83378327 1.01781215 1.01903794]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [5,3,2,1,0],\n",
    "    [3,0,4,2,1],\n",
    "    [4,0,0,1,0],\n",
    "    [0,0,2,4,0],\n",
    "    [5,3,0,0,1]\n",
    "])\n",
    "\n",
    "m = A.shape[0]\n",
    "n = A.shape[1]\n",
    "user_features = 2\n",
    "\n",
    "np.random.seed(24)\n",
    "\n",
    "# generate matrices with entries of random numbers 0 - 1\n",
    "U = np.random.rand(m, user_features)\n",
    "V = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U)\n",
    "print('V is:\\n', V)\n",
    "\n",
    "# perform factorization\n",
    "\n",
    "U_hat, V_hat = matrix_factorization(A, U, V, user_features, 200)\n",
    "\n",
    "print('U_hat is:\\n', U_hat)\n",
    "print('V_hat is:\\n', V_hat)\n",
    "\n",
    "## check if the product results in something close to A\n",
    "\n",
    "print('A_hat is:\\n', np.dot(U_hat, V_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9e1ff",
   "metadata": {},
   "source": [
    "This does not look good after 200 epochs! Let's try more epochs, and if that doesn't work, let's increase the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a54ec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 45.564\n",
      "Epoch 11 : Total Loss 43.244\n",
      "Epoch 21 : Total Loss 41.013\n",
      "Epoch 31 : Total Loss 38.877\n",
      "Epoch 41 : Total Loss 36.838\n",
      "Epoch 51 : Total Loss 34.898\n",
      "Epoch 61 : Total Loss 33.060\n",
      "Epoch 71 : Total Loss 31.323\n",
      "Epoch 81 : Total Loss 29.687\n",
      "Epoch 91 : Total Loss 28.150\n",
      "Epoch 101 : Total Loss 26.711\n",
      "Epoch 111 : Total Loss 25.366\n",
      "Epoch 121 : Total Loss 24.114\n",
      "Epoch 131 : Total Loss 22.950\n",
      "Epoch 141 : Total Loss 21.870\n",
      "Epoch 151 : Total Loss 20.871\n",
      "Epoch 161 : Total Loss 19.947\n",
      "Epoch 171 : Total Loss 19.095\n",
      "Epoch 181 : Total Loss 18.310\n",
      "Epoch 191 : Total Loss 17.587\n",
      "Epoch 201 : Total Loss 16.923\n",
      "Epoch 211 : Total Loss 16.312\n",
      "Epoch 221 : Total Loss 15.751\n",
      "Epoch 231 : Total Loss 15.236\n",
      "Epoch 241 : Total Loss 14.763\n",
      "Epoch 251 : Total Loss 14.328\n",
      "Epoch 261 : Total Loss 13.929\n",
      "Epoch 271 : Total Loss 13.563\n",
      "Epoch 281 : Total Loss 13.225\n",
      "Epoch 291 : Total Loss 12.914\n",
      "Epoch 301 : Total Loss 12.628\n",
      "Epoch 311 : Total Loss 12.364\n",
      "Epoch 321 : Total Loss 12.120\n",
      "Epoch 331 : Total Loss 11.894\n",
      "Epoch 341 : Total Loss 11.684\n",
      "Epoch 351 : Total Loss 11.490\n",
      "Epoch 361 : Total Loss 11.309\n",
      "Epoch 371 : Total Loss 11.140\n",
      "Epoch 381 : Total Loss 10.983\n",
      "Epoch 391 : Total Loss 10.835\n",
      "Epoch 401 : Total Loss 10.697\n",
      "Epoch 411 : Total Loss 10.568\n",
      "Epoch 421 : Total Loss 10.445\n",
      "Epoch 431 : Total Loss 10.330\n",
      "Epoch 441 : Total Loss 10.221\n",
      "Epoch 451 : Total Loss 10.118\n",
      "Epoch 461 : Total Loss 10.020\n",
      "Epoch 471 : Total Loss 9.927\n",
      "Epoch 481 : Total Loss 9.838\n",
      "Epoch 491 : Total Loss 9.753\n",
      "Epoch 501 : Total Loss 9.672\n",
      "Epoch 511 : Total Loss 9.594\n",
      "Epoch 521 : Total Loss 9.519\n",
      "Epoch 531 : Total Loss 9.448\n",
      "Epoch 541 : Total Loss 9.378\n",
      "Epoch 551 : Total Loss 9.312\n",
      "Epoch 561 : Total Loss 9.247\n",
      "Epoch 571 : Total Loss 9.185\n",
      "Epoch 581 : Total Loss 9.125\n",
      "Epoch 591 : Total Loss 9.066\n",
      "Epoch 601 : Total Loss 9.009\n",
      "Epoch 611 : Total Loss 8.954\n",
      "Epoch 621 : Total Loss 8.900\n",
      "Epoch 631 : Total Loss 8.847\n",
      "Epoch 641 : Total Loss 8.796\n",
      "Epoch 651 : Total Loss 8.746\n",
      "Epoch 661 : Total Loss 8.697\n",
      "Epoch 671 : Total Loss 8.649\n",
      "Epoch 681 : Total Loss 8.602\n",
      "Epoch 691 : Total Loss 8.556\n",
      "Epoch 701 : Total Loss 8.511\n",
      "Epoch 711 : Total Loss 8.467\n",
      "Epoch 721 : Total Loss 8.423\n",
      "Epoch 731 : Total Loss 8.380\n",
      "Epoch 741 : Total Loss 8.338\n",
      "Epoch 751 : Total Loss 8.297\n",
      "Epoch 761 : Total Loss 8.256\n",
      "Epoch 771 : Total Loss 8.216\n",
      "Epoch 781 : Total Loss 8.176\n",
      "Epoch 791 : Total Loss 8.137\n",
      "Epoch 801 : Total Loss 8.099\n",
      "Epoch 811 : Total Loss 8.061\n",
      "Epoch 821 : Total Loss 8.024\n",
      "Epoch 831 : Total Loss 7.987\n",
      "Epoch 841 : Total Loss 7.950\n",
      "Epoch 851 : Total Loss 7.914\n",
      "Epoch 861 : Total Loss 7.878\n",
      "Epoch 871 : Total Loss 7.843\n",
      "Epoch 881 : Total Loss 7.808\n",
      "Epoch 891 : Total Loss 7.774\n",
      "Epoch 901 : Total Loss 7.740\n",
      "Epoch 911 : Total Loss 7.706\n",
      "Epoch 921 : Total Loss 7.673\n",
      "Epoch 931 : Total Loss 7.640\n",
      "Epoch 941 : Total Loss 7.608\n",
      "Epoch 951 : Total Loss 7.575\n",
      "Epoch 961 : Total Loss 7.544\n",
      "Epoch 971 : Total Loss 7.512\n",
      "Epoch 981 : Total Loss 7.481\n",
      "Epoch 991 : Total Loss 7.450\n",
      "Epoch 1001 : Total Loss 7.419\n",
      "Epoch 1011 : Total Loss 7.389\n",
      "Epoch 1021 : Total Loss 7.359\n",
      "Epoch 1031 : Total Loss 7.330\n",
      "Epoch 1041 : Total Loss 7.300\n",
      "Epoch 1051 : Total Loss 7.271\n",
      "Epoch 1061 : Total Loss 7.243\n",
      "Epoch 1071 : Total Loss 7.214\n",
      "Epoch 1081 : Total Loss 7.186\n",
      "Epoch 1091 : Total Loss 7.158\n",
      "Epoch 1101 : Total Loss 7.131\n",
      "Epoch 1111 : Total Loss 7.104\n",
      "Epoch 1121 : Total Loss 7.077\n",
      "Epoch 1131 : Total Loss 7.050\n",
      "Epoch 1141 : Total Loss 7.024\n",
      "Epoch 1151 : Total Loss 6.998\n",
      "Epoch 1161 : Total Loss 6.972\n",
      "Epoch 1171 : Total Loss 6.947\n",
      "Epoch 1181 : Total Loss 6.922\n",
      "Epoch 1191 : Total Loss 6.897\n",
      "Epoch 1201 : Total Loss 6.873\n",
      "Epoch 1211 : Total Loss 6.849\n",
      "Epoch 1221 : Total Loss 6.825\n",
      "Epoch 1231 : Total Loss 6.801\n",
      "Epoch 1241 : Total Loss 6.778\n",
      "Epoch 1251 : Total Loss 6.755\n",
      "Epoch 1261 : Total Loss 6.732\n",
      "Epoch 1271 : Total Loss 6.709\n",
      "Epoch 1281 : Total Loss 6.687\n",
      "Epoch 1291 : Total Loss 6.665\n",
      "Epoch 1301 : Total Loss 6.644\n",
      "Epoch 1311 : Total Loss 6.622\n",
      "Epoch 1321 : Total Loss 6.601\n",
      "Epoch 1331 : Total Loss 6.581\n",
      "Epoch 1341 : Total Loss 6.560\n",
      "Epoch 1351 : Total Loss 6.540\n",
      "Epoch 1361 : Total Loss 6.520\n",
      "Epoch 1371 : Total Loss 6.501\n",
      "Epoch 1381 : Total Loss 6.481\n",
      "Epoch 1391 : Total Loss 6.462\n",
      "Epoch 1401 : Total Loss 6.443\n",
      "Epoch 1411 : Total Loss 6.425\n",
      "Epoch 1421 : Total Loss 6.407\n",
      "Epoch 1431 : Total Loss 6.389\n",
      "Epoch 1441 : Total Loss 6.371\n",
      "Epoch 1451 : Total Loss 6.354\n",
      "Epoch 1461 : Total Loss 6.337\n",
      "Epoch 1471 : Total Loss 6.320\n",
      "Epoch 1481 : Total Loss 6.303\n",
      "Epoch 1491 : Total Loss 6.287\n",
      "Epoch 1501 : Total Loss 6.271\n",
      "Epoch 1511 : Total Loss 6.255\n",
      "Epoch 1521 : Total Loss 6.240\n",
      "Epoch 1531 : Total Loss 6.224\n",
      "Epoch 1541 : Total Loss 6.209\n",
      "Epoch 1551 : Total Loss 6.195\n",
      "Epoch 1561 : Total Loss 6.180\n",
      "Epoch 1571 : Total Loss 6.166\n",
      "Epoch 1581 : Total Loss 6.152\n",
      "Epoch 1591 : Total Loss 6.138\n",
      "Epoch 1601 : Total Loss 6.124\n",
      "Epoch 1611 : Total Loss 6.111\n",
      "Epoch 1621 : Total Loss 6.098\n",
      "Epoch 1631 : Total Loss 6.085\n",
      "Epoch 1641 : Total Loss 6.073\n",
      "Epoch 1651 : Total Loss 6.060\n",
      "Epoch 1661 : Total Loss 6.048\n",
      "Epoch 1671 : Total Loss 6.036\n",
      "Epoch 1681 : Total Loss 6.025\n",
      "Epoch 1691 : Total Loss 6.013\n",
      "Epoch 1701 : Total Loss 6.002\n",
      "Epoch 1711 : Total Loss 5.991\n",
      "Epoch 1721 : Total Loss 5.980\n",
      "Epoch 1731 : Total Loss 5.970\n",
      "Epoch 1741 : Total Loss 5.959\n",
      "Epoch 1751 : Total Loss 5.949\n",
      "Epoch 1761 : Total Loss 5.939\n",
      "Epoch 1771 : Total Loss 5.929\n",
      "Epoch 1781 : Total Loss 5.920\n",
      "Epoch 1791 : Total Loss 5.910\n",
      "Epoch 1801 : Total Loss 5.901\n",
      "Epoch 1811 : Total Loss 5.892\n",
      "Epoch 1821 : Total Loss 5.883\n",
      "Epoch 1831 : Total Loss 5.875\n",
      "Epoch 1841 : Total Loss 5.866\n",
      "Epoch 1851 : Total Loss 5.858\n",
      "Epoch 1861 : Total Loss 5.850\n",
      "Epoch 1871 : Total Loss 5.842\n",
      "Epoch 1881 : Total Loss 5.834\n",
      "Epoch 1891 : Total Loss 5.827\n",
      "Epoch 1901 : Total Loss 5.819\n",
      "Epoch 1911 : Total Loss 5.812\n",
      "Epoch 1921 : Total Loss 5.805\n",
      "Epoch 1931 : Total Loss 5.798\n",
      "Epoch 1941 : Total Loss 5.791\n",
      "Epoch 1951 : Total Loss 5.784\n",
      "Epoch 1961 : Total Loss 5.778\n",
      "Epoch 1971 : Total Loss 5.771\n",
      "Epoch 1981 : Total Loss 5.765\n",
      "Epoch 1991 : Total Loss 5.759\n",
      "Epoch 2001 : Total Loss 5.753\n",
      "Epoch 2011 : Total Loss 5.747\n",
      "Epoch 2021 : Total Loss 5.741\n",
      "Epoch 2031 : Total Loss 5.736\n",
      "Epoch 2041 : Total Loss 5.730\n",
      "Epoch 2051 : Total Loss 5.725\n",
      "Epoch 2061 : Total Loss 5.720\n",
      "Epoch 2071 : Total Loss 5.714\n",
      "Epoch 2081 : Total Loss 5.709\n",
      "Epoch 2091 : Total Loss 5.705\n",
      "Epoch 2101 : Total Loss 5.700\n",
      "Epoch 2111 : Total Loss 5.695\n",
      "Epoch 2121 : Total Loss 5.691\n",
      "Epoch 2131 : Total Loss 5.686\n",
      "Epoch 2141 : Total Loss 5.682\n",
      "Epoch 2151 : Total Loss 5.678\n",
      "Epoch 2161 : Total Loss 5.673\n",
      "Epoch 2171 : Total Loss 5.669\n",
      "Epoch 2181 : Total Loss 5.665\n",
      "Epoch 2191 : Total Loss 5.661\n",
      "Epoch 2201 : Total Loss 5.658\n",
      "Epoch 2211 : Total Loss 5.654\n",
      "Epoch 2221 : Total Loss 5.650\n",
      "Epoch 2231 : Total Loss 5.647\n",
      "Epoch 2241 : Total Loss 5.643\n",
      "Epoch 2251 : Total Loss 5.640\n",
      "Epoch 2261 : Total Loss 5.637\n",
      "Epoch 2271 : Total Loss 5.634\n",
      "Epoch 2281 : Total Loss 5.630\n",
      "Epoch 2291 : Total Loss 5.627\n",
      "Epoch 2301 : Total Loss 5.624\n",
      "Epoch 2311 : Total Loss 5.621\n",
      "Epoch 2321 : Total Loss 5.618\n",
      "Epoch 2331 : Total Loss 5.616\n",
      "Epoch 2341 : Total Loss 5.613\n",
      "Epoch 2351 : Total Loss 5.610\n",
      "Epoch 2361 : Total Loss 5.608\n",
      "Epoch 2371 : Total Loss 5.605\n",
      "Epoch 2381 : Total Loss 5.603\n",
      "Epoch 2391 : Total Loss 5.600\n",
      "Epoch 2401 : Total Loss 5.598\n",
      "Epoch 2411 : Total Loss 5.596\n",
      "Epoch 2421 : Total Loss 5.593\n",
      "Epoch 2431 : Total Loss 5.591\n",
      "Epoch 2441 : Total Loss 5.589\n",
      "Epoch 2451 : Total Loss 5.587\n",
      "Epoch 2461 : Total Loss 5.585\n",
      "Epoch 2471 : Total Loss 5.583\n",
      "Epoch 2481 : Total Loss 5.581\n",
      "Epoch 2491 : Total Loss 5.579\n",
      "Epoch 2501 : Total Loss 5.577\n",
      "Epoch 2511 : Total Loss 5.575\n",
      "Epoch 2521 : Total Loss 5.573\n",
      "Epoch 2531 : Total Loss 5.571\n",
      "Epoch 2541 : Total Loss 5.570\n",
      "Epoch 2551 : Total Loss 5.568\n",
      "Epoch 2561 : Total Loss 5.566\n",
      "Epoch 2571 : Total Loss 5.565\n",
      "Epoch 2581 : Total Loss 5.563\n",
      "Epoch 2591 : Total Loss 5.562\n",
      "Epoch 2601 : Total Loss 5.560\n",
      "Epoch 2611 : Total Loss 5.559\n",
      "Epoch 2621 : Total Loss 5.557\n",
      "Epoch 2631 : Total Loss 5.556\n",
      "Epoch 2641 : Total Loss 5.554\n",
      "Epoch 2651 : Total Loss 5.553\n",
      "Epoch 2661 : Total Loss 5.552\n",
      "Epoch 2671 : Total Loss 5.551\n",
      "Epoch 2681 : Total Loss 5.549\n",
      "Epoch 2691 : Total Loss 5.548\n",
      "Epoch 2701 : Total Loss 5.547\n",
      "Epoch 2711 : Total Loss 5.546\n",
      "Epoch 2721 : Total Loss 5.544\n",
      "Epoch 2731 : Total Loss 5.543\n",
      "Epoch 2741 : Total Loss 5.542\n",
      "Epoch 2751 : Total Loss 5.541\n",
      "Epoch 2761 : Total Loss 5.540\n",
      "Epoch 2771 : Total Loss 5.539\n",
      "Epoch 2781 : Total Loss 5.538\n",
      "Epoch 2791 : Total Loss 5.537\n",
      "Epoch 2801 : Total Loss 5.536\n",
      "Epoch 2811 : Total Loss 5.535\n",
      "Epoch 2821 : Total Loss 5.534\n",
      "Epoch 2831 : Total Loss 5.533\n",
      "Epoch 2841 : Total Loss 5.532\n",
      "Epoch 2851 : Total Loss 5.531\n",
      "Epoch 2861 : Total Loss 5.531\n",
      "Epoch 2871 : Total Loss 5.530\n",
      "Epoch 2881 : Total Loss 5.529\n",
      "Epoch 2891 : Total Loss 5.528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2901 : Total Loss 5.527\n",
      "Epoch 2911 : Total Loss 5.526\n",
      "Epoch 2921 : Total Loss 5.526\n",
      "Epoch 2931 : Total Loss 5.525\n",
      "Epoch 2941 : Total Loss 5.524\n",
      "Epoch 2951 : Total Loss 5.523\n",
      "Epoch 2961 : Total Loss 5.523\n",
      "Epoch 2971 : Total Loss 5.522\n",
      "Epoch 2981 : Total Loss 5.521\n",
      "Epoch 2991 : Total Loss 5.521\n",
      "Epoch 3001 : Total Loss 5.520\n",
      "Epoch 3011 : Total Loss 5.519\n",
      "Epoch 3021 : Total Loss 5.519\n",
      "Epoch 3031 : Total Loss 5.518\n",
      "Epoch 3041 : Total Loss 5.517\n",
      "Epoch 3051 : Total Loss 5.517\n",
      "Epoch 3061 : Total Loss 5.516\n",
      "Epoch 3071 : Total Loss 5.516\n",
      "Epoch 3081 : Total Loss 5.515\n",
      "Epoch 3091 : Total Loss 5.515\n",
      "Epoch 3101 : Total Loss 5.514\n",
      "Epoch 3111 : Total Loss 5.513\n",
      "Epoch 3121 : Total Loss 5.513\n",
      "Epoch 3131 : Total Loss 5.512\n",
      "Epoch 3141 : Total Loss 5.512\n",
      "Epoch 3151 : Total Loss 5.511\n",
      "Epoch 3161 : Total Loss 5.511\n",
      "Epoch 3171 : Total Loss 5.510\n",
      "Epoch 3181 : Total Loss 5.510\n",
      "Epoch 3191 : Total Loss 5.509\n",
      "Epoch 3201 : Total Loss 5.509\n",
      "Epoch 3211 : Total Loss 5.508\n",
      "Epoch 3221 : Total Loss 5.508\n",
      "Epoch 3231 : Total Loss 5.508\n",
      "Epoch 3241 : Total Loss 5.507\n",
      "Epoch 3251 : Total Loss 5.507\n",
      "Epoch 3261 : Total Loss 5.506\n",
      "Epoch 3271 : Total Loss 5.506\n",
      "Epoch 3281 : Total Loss 5.505\n",
      "Epoch 3291 : Total Loss 5.505\n",
      "Epoch 3301 : Total Loss 5.505\n",
      "Epoch 3311 : Total Loss 5.504\n",
      "Epoch 3321 : Total Loss 5.504\n",
      "Epoch 3331 : Total Loss 5.503\n",
      "Epoch 3341 : Total Loss 5.503\n",
      "Epoch 3351 : Total Loss 5.503\n",
      "Epoch 3361 : Total Loss 5.502\n",
      "Epoch 3371 : Total Loss 5.502\n",
      "Epoch 3381 : Total Loss 5.502\n",
      "Epoch 3391 : Total Loss 5.501\n",
      "Epoch 3401 : Total Loss 5.501\n",
      "Epoch 3411 : Total Loss 5.501\n",
      "Epoch 3421 : Total Loss 5.500\n",
      "Epoch 3431 : Total Loss 5.500\n",
      "Epoch 3441 : Total Loss 5.500\n",
      "Epoch 3451 : Total Loss 5.499\n",
      "Epoch 3461 : Total Loss 5.499\n",
      "Epoch 3471 : Total Loss 5.499\n",
      "Epoch 3481 : Total Loss 5.498\n",
      "Epoch 3491 : Total Loss 5.498\n",
      "Epoch 3501 : Total Loss 5.498\n",
      "Epoch 3511 : Total Loss 5.497\n",
      "Epoch 3521 : Total Loss 5.497\n",
      "Epoch 3531 : Total Loss 5.497\n",
      "Epoch 3541 : Total Loss 5.496\n",
      "Epoch 3551 : Total Loss 5.496\n",
      "Epoch 3561 : Total Loss 5.496\n",
      "Epoch 3571 : Total Loss 5.496\n",
      "Epoch 3581 : Total Loss 5.495\n",
      "Epoch 3591 : Total Loss 5.495\n",
      "Epoch 3601 : Total Loss 5.495\n",
      "Epoch 3611 : Total Loss 5.494\n",
      "Epoch 3621 : Total Loss 5.494\n",
      "Epoch 3631 : Total Loss 5.494\n",
      "Epoch 3641 : Total Loss 5.494\n",
      "Epoch 3651 : Total Loss 5.493\n",
      "Epoch 3661 : Total Loss 5.493\n",
      "Epoch 3671 : Total Loss 5.493\n",
      "Epoch 3681 : Total Loss 5.493\n",
      "Epoch 3691 : Total Loss 5.492\n",
      "Epoch 3701 : Total Loss 5.492\n",
      "Epoch 3711 : Total Loss 5.492\n",
      "Epoch 3721 : Total Loss 5.492\n",
      "Epoch 3731 : Total Loss 5.491\n",
      "Epoch 3741 : Total Loss 5.491\n",
      "Epoch 3751 : Total Loss 5.491\n",
      "Epoch 3761 : Total Loss 5.491\n",
      "Epoch 3771 : Total Loss 5.491\n",
      "Epoch 3781 : Total Loss 5.490\n",
      "Epoch 3791 : Total Loss 5.490\n",
      "Epoch 3801 : Total Loss 5.490\n",
      "Epoch 3811 : Total Loss 5.490\n",
      "Epoch 3821 : Total Loss 5.489\n",
      "Epoch 3831 : Total Loss 5.489\n",
      "Epoch 3841 : Total Loss 5.489\n",
      "Epoch 3851 : Total Loss 5.489\n",
      "Epoch 3861 : Total Loss 5.489\n",
      "Epoch 3871 : Total Loss 5.488\n",
      "Epoch 3881 : Total Loss 5.488\n",
      "Epoch 3891 : Total Loss 5.488\n",
      "Epoch 3901 : Total Loss 5.488\n",
      "Epoch 3911 : Total Loss 5.488\n",
      "Epoch 3921 : Total Loss 5.487\n",
      "Epoch 3931 : Total Loss 5.487\n",
      "Epoch 3941 : Total Loss 5.487\n",
      "Epoch 3951 : Total Loss 5.487\n",
      "Epoch 3961 : Total Loss 5.487\n",
      "Epoch 3971 : Total Loss 5.486\n",
      "Epoch 3981 : Total Loss 5.486\n",
      "Epoch 3991 : Total Loss 5.486\n",
      "Epoch 4001 : Total Loss 5.486\n",
      "Epoch 4011 : Total Loss 5.486\n",
      "Epoch 4021 : Total Loss 5.485\n",
      "Epoch 4031 : Total Loss 5.485\n",
      "Epoch 4041 : Total Loss 5.485\n",
      "Epoch 4051 : Total Loss 5.485\n",
      "Epoch 4061 : Total Loss 5.485\n",
      "Epoch 4071 : Total Loss 5.485\n",
      "Epoch 4081 : Total Loss 5.484\n",
      "Epoch 4091 : Total Loss 5.484\n",
      "Epoch 4101 : Total Loss 5.484\n",
      "Epoch 4111 : Total Loss 5.484\n",
      "Epoch 4121 : Total Loss 5.484\n",
      "Epoch 4131 : Total Loss 5.483\n",
      "Epoch 4141 : Total Loss 5.483\n",
      "Epoch 4151 : Total Loss 5.483\n",
      "Epoch 4161 : Total Loss 5.483\n",
      "Epoch 4171 : Total Loss 5.483\n",
      "Epoch 4181 : Total Loss 5.483\n",
      "Epoch 4191 : Total Loss 5.482\n",
      "Epoch 4201 : Total Loss 5.482\n",
      "Epoch 4211 : Total Loss 5.482\n",
      "Epoch 4221 : Total Loss 5.482\n",
      "Epoch 4231 : Total Loss 5.482\n",
      "Epoch 4241 : Total Loss 5.482\n",
      "Epoch 4251 : Total Loss 5.482\n",
      "Epoch 4261 : Total Loss 5.481\n",
      "Epoch 4271 : Total Loss 5.481\n",
      "Epoch 4281 : Total Loss 5.481\n",
      "Epoch 4291 : Total Loss 5.481\n",
      "Epoch 4301 : Total Loss 5.481\n",
      "Epoch 4311 : Total Loss 5.481\n",
      "Epoch 4321 : Total Loss 5.480\n",
      "Epoch 4331 : Total Loss 5.480\n",
      "Epoch 4341 : Total Loss 5.480\n",
      "Epoch 4351 : Total Loss 5.480\n",
      "Epoch 4361 : Total Loss 5.480\n",
      "Epoch 4371 : Total Loss 5.480\n",
      "Epoch 4381 : Total Loss 5.479\n",
      "Epoch 4391 : Total Loss 5.479\n",
      "Epoch 4401 : Total Loss 5.479\n",
      "Epoch 4411 : Total Loss 5.479\n",
      "Epoch 4421 : Total Loss 5.479\n",
      "Epoch 4431 : Total Loss 5.479\n",
      "Epoch 4441 : Total Loss 5.479\n",
      "Epoch 4451 : Total Loss 5.478\n",
      "Epoch 4461 : Total Loss 5.478\n",
      "Epoch 4471 : Total Loss 5.478\n",
      "Epoch 4481 : Total Loss 5.478\n",
      "Epoch 4491 : Total Loss 5.478\n",
      "Epoch 4501 : Total Loss 5.478\n",
      "Epoch 4511 : Total Loss 5.478\n",
      "Epoch 4521 : Total Loss 5.477\n",
      "Epoch 4531 : Total Loss 5.477\n",
      "Epoch 4541 : Total Loss 5.477\n",
      "Epoch 4551 : Total Loss 5.477\n",
      "Epoch 4561 : Total Loss 5.477\n",
      "Epoch 4571 : Total Loss 5.477\n",
      "Epoch 4581 : Total Loss 5.477\n",
      "Epoch 4591 : Total Loss 5.476\n",
      "Epoch 4601 : Total Loss 5.476\n",
      "Epoch 4611 : Total Loss 5.476\n",
      "Epoch 4621 : Total Loss 5.476\n",
      "Epoch 4631 : Total Loss 5.476\n",
      "Epoch 4641 : Total Loss 5.476\n",
      "Epoch 4651 : Total Loss 5.476\n",
      "Epoch 4661 : Total Loss 5.475\n",
      "Epoch 4671 : Total Loss 5.475\n",
      "Epoch 4681 : Total Loss 5.475\n",
      "Epoch 4691 : Total Loss 5.475\n",
      "Epoch 4701 : Total Loss 5.475\n",
      "Epoch 4711 : Total Loss 5.475\n",
      "Epoch 4721 : Total Loss 5.475\n",
      "Epoch 4731 : Total Loss 5.474\n",
      "Epoch 4741 : Total Loss 5.474\n",
      "Epoch 4751 : Total Loss 5.474\n",
      "Epoch 4761 : Total Loss 5.474\n",
      "Epoch 4771 : Total Loss 5.474\n",
      "Epoch 4781 : Total Loss 5.474\n",
      "Epoch 4791 : Total Loss 5.474\n",
      "Epoch 4801 : Total Loss 5.473\n",
      "Epoch 4811 : Total Loss 5.473\n",
      "Epoch 4821 : Total Loss 5.473\n",
      "Epoch 4831 : Total Loss 5.473\n",
      "Epoch 4841 : Total Loss 5.473\n",
      "Epoch 4851 : Total Loss 5.473\n",
      "Epoch 4861 : Total Loss 5.473\n",
      "Epoch 4871 : Total Loss 5.473\n",
      "Epoch 4881 : Total Loss 5.472\n",
      "Epoch 4891 : Total Loss 5.472\n",
      "Epoch 4901 : Total Loss 5.472\n",
      "Epoch 4911 : Total Loss 5.472\n",
      "Epoch 4921 : Total Loss 5.472\n",
      "Epoch 4931 : Total Loss 5.472\n",
      "Epoch 4941 : Total Loss 5.472\n",
      "Epoch 4951 : Total Loss 5.471\n",
      "Epoch 4961 : Total Loss 5.471\n",
      "Epoch 4971 : Total Loss 5.471\n",
      "Epoch 4981 : Total Loss 5.471\n",
      "Epoch 4991 : Total Loss 5.471\n",
      "A_hat is:\n",
      " [[4.98447014 2.97009182 1.54598166 1.50219606 0.9902487 ]\n",
      " [2.99605858 2.12761136 3.17161116 2.87072417 0.95654322]\n",
      " [3.98158649 2.34293891 1.04127825 1.03001406 0.75980498]\n",
      " [4.52329661 2.9474781  3.05471784 2.81272953 1.16479036]\n",
      " [4.96573884 3.01061644 1.87870719 1.79363369 1.0410779 ]]\n"
     ]
    }
   ],
   "source": [
    "U_hat, V_hat = matrix_factorization(A, U, V, user_features, 5000)\n",
    "\n",
    "print('A_hat is:\\n', np.dot(U_hat, V_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefe3a4",
   "metadata": {},
   "source": [
    "After training for long enough, we see that the existing interactions are predicted quite well. To see the average error of each entry in $A_{hat}$ corresponding to NONZEROS in $A$, we can just use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c304c6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.98447014, 2.97009182, 1.54598166, 1.50219606, 0.9902487 ],\n",
       "       [2.99605858, 2.12761136, 3.17161116, 2.87072417, 0.95654322],\n",
       "       [3.98158649, 2.34293891, 1.04127825, 1.03001406, 0.75980498],\n",
       "       [4.52329661, 2.9474781 , 3.05471784, 2.81272953, 1.16479036],\n",
       "       [4.96573884, 3.01061644, 1.87870719, 1.79363369, 1.0410779 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_hat = np.dot(U_hat, V_hat.T)\n",
    "A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a91f831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 2, 1, 0],\n",
       "       [3, 0, 4, 2, 1],\n",
       "       [4, 0, 0, 1, 0],\n",
       "       [0, 0, 2, 4, 0],\n",
       "       [5, 3, 0, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64eecf",
   "metadata": {},
   "source": [
    "### Write a Method to Evaluate our Output\n",
    "\n",
    "Since this is a regression task, we will be evaluating the performance of our model through (root) mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29d81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have function to evaluate total loss in our prediction\n",
    "\n",
    "def evaluation_metrics(A, A_hat, return_nonzero_indices = False, return_rmse = True):\n",
    "    # find all nonzero indices of ground truth matrix\n",
    "    nonzero_ind = [(i,j) for i in range(A.shape[0]) for j in range(A.shape[1]) if A[i,j] > 0]\n",
    "    \n",
    "    # compute the losses in each ground truth matrix\n",
    "    avg_error = sum([(A_hat[i][j] - A[i][j])**2 for i, j in nonzero_ind])/len(nonzero_ind)\n",
    "    rmse = np.sqrt(avg_error)\n",
    "    \n",
    "    # set a flag for if we want to return the nonzero indices or not\n",
    "    li_results = []\n",
    "    if return_rmse:\n",
    "        li_results.append(avg_error)\n",
    "        li_results.append(rmse)\n",
    "    else:\n",
    "        li_results.append(avg_error)\n",
    "    \n",
    "    if return_nonzero_indices:\n",
    "        li_results.append(nonzero_indices)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return li_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8f88f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of all predictions is 0.295.\n",
      "The root mean squared error of all predictions is 0.544.\n"
     ]
    }
   ],
   "source": [
    "mse, rmse = evaluation_metrics(A, A_hat)\n",
    "\n",
    "print(f'The mean squared error of all predictions is {mse:.3f}.')\n",
    "print(f'The root mean squared error of all predictions is {rmse:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381c12d",
   "metadata": {},
   "source": [
    "Therefore, our matrix factorization algorithm is very good at predicting the existing entries. Let us see what the predictions were for the empty entries.\n",
    "\n",
    "### Write Method to Predict the Zero Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e9733bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a function predict the missing entries of our ground truth matrix\n",
    "\n",
    "def mat_predict(A, A_hat, conv_to_int = True, zip_indices = False):\n",
    "    # find zero indices of A (our missing entries that we are predicting)\n",
    "    zero_ind = [(i,j) for i in range(A.shape[0]) for j in range(A.shape[1]) if A[i][j] == 0]\n",
    "    \n",
    "    # find predictions\n",
    "    og_pred = [A_hat[i,j] for i,j in zero_ind]\n",
    "    \n",
    "    # WARNING: FOR VERY LARGE AND SPARSE MATRICES, I DO NOT RECOMMEND DOING THIS. JUST LOOK AT THE OUTPUT\n",
    "    if zip_indices:\n",
    "        for prediction, indices in zip(pred, zero_ind):\n",
    "            print(f'The interaction at row, column {indices} is {prediction}.')\n",
    "            \n",
    "    # convert output to integers\n",
    "    if conv_to_int:\n",
    "        # if the rounded A_hat has entries greater than the max of A, then set it to floor, otherwise, round it to integer\n",
    "        int_prediction = np.where(A_hat > np.max(A), np.floor(A_hat), np.round(A_hat, decimals=0))\n",
    "        \n",
    "        # return integer predictions at indices where 0's originally were in the ground truth A\n",
    "        return [int_prediction[i,j] for i,j in zero_ind]\n",
    "    else:\n",
    "        # return the original floating point predictions from the algorithm output\n",
    "        \n",
    "        return og_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7faa3856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 2.0, 1.0, 1.0, 5.0, 3.0, 1.0, 2.0, 2.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_predict(A, A_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d85c4066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 4),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 4),\n",
       " (4, 2),\n",
       " (4, 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find zero indices of A\n",
    "zero_ind = [(i,j) for i in range(A.shape[0]) for j in range(A.shape[1]) if A[i][j] == 0]\n",
    "\n",
    "zero_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcde43c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.990248697667072, 'At index:(0, 4)'],\n",
       " [2.1276113626791973, 'At index:(1, 1)'],\n",
       " [2.3429389126641804, 'At index:(2, 1)'],\n",
       " [1.0412782499755882, 'At index:(2, 2)'],\n",
       " [0.7598049813295886, 'At index:(2, 4)'],\n",
       " [4.523296605425644, 'At index:(3, 0)'],\n",
       " [2.947478096798725, 'At index:(3, 1)'],\n",
       " [1.1647903631780367, 'At index:(3, 4)'],\n",
       " [1.8787071873760717, 'At index:(4, 2)'],\n",
       " [1.793633687301854, 'At index:(4, 3)']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [[A_hat[i][j], f'At index:{(i,j)}'] for i,j in zero_ind]\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a54036",
   "metadata": {},
   "source": [
    "To take it one step further, we could even round these up or down in order to give a good prediction for what these interactions *would have* been rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103910d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'At index:(0, 4)'],\n",
       " [2, 'At index:(1, 1)'],\n",
       " [2, 'At index:(2, 1)'],\n",
       " [1, 'At index:(2, 2)'],\n",
       " [1, 'At index:(2, 4)'],\n",
       " [5, 'At index:(3, 0)'],\n",
       " [3, 'At index:(3, 1)'],\n",
       " [1, 'At index:(3, 4)'],\n",
       " [2, 'At index:(4, 2)'],\n",
       " [2, 'At index:(4, 3)']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_predictions = [[int(np.round(A_hat[i][j], decimals=0)), f'At index:{(i,j)}'] for i,j in zero_ind]\n",
    "\n",
    "rounded_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b186a",
   "metadata": {},
   "source": [
    "**Non-Square Matrix Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dad1eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.37454012 0.95071431]\n",
      " [0.73199394 0.59865848]\n",
      " [0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615]]\n",
      "V is:\n",
      " [[0.60111501 0.70807258]\n",
      " [0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911]\n",
      " [0.18182497 0.18340451]]\n",
      "Epoch 1 : Total Loss 70.905\n",
      "Epoch 11 : Total Loss 69.765\n",
      "Epoch 21 : Total Loss 68.610\n",
      "Epoch 31 : Total Loss 67.441\n",
      "Epoch 41 : Total Loss 66.260\n",
      "Epoch 51 : Total Loss 65.068\n",
      "Epoch 61 : Total Loss 63.867\n",
      "Epoch 71 : Total Loss 62.659\n",
      "Epoch 81 : Total Loss 61.447\n",
      "Epoch 91 : Total Loss 60.231\n",
      "Epoch 101 : Total Loss 59.014\n",
      "Epoch 111 : Total Loss 57.799\n",
      "Epoch 121 : Total Loss 56.587\n",
      "Epoch 131 : Total Loss 55.380\n",
      "Epoch 141 : Total Loss 54.182\n",
      "Epoch 151 : Total Loss 52.994\n",
      "Epoch 161 : Total Loss 51.819\n",
      "Epoch 171 : Total Loss 50.657\n",
      "Epoch 181 : Total Loss 49.513\n",
      "Epoch 191 : Total Loss 48.386\n",
      "Epoch 201 : Total Loss 47.280\n",
      "Epoch 211 : Total Loss 46.195\n",
      "Epoch 221 : Total Loss 45.134\n",
      "Epoch 231 : Total Loss 44.098\n",
      "Epoch 241 : Total Loss 43.087\n",
      "Epoch 251 : Total Loss 42.103\n",
      "Epoch 261 : Total Loss 41.147\n",
      "Epoch 271 : Total Loss 40.219\n",
      "Epoch 281 : Total Loss 39.320\n",
      "Epoch 291 : Total Loss 38.450\n",
      "Epoch 301 : Total Loss 37.609\n",
      "Epoch 311 : Total Loss 36.798\n",
      "Epoch 321 : Total Loss 36.016\n",
      "Epoch 331 : Total Loss 35.263\n",
      "Epoch 341 : Total Loss 34.538\n",
      "Epoch 351 : Total Loss 33.841\n",
      "Epoch 361 : Total Loss 33.172\n",
      "Epoch 371 : Total Loss 32.529\n",
      "Epoch 381 : Total Loss 31.912\n",
      "Epoch 391 : Total Loss 31.321\n",
      "Epoch 401 : Total Loss 30.753\n",
      "Epoch 411 : Total Loss 30.209\n",
      "Epoch 421 : Total Loss 29.688\n",
      "Epoch 431 : Total Loss 29.188\n",
      "Epoch 441 : Total Loss 28.708\n",
      "Epoch 451 : Total Loss 28.248\n",
      "Epoch 461 : Total Loss 27.806\n",
      "Epoch 471 : Total Loss 27.383\n",
      "Epoch 481 : Total Loss 26.976\n",
      "Epoch 491 : Total Loss 26.585\n",
      "Epoch 501 : Total Loss 26.209\n",
      "Epoch 511 : Total Loss 25.847\n",
      "Epoch 521 : Total Loss 25.499\n",
      "Epoch 531 : Total Loss 25.164\n",
      "Epoch 541 : Total Loss 24.840\n",
      "Epoch 551 : Total Loss 24.529\n",
      "Epoch 561 : Total Loss 24.227\n",
      "Epoch 571 : Total Loss 23.936\n",
      "Epoch 581 : Total Loss 23.655\n",
      "Epoch 591 : Total Loss 23.383\n",
      "Epoch 601 : Total Loss 23.119\n",
      "Epoch 611 : Total Loss 22.864\n",
      "Epoch 621 : Total Loss 22.616\n",
      "Epoch 631 : Total Loss 22.375\n",
      "Epoch 641 : Total Loss 22.142\n",
      "Epoch 651 : Total Loss 21.915\n",
      "Epoch 661 : Total Loss 21.694\n",
      "Epoch 671 : Total Loss 21.479\n",
      "Epoch 681 : Total Loss 21.270\n",
      "Epoch 691 : Total Loss 21.066\n",
      "Epoch 701 : Total Loss 20.867\n",
      "Epoch 711 : Total Loss 20.673\n",
      "Epoch 721 : Total Loss 20.484\n",
      "Epoch 731 : Total Loss 20.299\n",
      "Epoch 741 : Total Loss 20.118\n",
      "Epoch 751 : Total Loss 19.941\n",
      "Epoch 761 : Total Loss 19.768\n",
      "Epoch 771 : Total Loss 19.599\n",
      "Epoch 781 : Total Loss 19.433\n",
      "Epoch 791 : Total Loss 19.271\n",
      "Epoch 801 : Total Loss 19.111\n",
      "Epoch 811 : Total Loss 18.955\n",
      "Epoch 821 : Total Loss 18.802\n",
      "Epoch 831 : Total Loss 18.651\n",
      "Epoch 841 : Total Loss 18.503\n",
      "Epoch 851 : Total Loss 18.358\n",
      "Epoch 861 : Total Loss 18.214\n",
      "Epoch 871 : Total Loss 18.074\n",
      "Epoch 881 : Total Loss 17.935\n",
      "Epoch 891 : Total Loss 17.798\n",
      "Epoch 901 : Total Loss 17.663\n",
      "Epoch 911 : Total Loss 17.530\n",
      "Epoch 921 : Total Loss 17.398\n",
      "Epoch 931 : Total Loss 17.268\n",
      "Epoch 941 : Total Loss 17.140\n",
      "Epoch 951 : Total Loss 17.013\n",
      "Epoch 961 : Total Loss 16.887\n",
      "Epoch 971 : Total Loss 16.763\n",
      "Epoch 981 : Total Loss 16.639\n",
      "Epoch 991 : Total Loss 16.516\n",
      "Epoch 1001 : Total Loss 16.395\n",
      "Epoch 1011 : Total Loss 16.274\n",
      "Epoch 1021 : Total Loss 16.154\n",
      "Epoch 1031 : Total Loss 16.035\n",
      "Epoch 1041 : Total Loss 15.916\n",
      "Epoch 1051 : Total Loss 15.797\n",
      "Epoch 1061 : Total Loss 15.680\n",
      "Epoch 1071 : Total Loss 15.562\n",
      "Epoch 1081 : Total Loss 15.445\n",
      "Epoch 1091 : Total Loss 15.328\n",
      "Epoch 1101 : Total Loss 15.211\n",
      "Epoch 1111 : Total Loss 15.094\n",
      "Epoch 1121 : Total Loss 14.978\n",
      "Epoch 1131 : Total Loss 14.861\n",
      "Epoch 1141 : Total Loss 14.744\n",
      "Epoch 1151 : Total Loss 14.628\n",
      "Epoch 1161 : Total Loss 14.511\n",
      "Epoch 1171 : Total Loss 14.394\n",
      "Epoch 1181 : Total Loss 14.276\n",
      "Epoch 1191 : Total Loss 14.159\n",
      "Epoch 1201 : Total Loss 14.041\n",
      "Epoch 1211 : Total Loss 13.922\n",
      "Epoch 1221 : Total Loss 13.804\n",
      "Epoch 1231 : Total Loss 13.685\n",
      "Epoch 1241 : Total Loss 13.565\n",
      "Epoch 1251 : Total Loss 13.446\n",
      "Epoch 1261 : Total Loss 13.325\n",
      "Epoch 1271 : Total Loss 13.205\n",
      "Epoch 1281 : Total Loss 13.083\n",
      "Epoch 1291 : Total Loss 12.962\n",
      "Epoch 1301 : Total Loss 12.840\n",
      "Epoch 1311 : Total Loss 12.717\n",
      "Epoch 1321 : Total Loss 12.594\n",
      "Epoch 1331 : Total Loss 12.470\n",
      "Epoch 1341 : Total Loss 12.346\n",
      "Epoch 1351 : Total Loss 12.221\n",
      "Epoch 1361 : Total Loss 12.096\n",
      "Epoch 1371 : Total Loss 11.971\n",
      "Epoch 1381 : Total Loss 11.845\n",
      "Epoch 1391 : Total Loss 11.719\n",
      "Epoch 1401 : Total Loss 11.592\n",
      "Epoch 1411 : Total Loss 11.465\n",
      "Epoch 1421 : Total Loss 11.338\n",
      "Epoch 1431 : Total Loss 11.211\n",
      "Epoch 1441 : Total Loss 11.083\n",
      "Epoch 1451 : Total Loss 10.955\n",
      "Epoch 1461 : Total Loss 10.826\n",
      "Epoch 1471 : Total Loss 10.698\n",
      "Epoch 1481 : Total Loss 10.570\n",
      "Epoch 1491 : Total Loss 10.441\n",
      "Epoch 1501 : Total Loss 10.313\n",
      "Epoch 1511 : Total Loss 10.184\n",
      "Epoch 1521 : Total Loss 10.056\n",
      "Epoch 1531 : Total Loss 9.927\n",
      "Epoch 1541 : Total Loss 9.799\n",
      "Epoch 1551 : Total Loss 9.671\n",
      "Epoch 1561 : Total Loss 9.544\n",
      "Epoch 1571 : Total Loss 9.417\n",
      "Epoch 1581 : Total Loss 9.290\n",
      "Epoch 1591 : Total Loss 9.164\n",
      "Epoch 1601 : Total Loss 9.038\n",
      "Epoch 1611 : Total Loss 8.913\n",
      "Epoch 1621 : Total Loss 8.788\n",
      "Epoch 1631 : Total Loss 8.664\n",
      "Epoch 1641 : Total Loss 8.541\n",
      "Epoch 1651 : Total Loss 8.419\n",
      "Epoch 1661 : Total Loss 8.297\n",
      "Epoch 1671 : Total Loss 8.177\n",
      "Epoch 1681 : Total Loss 8.057\n",
      "Epoch 1691 : Total Loss 7.939\n",
      "Epoch 1701 : Total Loss 7.821\n",
      "Epoch 1711 : Total Loss 7.705\n",
      "Epoch 1721 : Total Loss 7.590\n",
      "Epoch 1731 : Total Loss 7.476\n",
      "Epoch 1741 : Total Loss 7.363\n",
      "Epoch 1751 : Total Loss 7.252\n",
      "Epoch 1761 : Total Loss 7.142\n",
      "Epoch 1771 : Total Loss 7.033\n",
      "Epoch 1781 : Total Loss 6.926\n",
      "Epoch 1791 : Total Loss 6.820\n",
      "Epoch 1801 : Total Loss 6.716\n",
      "Epoch 1811 : Total Loss 6.614\n",
      "Epoch 1821 : Total Loss 6.513\n",
      "Epoch 1831 : Total Loss 6.414\n",
      "Epoch 1841 : Total Loss 6.316\n",
      "Epoch 1851 : Total Loss 6.220\n",
      "Epoch 1861 : Total Loss 6.126\n",
      "Epoch 1871 : Total Loss 6.033\n",
      "Epoch 1881 : Total Loss 5.942\n",
      "Epoch 1891 : Total Loss 5.853\n",
      "Epoch 1901 : Total Loss 5.766\n",
      "Epoch 1911 : Total Loss 5.680\n",
      "Epoch 1921 : Total Loss 5.597\n",
      "Epoch 1931 : Total Loss 5.515\n",
      "Epoch 1941 : Total Loss 5.435\n",
      "Epoch 1951 : Total Loss 5.356\n",
      "Epoch 1961 : Total Loss 5.280\n",
      "Epoch 1971 : Total Loss 5.205\n",
      "Epoch 1981 : Total Loss 5.132\n",
      "Epoch 1991 : Total Loss 5.061\n",
      "Epoch 2001 : Total Loss 4.991\n",
      "Epoch 2011 : Total Loss 4.923\n",
      "Epoch 2021 : Total Loss 4.857\n",
      "Epoch 2031 : Total Loss 4.793\n",
      "Epoch 2041 : Total Loss 4.731\n",
      "Epoch 2051 : Total Loss 4.670\n",
      "Epoch 2061 : Total Loss 4.610\n",
      "Epoch 2071 : Total Loss 4.553\n",
      "Epoch 2081 : Total Loss 4.497\n",
      "Epoch 2091 : Total Loss 4.443\n",
      "Epoch 2101 : Total Loss 4.390\n",
      "Epoch 2111 : Total Loss 4.339\n",
      "Epoch 2121 : Total Loss 4.289\n",
      "Epoch 2131 : Total Loss 4.241\n",
      "Epoch 2141 : Total Loss 4.194\n",
      "Epoch 2151 : Total Loss 4.149\n",
      "Epoch 2161 : Total Loss 4.105\n",
      "Epoch 2171 : Total Loss 4.063\n",
      "Epoch 2181 : Total Loss 4.022\n",
      "Epoch 2191 : Total Loss 3.982\n",
      "Epoch 2201 : Total Loss 3.944\n",
      "Epoch 2211 : Total Loss 3.906\n",
      "Epoch 2221 : Total Loss 3.871\n",
      "Epoch 2231 : Total Loss 3.836\n",
      "Epoch 2241 : Total Loss 3.802\n",
      "Epoch 2251 : Total Loss 3.770\n",
      "Epoch 2261 : Total Loss 3.738\n",
      "Epoch 2271 : Total Loss 3.708\n",
      "Epoch 2281 : Total Loss 3.679\n",
      "Epoch 2291 : Total Loss 3.651\n",
      "Epoch 2301 : Total Loss 3.624\n",
      "Epoch 2311 : Total Loss 3.597\n",
      "Epoch 2321 : Total Loss 3.572\n",
      "Epoch 2331 : Total Loss 3.548\n",
      "Epoch 2341 : Total Loss 3.524\n",
      "Epoch 2351 : Total Loss 3.502\n",
      "Epoch 2361 : Total Loss 3.480\n",
      "Epoch 2371 : Total Loss 3.459\n",
      "Epoch 2381 : Total Loss 3.438\n",
      "Epoch 2391 : Total Loss 3.419\n",
      "Epoch 2401 : Total Loss 3.400\n",
      "Epoch 2411 : Total Loss 3.382\n",
      "Epoch 2421 : Total Loss 3.364\n",
      "Epoch 2431 : Total Loss 3.347\n",
      "Epoch 2441 : Total Loss 3.331\n",
      "Epoch 2451 : Total Loss 3.315\n",
      "Epoch 2461 : Total Loss 3.300\n",
      "Epoch 2471 : Total Loss 3.286\n",
      "Epoch 2481 : Total Loss 3.272\n",
      "Epoch 2491 : Total Loss 3.258\n",
      "Epoch 2501 : Total Loss 3.245\n",
      "Epoch 2511 : Total Loss 3.233\n",
      "Epoch 2521 : Total Loss 3.221\n",
      "Epoch 2531 : Total Loss 3.209\n",
      "Epoch 2541 : Total Loss 3.198\n",
      "Epoch 2551 : Total Loss 3.187\n",
      "Epoch 2561 : Total Loss 3.177\n",
      "Epoch 2571 : Total Loss 3.167\n",
      "Epoch 2581 : Total Loss 3.157\n",
      "Epoch 2591 : Total Loss 3.148\n",
      "Epoch 2601 : Total Loss 3.139\n",
      "Epoch 2611 : Total Loss 3.130\n",
      "Epoch 2621 : Total Loss 3.122\n",
      "Epoch 2631 : Total Loss 3.114\n",
      "Epoch 2641 : Total Loss 3.106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2651 : Total Loss 3.099\n",
      "Epoch 2661 : Total Loss 3.091\n",
      "Epoch 2671 : Total Loss 3.084\n",
      "Epoch 2681 : Total Loss 3.078\n",
      "Epoch 2691 : Total Loss 3.071\n",
      "Epoch 2701 : Total Loss 3.065\n",
      "Epoch 2711 : Total Loss 3.059\n",
      "Epoch 2721 : Total Loss 3.053\n",
      "Epoch 2731 : Total Loss 3.047\n",
      "Epoch 2741 : Total Loss 3.041\n",
      "Epoch 2751 : Total Loss 3.036\n",
      "Epoch 2761 : Total Loss 3.031\n",
      "Epoch 2771 : Total Loss 3.026\n",
      "Epoch 2781 : Total Loss 3.021\n",
      "Epoch 2791 : Total Loss 3.016\n",
      "Epoch 2801 : Total Loss 3.011\n",
      "Epoch 2811 : Total Loss 3.007\n",
      "Epoch 2821 : Total Loss 3.002\n",
      "Epoch 2831 : Total Loss 2.998\n",
      "Epoch 2841 : Total Loss 2.994\n",
      "Epoch 2851 : Total Loss 2.990\n",
      "Epoch 2861 : Total Loss 2.986\n",
      "Epoch 2871 : Total Loss 2.982\n",
      "Epoch 2881 : Total Loss 2.979\n",
      "Epoch 2891 : Total Loss 2.975\n",
      "Epoch 2901 : Total Loss 2.972\n",
      "Epoch 2911 : Total Loss 2.968\n",
      "Epoch 2921 : Total Loss 2.965\n",
      "Epoch 2931 : Total Loss 2.961\n",
      "Epoch 2941 : Total Loss 2.958\n",
      "Epoch 2951 : Total Loss 2.955\n",
      "Epoch 2961 : Total Loss 2.952\n",
      "Epoch 2971 : Total Loss 2.949\n",
      "Epoch 2981 : Total Loss 2.946\n",
      "Epoch 2991 : Total Loss 2.943\n",
      "Epoch 3001 : Total Loss 2.940\n",
      "Epoch 3011 : Total Loss 2.938\n",
      "Epoch 3021 : Total Loss 2.935\n",
      "Epoch 3031 : Total Loss 2.932\n",
      "Epoch 3041 : Total Loss 2.930\n",
      "Epoch 3051 : Total Loss 2.927\n",
      "Epoch 3061 : Total Loss 2.925\n",
      "Epoch 3071 : Total Loss 2.922\n",
      "Epoch 3081 : Total Loss 2.920\n",
      "Epoch 3091 : Total Loss 2.917\n",
      "Epoch 3101 : Total Loss 2.915\n",
      "Epoch 3111 : Total Loss 2.912\n",
      "Epoch 3121 : Total Loss 2.910\n",
      "Epoch 3131 : Total Loss 2.908\n",
      "Epoch 3141 : Total Loss 2.906\n",
      "Epoch 3151 : Total Loss 2.903\n",
      "Epoch 3161 : Total Loss 2.901\n",
      "Epoch 3171 : Total Loss 2.899\n",
      "Epoch 3181 : Total Loss 2.897\n",
      "Epoch 3191 : Total Loss 2.895\n",
      "Epoch 3201 : Total Loss 2.893\n",
      "Epoch 3211 : Total Loss 2.891\n",
      "Epoch 3221 : Total Loss 2.888\n",
      "Epoch 3231 : Total Loss 2.886\n",
      "Epoch 3241 : Total Loss 2.884\n",
      "Epoch 3251 : Total Loss 2.882\n",
      "Epoch 3261 : Total Loss 2.880\n",
      "Epoch 3271 : Total Loss 2.878\n",
      "Epoch 3281 : Total Loss 2.877\n",
      "Epoch 3291 : Total Loss 2.875\n",
      "Epoch 3301 : Total Loss 2.873\n",
      "Epoch 3311 : Total Loss 2.871\n",
      "Epoch 3321 : Total Loss 2.869\n",
      "Epoch 3331 : Total Loss 2.867\n",
      "Epoch 3341 : Total Loss 2.865\n",
      "Epoch 3351 : Total Loss 2.863\n",
      "Epoch 3361 : Total Loss 2.861\n",
      "Epoch 3371 : Total Loss 2.859\n",
      "Epoch 3381 : Total Loss 2.858\n",
      "Epoch 3391 : Total Loss 2.856\n",
      "Epoch 3401 : Total Loss 2.854\n",
      "Epoch 3411 : Total Loss 2.852\n",
      "Epoch 3421 : Total Loss 2.850\n",
      "Epoch 3431 : Total Loss 2.849\n",
      "Epoch 3441 : Total Loss 2.847\n",
      "Epoch 3451 : Total Loss 2.845\n",
      "Epoch 3461 : Total Loss 2.843\n",
      "Epoch 3471 : Total Loss 2.841\n",
      "Epoch 3481 : Total Loss 2.840\n",
      "Epoch 3491 : Total Loss 2.838\n",
      "Epoch 3501 : Total Loss 2.836\n",
      "Epoch 3511 : Total Loss 2.834\n",
      "Epoch 3521 : Total Loss 2.833\n",
      "Epoch 3531 : Total Loss 2.831\n",
      "Epoch 3541 : Total Loss 2.829\n",
      "Epoch 3551 : Total Loss 2.828\n",
      "Epoch 3561 : Total Loss 2.826\n",
      "Epoch 3571 : Total Loss 2.824\n",
      "Epoch 3581 : Total Loss 2.822\n",
      "Epoch 3591 : Total Loss 2.821\n",
      "Epoch 3601 : Total Loss 2.819\n",
      "Epoch 3611 : Total Loss 2.817\n",
      "Epoch 3621 : Total Loss 2.815\n",
      "Epoch 3631 : Total Loss 2.814\n",
      "Epoch 3641 : Total Loss 2.812\n",
      "Epoch 3651 : Total Loss 2.810\n",
      "Epoch 3661 : Total Loss 2.809\n",
      "Epoch 3671 : Total Loss 2.807\n",
      "Epoch 3681 : Total Loss 2.805\n",
      "Epoch 3691 : Total Loss 2.804\n",
      "Epoch 3701 : Total Loss 2.802\n",
      "Epoch 3711 : Total Loss 2.800\n",
      "Epoch 3721 : Total Loss 2.798\n",
      "Epoch 3731 : Total Loss 2.797\n",
      "Epoch 3741 : Total Loss 2.795\n",
      "Epoch 3751 : Total Loss 2.793\n",
      "Epoch 3761 : Total Loss 2.792\n",
      "Epoch 3771 : Total Loss 2.790\n",
      "Epoch 3781 : Total Loss 2.788\n",
      "Epoch 3791 : Total Loss 2.787\n",
      "Epoch 3801 : Total Loss 2.785\n",
      "Epoch 3811 : Total Loss 2.783\n",
      "Epoch 3821 : Total Loss 2.782\n",
      "Epoch 3831 : Total Loss 2.780\n",
      "Epoch 3841 : Total Loss 2.778\n",
      "Epoch 3851 : Total Loss 2.776\n",
      "Epoch 3861 : Total Loss 2.775\n",
      "Epoch 3871 : Total Loss 2.773\n",
      "Epoch 3881 : Total Loss 2.771\n",
      "Epoch 3891 : Total Loss 2.770\n",
      "Epoch 3901 : Total Loss 2.768\n",
      "Epoch 3911 : Total Loss 2.766\n",
      "Epoch 3921 : Total Loss 2.765\n",
      "Epoch 3931 : Total Loss 2.763\n",
      "Epoch 3941 : Total Loss 2.761\n",
      "Epoch 3951 : Total Loss 2.760\n",
      "Epoch 3961 : Total Loss 2.758\n",
      "Epoch 3971 : Total Loss 2.756\n",
      "Epoch 3981 : Total Loss 2.754\n",
      "Epoch 3991 : Total Loss 2.753\n",
      "Epoch 4001 : Total Loss 2.751\n",
      "Epoch 4011 : Total Loss 2.749\n",
      "Epoch 4021 : Total Loss 2.748\n",
      "Epoch 4031 : Total Loss 2.746\n",
      "Epoch 4041 : Total Loss 2.744\n",
      "Epoch 4051 : Total Loss 2.743\n",
      "Epoch 4061 : Total Loss 2.741\n",
      "Epoch 4071 : Total Loss 2.739\n",
      "Epoch 4081 : Total Loss 2.737\n",
      "Epoch 4091 : Total Loss 2.736\n",
      "Epoch 4101 : Total Loss 2.734\n",
      "Epoch 4111 : Total Loss 2.732\n",
      "Epoch 4121 : Total Loss 2.731\n",
      "Epoch 4131 : Total Loss 2.729\n",
      "Epoch 4141 : Total Loss 2.727\n",
      "Epoch 4151 : Total Loss 2.725\n",
      "Epoch 4161 : Total Loss 2.724\n",
      "Epoch 4171 : Total Loss 2.722\n",
      "Epoch 4181 : Total Loss 2.720\n",
      "Epoch 4191 : Total Loss 2.719\n",
      "Epoch 4201 : Total Loss 2.717\n",
      "Epoch 4211 : Total Loss 2.715\n",
      "Epoch 4221 : Total Loss 2.713\n",
      "Epoch 4231 : Total Loss 2.712\n",
      "Epoch 4241 : Total Loss 2.710\n",
      "Epoch 4251 : Total Loss 2.708\n",
      "Epoch 4261 : Total Loss 2.706\n",
      "Epoch 4271 : Total Loss 2.705\n",
      "Epoch 4281 : Total Loss 2.703\n",
      "Epoch 4291 : Total Loss 2.701\n",
      "Epoch 4301 : Total Loss 2.699\n",
      "Epoch 4311 : Total Loss 2.698\n",
      "Epoch 4321 : Total Loss 2.696\n",
      "Epoch 4331 : Total Loss 2.694\n",
      "Epoch 4341 : Total Loss 2.692\n",
      "Epoch 4351 : Total Loss 2.691\n",
      "Epoch 4361 : Total Loss 2.689\n",
      "Epoch 4371 : Total Loss 2.687\n",
      "Epoch 4381 : Total Loss 2.685\n",
      "Epoch 4391 : Total Loss 2.684\n",
      "Epoch 4401 : Total Loss 2.682\n",
      "Epoch 4411 : Total Loss 2.680\n",
      "Epoch 4421 : Total Loss 2.678\n",
      "Epoch 4431 : Total Loss 2.677\n",
      "Epoch 4441 : Total Loss 2.675\n",
      "Epoch 4451 : Total Loss 2.673\n",
      "Epoch 4461 : Total Loss 2.671\n",
      "Epoch 4471 : Total Loss 2.669\n",
      "Epoch 4481 : Total Loss 2.668\n",
      "Epoch 4491 : Total Loss 2.666\n",
      "Epoch 4501 : Total Loss 2.664\n",
      "Epoch 4511 : Total Loss 2.662\n",
      "Epoch 4521 : Total Loss 2.660\n",
      "Epoch 4531 : Total Loss 2.659\n",
      "Epoch 4541 : Total Loss 2.657\n",
      "Epoch 4551 : Total Loss 2.655\n",
      "Epoch 4561 : Total Loss 2.653\n",
      "Epoch 4571 : Total Loss 2.651\n",
      "Epoch 4581 : Total Loss 2.650\n",
      "Epoch 4591 : Total Loss 2.648\n",
      "Epoch 4601 : Total Loss 2.646\n",
      "Epoch 4611 : Total Loss 2.644\n",
      "Epoch 4621 : Total Loss 2.642\n",
      "Epoch 4631 : Total Loss 2.641\n",
      "Epoch 4641 : Total Loss 2.639\n",
      "Epoch 4651 : Total Loss 2.637\n",
      "Epoch 4661 : Total Loss 2.635\n",
      "Epoch 4671 : Total Loss 2.633\n",
      "Epoch 4681 : Total Loss 2.631\n",
      "Epoch 4691 : Total Loss 2.630\n",
      "Epoch 4701 : Total Loss 2.628\n",
      "Epoch 4711 : Total Loss 2.626\n",
      "Epoch 4721 : Total Loss 2.624\n",
      "Epoch 4731 : Total Loss 2.622\n",
      "Epoch 4741 : Total Loss 2.620\n",
      "Epoch 4751 : Total Loss 2.619\n",
      "Epoch 4761 : Total Loss 2.617\n",
      "Epoch 4771 : Total Loss 2.615\n",
      "Epoch 4781 : Total Loss 2.613\n",
      "Epoch 4791 : Total Loss 2.611\n",
      "Epoch 4801 : Total Loss 2.609\n",
      "Epoch 4811 : Total Loss 2.607\n",
      "Epoch 4821 : Total Loss 2.606\n",
      "Epoch 4831 : Total Loss 2.604\n",
      "Epoch 4841 : Total Loss 2.602\n",
      "Epoch 4851 : Total Loss 2.600\n",
      "Epoch 4861 : Total Loss 2.598\n",
      "Epoch 4871 : Total Loss 2.596\n",
      "Epoch 4881 : Total Loss 2.594\n",
      "Epoch 4891 : Total Loss 2.592\n",
      "Epoch 4901 : Total Loss 2.591\n",
      "Epoch 4911 : Total Loss 2.589\n",
      "Epoch 4921 : Total Loss 2.587\n",
      "Epoch 4931 : Total Loss 2.585\n",
      "Epoch 4941 : Total Loss 2.583\n",
      "Epoch 4951 : Total Loss 2.581\n",
      "Epoch 4961 : Total Loss 2.579\n",
      "Epoch 4971 : Total Loss 2.577\n",
      "Epoch 4981 : Total Loss 2.575\n",
      "Epoch 4991 : Total Loss 2.573\n",
      "U_hat is:\n",
      " [[ 2.05048103  1.25238602]\n",
      " [ 0.93699003  1.17998147]\n",
      " [ 1.3271836   1.04952428]\n",
      " [-0.74675306  2.26241272]]\n",
      "V_hat is:\n",
      " [[ 1.83237981  1.0451009 ]\n",
      " [ 0.22721574  1.51001292]\n",
      " [ 2.05903658  1.05193632]\n",
      " [-0.5466826   2.02986403]]\n",
      "A is:\n",
      " [[5 3 0 1]\n",
      " [3 2 0 0]\n",
      " [0 1 4 2]\n",
      " [1 0 0 5]]\n",
      "A_hat is:\n",
      " [[5.06612981 2.35702065 5.53944581 1.42121104]\n",
      " [2.95012131 1.99468616 3.17056212 1.8829658 ]\n",
      " [3.52876319 1.88635223 3.83675229 1.4048434 ]\n",
      " [0.99611434 3.24659839 0.84232225 5.00062711]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [5,3,0,1],\n",
    "    [3,2,0,0],\n",
    "    [0,1,4,2],\n",
    "    [1,0,0,5]\n",
    "])\n",
    "\n",
    "m, n = A.shape[0], A.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "U_1 = np.random.rand(m, user_features)\n",
    "V_1 = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U_1)\n",
    "print('V is:\\n', V_1)\n",
    "\n",
    "# perform factorization\n",
    "\n",
    "U_hat, V_hat = matrix_factorization(A, U_1, V_1, user_features, 5000)\n",
    "\n",
    "A_hat = np.dot(U_hat, V_hat.T)\n",
    "\n",
    "print('U_hat is:\\n', U_hat)\n",
    "print('V_hat is:\\n', V_hat)\n",
    "\n",
    "## check if the product results in something close to A\n",
    "print('A is:\\n', A)\n",
    "print('A_hat is:\\n', A_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c550535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of all predictions is 0.176.\n",
      "The root mean squared error of all predictions is 0.420.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.0, 3.0, 2.0, 4.0, 3.0, 1.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse, rmse = evaluation_metrics(A, A_hat)\n",
    "\n",
    "print(f'The mean squared error of all predictions is {mse:.3f}.')\n",
    "print(f'The root mean squared error of all predictions is {rmse:.3f}.')\n",
    "\n",
    "mat_predict(A, A_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a0fb2",
   "metadata": {},
   "source": [
    "**Try a Large Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b3a5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "# A = np.random.randint(0, 6, size=(100,100))\n",
    "\n",
    "# m, n = A.shape[0], A.shape[1]\n",
    "\n",
    "# user_features = 2\n",
    "\n",
    "# U_2 = np.random.rand(m, user_features)\n",
    "# V_2 = np.random.rand(n, user_features)\n",
    "\n",
    "# # check entries of matrices\n",
    "\n",
    "# print('U is:\\n', U_2)\n",
    "# print('V is:\\n', V_2)\n",
    "\n",
    "# # perform factorization\n",
    "\n",
    "# U_hat, V_hat = matrix_factorization(A, U_2, V_2, user_features, 5000)\n",
    "\n",
    "# A_hat = np.dot(U_hat, V_hat.T)\n",
    "\n",
    "# print('U_hat is:\\n', U_hat)\n",
    "# print('V_hat is:\\n', V_hat)\n",
    "\n",
    "# ## check if the product results in something close to A\n",
    "# print('A is:\\n', A)\n",
    "# print('A_hat is:\\n', A_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14026e",
   "metadata": {},
   "source": [
    "**Note: This takes a really long time to train and evaluate. It took over 15 minutes to train and evaluate a 100 by 100 matrix with 2 latent features. Most datasets that we will put in in a business situation will be WELL over that amount so this current implementation is not feasible for large datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a781c",
   "metadata": {},
   "source": [
    "## Matrix Factorization Implementation for Sparse Matrices\n",
    "\n",
    "We will investigate how to perform matrix factorization for sparse matrices, as most matrices we will work with for the purpose of matrix factorization are large and sparse (if it were large but not sparse, we would have most of our information anyways, so this technique wouldn't be super useful).\n",
    "\n",
    "### Sparse Matrices\n",
    "\n",
    "The definition of a sparse matrix is very imprecise, but for our purposes, we define a sparse matrix as any matrix that stores a 0 (or any other value that represents the absence of meaningful data) in at most $\\max(m,n)$ entries in an $m\\times n$ matrix.\n",
    "\n",
    "We can use scipy's csr_matrix class to get the nonzero elements of a sparse matrix. Let us use a toy example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbfc2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7136888b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 5, 0, 0, 1, 0],\n",
       "       [0, 3, 0, 0, 0, 0, 1],\n",
       "       [4, 0, 0, 2, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0,1,0,0,0,0,0],\n",
    "    [0,0,5,0,0,1,0],\n",
    "    [0,3,0,0,0,0,1],\n",
    "    [4,0,0,2,0,0,0]    \n",
    "          ])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39f0e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t5\n",
      "  (0, 5)\t1\n",
      "  (0, 1)\t3\n",
      "  (0, 6)\t1\n",
      "  (0, 0)\t4\n",
      "  (0, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "# print items in csr_matrix\n",
    "\n",
    "for item in csr_matrix(x):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a380e",
   "metadata": {},
   "source": [
    "So a csr_matrix object only stores nonzero elements (the meaningful data entries) in memory!\n",
    "\n",
    "This is important for our matrix factorization algorithm implementation here, because recall that **our loss computation only involves NONZERO elements** anyways! So we do not need any information about the matrix with the 0 entries!\n",
    "\n",
    "We can pick out the nonzero indices of the matrix with **csr_matrix's nonzero method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8189af01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 2, 2, 3, 3], dtype=int32),\n",
       " array([1, 2, 5, 1, 6, 0, 3], dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix(x).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc764a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (1, 5), (2, 1), (2, 6), (3, 0), (3, 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index the nonzero elements through extracting the indices\n",
    "li_ind = []\n",
    "\n",
    "for x_ind, y_ind in zip(csr_matrix(x).nonzero()[0], csr_matrix(x).nonzero()[1]):\n",
    "    li_ind.append((x_ind, y_ind))\n",
    "\n",
    "li_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b3e2909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# access all elements of our original array\n",
    "for i, j in zip(csr_matrix(x).nonzero()[0], csr_matrix(x).nonzero()[1]):\n",
    "    print(x[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a93b2d",
   "metadata": {},
   "source": [
    "### Matrix Factorization for Sparse Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c7d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is different from the previous one because we do not need to traverse each array separately\n",
    "# we isolate and extract all of the nonzero entries from the matrix in linear time (at worst O(n) for ground truth mxn)\n",
    "# and then we only loop through epochs and the number of latent features\n",
    "\n",
    "# ARGUMENTS\n",
    "# A - ground truth matrix (sparse numpy array)\n",
    "# U - product 1\n",
    "# V - product 2\n",
    "# number of features\n",
    "# epochs - number of epochs to train\n",
    "# lambda - learning rate\n",
    "# beta - regularization parameter\n",
    "\n",
    "def matrix_factorization_sparse(A, U, V, k, steps, lmbda=0.0002, beta = 0.02):\n",
    "    # transpose V to make matrix multiplication work\n",
    "    V = V.T\n",
    "    \n",
    "    # convert A, a sparse numpy array, into a sparse scipy matrix object\n",
    "    sparse_A = csr_matrix(A)\n",
    "    \n",
    "    for epoch in range(steps):\n",
    "        # initialize total loss\n",
    "        e = 0\n",
    "        for i, j in zip(sparse_A.nonzero()[0], sparse_A.nonzero()[1]):\n",
    "            # component-wise error\n",
    "            eij = A[i,j] - np.dot(U[i,:], V[:,j])\n",
    "            # total error\n",
    "            e += (eij**2)\n",
    "            \n",
    "            for K in range(k):\n",
    "                # gradient descent step for each entry, with regularization term\n",
    "                U[i,K] = U[i,K] + lmbda * (2 * eij * V[K,j] - beta*U[i,K])\n",
    "                V[K,j] = V[K,j] + lmbda * (2 * eij * U[i,K] - beta*V[K,j])\n",
    "                \n",
    "                # sum up total loss with regularization terms\n",
    "                e += 0.5 * beta * (U[i][K]**2 + V[K][j]**2)\n",
    "\n",
    "        # give print output of training (print only every 10 epochs)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} : Total Loss {e:.3f}')\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return U, V.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf98739",
   "metadata": {},
   "source": [
    "### Test on a Small Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "716eab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.9600173  0.69951205]\n",
      " [0.99986729 0.2200673 ]\n",
      " [0.36105635 0.73984099]\n",
      " [0.99645573 0.31634698]\n",
      " [0.13654458 0.38398001]]\n",
      "V is:\n",
      " [[0.32051928 0.36641475]\n",
      " [0.70965156 0.90014243]\n",
      " [0.53411544 0.24729376]\n",
      " [0.67180656 0.56172911]\n",
      " [0.54255988 0.8934476 ]]\n",
      "Epoch 1 : Total Loss 99.448\n",
      "Epoch 11 : Total Loss 96.944\n",
      "Epoch 21 : Total Loss 94.384\n",
      "Epoch 31 : Total Loss 91.770\n",
      "Epoch 41 : Total Loss 89.107\n",
      "Epoch 51 : Total Loss 86.400\n",
      "Epoch 61 : Total Loss 83.655\n",
      "Epoch 71 : Total Loss 80.879\n",
      "Epoch 81 : Total Loss 78.078\n",
      "Epoch 91 : Total Loss 75.261\n",
      "Epoch 101 : Total Loss 72.436\n",
      "Epoch 111 : Total Loss 69.612\n",
      "Epoch 121 : Total Loss 66.798\n",
      "Epoch 131 : Total Loss 64.003\n",
      "Epoch 141 : Total Loss 61.238\n",
      "Epoch 151 : Total Loss 58.510\n",
      "Epoch 161 : Total Loss 55.830\n",
      "Epoch 171 : Total Loss 53.205\n",
      "Epoch 181 : Total Loss 50.644\n",
      "Epoch 191 : Total Loss 48.155\n",
      "Epoch 201 : Total Loss 45.745\n",
      "Epoch 211 : Total Loss 43.419\n",
      "Epoch 221 : Total Loss 41.183\n",
      "Epoch 231 : Total Loss 39.040\n",
      "Epoch 241 : Total Loss 36.995\n",
      "Epoch 251 : Total Loss 35.049\n",
      "Epoch 261 : Total Loss 33.205\n",
      "Epoch 271 : Total Loss 31.461\n",
      "Epoch 281 : Total Loss 29.818\n",
      "Epoch 291 : Total Loss 28.275\n",
      "Epoch 301 : Total Loss 26.829\n",
      "Epoch 311 : Total Loss 25.479\n",
      "Epoch 321 : Total Loss 24.220\n",
      "Epoch 331 : Total Loss 23.050\n",
      "Epoch 341 : Total Loss 21.965\n",
      "Epoch 351 : Total Loss 20.960\n",
      "Epoch 361 : Total Loss 20.032\n",
      "Epoch 371 : Total Loss 19.175\n",
      "Epoch 381 : Total Loss 18.385\n",
      "Epoch 391 : Total Loss 17.658\n",
      "Epoch 401 : Total Loss 16.989\n",
      "Epoch 411 : Total Loss 16.375\n",
      "Epoch 421 : Total Loss 15.810\n",
      "Epoch 431 : Total Loss 15.292\n",
      "Epoch 441 : Total Loss 14.816\n",
      "Epoch 451 : Total Loss 14.379\n",
      "Epoch 461 : Total Loss 13.977\n",
      "Epoch 471 : Total Loss 13.608\n",
      "Epoch 481 : Total Loss 13.269\n",
      "Epoch 491 : Total Loss 12.956\n",
      "Epoch 501 : Total Loss 12.668\n",
      "Epoch 511 : Total Loss 12.402\n",
      "Epoch 521 : Total Loss 12.156\n",
      "Epoch 531 : Total Loss 11.929\n",
      "Epoch 541 : Total Loss 11.718\n",
      "Epoch 551 : Total Loss 11.523\n",
      "Epoch 561 : Total Loss 11.341\n",
      "Epoch 571 : Total Loss 11.171\n",
      "Epoch 581 : Total Loss 11.013\n",
      "Epoch 591 : Total Loss 10.865\n",
      "Epoch 601 : Total Loss 10.726\n",
      "Epoch 611 : Total Loss 10.595\n",
      "Epoch 621 : Total Loss 10.472\n",
      "Epoch 631 : Total Loss 10.356\n",
      "Epoch 641 : Total Loss 10.247\n",
      "Epoch 651 : Total Loss 10.143\n",
      "Epoch 661 : Total Loss 10.045\n",
      "Epoch 671 : Total Loss 9.951\n",
      "Epoch 681 : Total Loss 9.862\n",
      "Epoch 691 : Total Loss 9.777\n",
      "Epoch 701 : Total Loss 9.695\n",
      "Epoch 711 : Total Loss 9.617\n",
      "Epoch 721 : Total Loss 9.542\n",
      "Epoch 731 : Total Loss 9.470\n",
      "Epoch 741 : Total Loss 9.401\n",
      "Epoch 751 : Total Loss 9.334\n",
      "Epoch 761 : Total Loss 9.269\n",
      "Epoch 771 : Total Loss 9.206\n",
      "Epoch 781 : Total Loss 9.146\n",
      "Epoch 791 : Total Loss 9.087\n",
      "Epoch 801 : Total Loss 9.030\n",
      "Epoch 811 : Total Loss 8.974\n",
      "Epoch 821 : Total Loss 8.920\n",
      "Epoch 831 : Total Loss 8.868\n",
      "Epoch 841 : Total Loss 8.816\n",
      "Epoch 851 : Total Loss 8.766\n",
      "Epoch 861 : Total Loss 8.717\n",
      "Epoch 871 : Total Loss 8.669\n",
      "Epoch 881 : Total Loss 8.622\n",
      "Epoch 891 : Total Loss 8.575\n",
      "Epoch 901 : Total Loss 8.530\n",
      "Epoch 911 : Total Loss 8.486\n",
      "Epoch 921 : Total Loss 8.442\n",
      "Epoch 931 : Total Loss 8.399\n",
      "Epoch 941 : Total Loss 8.357\n",
      "Epoch 951 : Total Loss 8.316\n",
      "Epoch 961 : Total Loss 8.275\n",
      "Epoch 971 : Total Loss 8.234\n",
      "Epoch 981 : Total Loss 8.195\n",
      "Epoch 991 : Total Loss 8.156\n",
      "Epoch 1001 : Total Loss 8.117\n",
      "Epoch 1011 : Total Loss 8.079\n",
      "Epoch 1021 : Total Loss 8.042\n",
      "Epoch 1031 : Total Loss 8.005\n",
      "Epoch 1041 : Total Loss 7.968\n",
      "Epoch 1051 : Total Loss 7.932\n",
      "Epoch 1061 : Total Loss 7.896\n",
      "Epoch 1071 : Total Loss 7.861\n",
      "Epoch 1081 : Total Loss 7.826\n",
      "Epoch 1091 : Total Loss 7.791\n",
      "Epoch 1101 : Total Loss 7.757\n",
      "Epoch 1111 : Total Loss 7.724\n",
      "Epoch 1121 : Total Loss 7.690\n",
      "Epoch 1131 : Total Loss 7.657\n",
      "Epoch 1141 : Total Loss 7.625\n",
      "Epoch 1151 : Total Loss 7.592\n",
      "Epoch 1161 : Total Loss 7.560\n",
      "Epoch 1171 : Total Loss 7.529\n",
      "Epoch 1181 : Total Loss 7.497\n",
      "Epoch 1191 : Total Loss 7.467\n",
      "Epoch 1201 : Total Loss 7.436\n",
      "Epoch 1211 : Total Loss 7.406\n",
      "Epoch 1221 : Total Loss 7.376\n",
      "Epoch 1231 : Total Loss 7.346\n",
      "Epoch 1241 : Total Loss 7.317\n",
      "Epoch 1251 : Total Loss 7.288\n",
      "Epoch 1261 : Total Loss 7.259\n",
      "Epoch 1271 : Total Loss 7.230\n",
      "Epoch 1281 : Total Loss 7.202\n",
      "Epoch 1291 : Total Loss 7.174\n",
      "Epoch 1301 : Total Loss 7.147\n",
      "Epoch 1311 : Total Loss 7.120\n",
      "Epoch 1321 : Total Loss 7.093\n",
      "Epoch 1331 : Total Loss 7.066\n",
      "Epoch 1341 : Total Loss 7.040\n",
      "Epoch 1351 : Total Loss 7.014\n",
      "Epoch 1361 : Total Loss 6.988\n",
      "Epoch 1371 : Total Loss 6.962\n",
      "Epoch 1381 : Total Loss 6.937\n",
      "Epoch 1391 : Total Loss 6.912\n",
      "Epoch 1401 : Total Loss 6.888\n",
      "Epoch 1411 : Total Loss 6.864\n",
      "Epoch 1421 : Total Loss 6.840\n",
      "Epoch 1431 : Total Loss 6.816\n",
      "Epoch 1441 : Total Loss 6.793\n",
      "Epoch 1451 : Total Loss 6.769\n",
      "Epoch 1461 : Total Loss 6.747\n",
      "Epoch 1471 : Total Loss 6.724\n",
      "Epoch 1481 : Total Loss 6.702\n",
      "Epoch 1491 : Total Loss 6.680\n",
      "Epoch 1501 : Total Loss 6.658\n",
      "Epoch 1511 : Total Loss 6.637\n",
      "Epoch 1521 : Total Loss 6.616\n",
      "Epoch 1531 : Total Loss 6.595\n",
      "Epoch 1541 : Total Loss 6.574\n",
      "Epoch 1551 : Total Loss 6.554\n",
      "Epoch 1561 : Total Loss 6.534\n",
      "Epoch 1571 : Total Loss 6.515\n",
      "Epoch 1581 : Total Loss 6.495\n",
      "Epoch 1591 : Total Loss 6.476\n",
      "Epoch 1601 : Total Loss 6.457\n",
      "Epoch 1611 : Total Loss 6.439\n",
      "Epoch 1621 : Total Loss 6.421\n",
      "Epoch 1631 : Total Loss 6.403\n",
      "Epoch 1641 : Total Loss 6.385\n",
      "Epoch 1651 : Total Loss 6.367\n",
      "Epoch 1661 : Total Loss 6.350\n",
      "Epoch 1671 : Total Loss 6.333\n",
      "Epoch 1681 : Total Loss 6.317\n",
      "Epoch 1691 : Total Loss 6.300\n",
      "Epoch 1701 : Total Loss 6.284\n",
      "Epoch 1711 : Total Loss 6.268\n",
      "Epoch 1721 : Total Loss 6.253\n",
      "Epoch 1731 : Total Loss 6.238\n",
      "Epoch 1741 : Total Loss 6.223\n",
      "Epoch 1751 : Total Loss 6.208\n",
      "Epoch 1761 : Total Loss 6.193\n",
      "Epoch 1771 : Total Loss 6.179\n",
      "Epoch 1781 : Total Loss 6.165\n",
      "Epoch 1791 : Total Loss 6.151\n",
      "Epoch 1801 : Total Loss 6.137\n",
      "Epoch 1811 : Total Loss 6.124\n",
      "Epoch 1821 : Total Loss 6.111\n",
      "Epoch 1831 : Total Loss 6.098\n",
      "Epoch 1841 : Total Loss 6.086\n",
      "Epoch 1851 : Total Loss 6.073\n",
      "Epoch 1861 : Total Loss 6.061\n",
      "Epoch 1871 : Total Loss 6.049\n",
      "Epoch 1881 : Total Loss 6.037\n",
      "Epoch 1891 : Total Loss 6.026\n",
      "Epoch 1901 : Total Loss 6.015\n",
      "Epoch 1911 : Total Loss 6.004\n",
      "Epoch 1921 : Total Loss 5.993\n",
      "Epoch 1931 : Total Loss 5.982\n",
      "Epoch 1941 : Total Loss 5.972\n",
      "Epoch 1951 : Total Loss 5.962\n",
      "Epoch 1961 : Total Loss 5.952\n",
      "Epoch 1971 : Total Loss 5.942\n",
      "Epoch 1981 : Total Loss 5.932\n",
      "Epoch 1991 : Total Loss 5.923\n",
      "Epoch 2001 : Total Loss 5.914\n",
      "Epoch 2011 : Total Loss 5.905\n",
      "Epoch 2021 : Total Loss 5.896\n",
      "Epoch 2031 : Total Loss 5.887\n",
      "Epoch 2041 : Total Loss 5.878\n",
      "Epoch 2051 : Total Loss 5.870\n",
      "Epoch 2061 : Total Loss 5.862\n",
      "Epoch 2071 : Total Loss 5.854\n",
      "Epoch 2081 : Total Loss 5.846\n",
      "Epoch 2091 : Total Loss 5.839\n",
      "Epoch 2101 : Total Loss 5.831\n",
      "Epoch 2111 : Total Loss 5.824\n",
      "Epoch 2121 : Total Loss 5.817\n",
      "Epoch 2131 : Total Loss 5.810\n",
      "Epoch 2141 : Total Loss 5.803\n",
      "Epoch 2151 : Total Loss 5.796\n",
      "Epoch 2161 : Total Loss 5.790\n",
      "Epoch 2171 : Total Loss 5.783\n",
      "Epoch 2181 : Total Loss 5.777\n",
      "Epoch 2191 : Total Loss 5.771\n",
      "Epoch 2201 : Total Loss 5.765\n",
      "Epoch 2211 : Total Loss 5.759\n",
      "Epoch 2221 : Total Loss 5.753\n",
      "Epoch 2231 : Total Loss 5.747\n",
      "Epoch 2241 : Total Loss 5.742\n",
      "Epoch 2251 : Total Loss 5.737\n",
      "Epoch 2261 : Total Loss 5.731\n",
      "Epoch 2271 : Total Loss 5.726\n",
      "Epoch 2281 : Total Loss 5.721\n",
      "Epoch 2291 : Total Loss 5.716\n",
      "Epoch 2301 : Total Loss 5.712\n",
      "Epoch 2311 : Total Loss 5.707\n",
      "Epoch 2321 : Total Loss 5.702\n",
      "Epoch 2331 : Total Loss 5.698\n",
      "Epoch 2341 : Total Loss 5.693\n",
      "Epoch 2351 : Total Loss 5.689\n",
      "Epoch 2361 : Total Loss 5.685\n",
      "Epoch 2371 : Total Loss 5.681\n",
      "Epoch 2381 : Total Loss 5.677\n",
      "Epoch 2391 : Total Loss 5.673\n",
      "Epoch 2401 : Total Loss 5.669\n",
      "Epoch 2411 : Total Loss 5.666\n",
      "Epoch 2421 : Total Loss 5.662\n",
      "Epoch 2431 : Total Loss 5.658\n",
      "Epoch 2441 : Total Loss 5.655\n",
      "Epoch 2451 : Total Loss 5.652\n",
      "Epoch 2461 : Total Loss 5.648\n",
      "Epoch 2471 : Total Loss 5.645\n",
      "Epoch 2481 : Total Loss 5.642\n",
      "Epoch 2491 : Total Loss 5.639\n",
      "Epoch 2501 : Total Loss 5.636\n",
      "Epoch 2511 : Total Loss 5.633\n",
      "Epoch 2521 : Total Loss 5.630\n",
      "Epoch 2531 : Total Loss 5.627\n",
      "Epoch 2541 : Total Loss 5.624\n",
      "Epoch 2551 : Total Loss 5.622\n",
      "Epoch 2561 : Total Loss 5.619\n",
      "Epoch 2571 : Total Loss 5.617\n",
      "Epoch 2581 : Total Loss 5.614\n",
      "Epoch 2591 : Total Loss 5.612\n",
      "Epoch 2601 : Total Loss 5.609\n",
      "Epoch 2611 : Total Loss 5.607\n",
      "Epoch 2621 : Total Loss 5.605\n",
      "Epoch 2631 : Total Loss 5.602\n",
      "Epoch 2641 : Total Loss 5.600\n",
      "Epoch 2651 : Total Loss 5.598\n",
      "Epoch 2661 : Total Loss 5.596\n",
      "Epoch 2671 : Total Loss 5.594\n",
      "Epoch 2681 : Total Loss 5.592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2691 : Total Loss 5.590\n",
      "Epoch 2701 : Total Loss 5.588\n",
      "Epoch 2711 : Total Loss 5.586\n",
      "Epoch 2721 : Total Loss 5.585\n",
      "Epoch 2731 : Total Loss 5.583\n",
      "Epoch 2741 : Total Loss 5.581\n",
      "Epoch 2751 : Total Loss 5.579\n",
      "Epoch 2761 : Total Loss 5.578\n",
      "Epoch 2771 : Total Loss 5.576\n",
      "Epoch 2781 : Total Loss 5.575\n",
      "Epoch 2791 : Total Loss 5.573\n",
      "Epoch 2801 : Total Loss 5.571\n",
      "Epoch 2811 : Total Loss 5.570\n",
      "Epoch 2821 : Total Loss 5.569\n",
      "Epoch 2831 : Total Loss 5.567\n",
      "Epoch 2841 : Total Loss 5.566\n",
      "Epoch 2851 : Total Loss 5.564\n",
      "Epoch 2861 : Total Loss 5.563\n",
      "Epoch 2871 : Total Loss 5.562\n",
      "Epoch 2881 : Total Loss 5.561\n",
      "Epoch 2891 : Total Loss 5.559\n",
      "Epoch 2901 : Total Loss 5.558\n",
      "Epoch 2911 : Total Loss 5.557\n",
      "Epoch 2921 : Total Loss 5.556\n",
      "Epoch 2931 : Total Loss 5.555\n",
      "Epoch 2941 : Total Loss 5.553\n",
      "Epoch 2951 : Total Loss 5.552\n",
      "Epoch 2961 : Total Loss 5.551\n",
      "Epoch 2971 : Total Loss 5.550\n",
      "Epoch 2981 : Total Loss 5.549\n",
      "Epoch 2991 : Total Loss 5.548\n",
      "Epoch 3001 : Total Loss 5.547\n",
      "Epoch 3011 : Total Loss 5.546\n",
      "Epoch 3021 : Total Loss 5.545\n",
      "Epoch 3031 : Total Loss 5.544\n",
      "Epoch 3041 : Total Loss 5.544\n",
      "Epoch 3051 : Total Loss 5.543\n",
      "Epoch 3061 : Total Loss 5.542\n",
      "Epoch 3071 : Total Loss 5.541\n",
      "Epoch 3081 : Total Loss 5.540\n",
      "Epoch 3091 : Total Loss 5.539\n",
      "Epoch 3101 : Total Loss 5.538\n",
      "Epoch 3111 : Total Loss 5.538\n",
      "Epoch 3121 : Total Loss 5.537\n",
      "Epoch 3131 : Total Loss 5.536\n",
      "Epoch 3141 : Total Loss 5.535\n",
      "Epoch 3151 : Total Loss 5.535\n",
      "Epoch 3161 : Total Loss 5.534\n",
      "Epoch 3171 : Total Loss 5.533\n",
      "Epoch 3181 : Total Loss 5.533\n",
      "Epoch 3191 : Total Loss 5.532\n",
      "Epoch 3201 : Total Loss 5.531\n",
      "Epoch 3211 : Total Loss 5.531\n",
      "Epoch 3221 : Total Loss 5.530\n",
      "Epoch 3231 : Total Loss 5.529\n",
      "Epoch 3241 : Total Loss 5.529\n",
      "Epoch 3251 : Total Loss 5.528\n",
      "Epoch 3261 : Total Loss 5.527\n",
      "Epoch 3271 : Total Loss 5.527\n",
      "Epoch 3281 : Total Loss 5.526\n",
      "Epoch 3291 : Total Loss 5.526\n",
      "Epoch 3301 : Total Loss 5.525\n",
      "Epoch 3311 : Total Loss 5.525\n",
      "Epoch 3321 : Total Loss 5.524\n",
      "Epoch 3331 : Total Loss 5.524\n",
      "Epoch 3341 : Total Loss 5.523\n",
      "Epoch 3351 : Total Loss 5.523\n",
      "Epoch 3361 : Total Loss 5.522\n",
      "Epoch 3371 : Total Loss 5.522\n",
      "Epoch 3381 : Total Loss 5.521\n",
      "Epoch 3391 : Total Loss 5.521\n",
      "Epoch 3401 : Total Loss 5.520\n",
      "Epoch 3411 : Total Loss 5.520\n",
      "Epoch 3421 : Total Loss 5.519\n",
      "Epoch 3431 : Total Loss 5.519\n",
      "Epoch 3441 : Total Loss 5.518\n",
      "Epoch 3451 : Total Loss 5.518\n",
      "Epoch 3461 : Total Loss 5.517\n",
      "Epoch 3471 : Total Loss 5.517\n",
      "Epoch 3481 : Total Loss 5.517\n",
      "Epoch 3491 : Total Loss 5.516\n",
      "Epoch 3501 : Total Loss 5.516\n",
      "Epoch 3511 : Total Loss 5.515\n",
      "Epoch 3521 : Total Loss 5.515\n",
      "Epoch 3531 : Total Loss 5.515\n",
      "Epoch 3541 : Total Loss 5.514\n",
      "Epoch 3551 : Total Loss 5.514\n",
      "Epoch 3561 : Total Loss 5.513\n",
      "Epoch 3571 : Total Loss 5.513\n",
      "Epoch 3581 : Total Loss 5.513\n",
      "Epoch 3591 : Total Loss 5.512\n",
      "Epoch 3601 : Total Loss 5.512\n",
      "Epoch 3611 : Total Loss 5.512\n",
      "Epoch 3621 : Total Loss 5.511\n",
      "Epoch 3631 : Total Loss 5.511\n",
      "Epoch 3641 : Total Loss 5.511\n",
      "Epoch 3651 : Total Loss 5.510\n",
      "Epoch 3661 : Total Loss 5.510\n",
      "Epoch 3671 : Total Loss 5.510\n",
      "Epoch 3681 : Total Loss 5.509\n",
      "Epoch 3691 : Total Loss 5.509\n",
      "Epoch 3701 : Total Loss 5.509\n",
      "Epoch 3711 : Total Loss 5.508\n",
      "Epoch 3721 : Total Loss 5.508\n",
      "Epoch 3731 : Total Loss 5.508\n",
      "Epoch 3741 : Total Loss 5.508\n",
      "Epoch 3751 : Total Loss 5.507\n",
      "Epoch 3761 : Total Loss 5.507\n",
      "Epoch 3771 : Total Loss 5.507\n",
      "Epoch 3781 : Total Loss 5.506\n",
      "Epoch 3791 : Total Loss 5.506\n",
      "Epoch 3801 : Total Loss 5.506\n",
      "Epoch 3811 : Total Loss 5.506\n",
      "Epoch 3821 : Total Loss 5.505\n",
      "Epoch 3831 : Total Loss 5.505\n",
      "Epoch 3841 : Total Loss 5.505\n",
      "Epoch 3851 : Total Loss 5.505\n",
      "Epoch 3861 : Total Loss 5.504\n",
      "Epoch 3871 : Total Loss 5.504\n",
      "Epoch 3881 : Total Loss 5.504\n",
      "Epoch 3891 : Total Loss 5.504\n",
      "Epoch 3901 : Total Loss 5.503\n",
      "Epoch 3911 : Total Loss 5.503\n",
      "Epoch 3921 : Total Loss 5.503\n",
      "Epoch 3931 : Total Loss 5.503\n",
      "Epoch 3941 : Total Loss 5.502\n",
      "Epoch 3951 : Total Loss 5.502\n",
      "Epoch 3961 : Total Loss 5.502\n",
      "Epoch 3971 : Total Loss 5.502\n",
      "Epoch 3981 : Total Loss 5.501\n",
      "Epoch 3991 : Total Loss 5.501\n",
      "Epoch 4001 : Total Loss 5.501\n",
      "Epoch 4011 : Total Loss 5.501\n",
      "Epoch 4021 : Total Loss 5.501\n",
      "Epoch 4031 : Total Loss 5.500\n",
      "Epoch 4041 : Total Loss 5.500\n",
      "Epoch 4051 : Total Loss 5.500\n",
      "Epoch 4061 : Total Loss 5.500\n",
      "Epoch 4071 : Total Loss 5.499\n",
      "Epoch 4081 : Total Loss 5.499\n",
      "Epoch 4091 : Total Loss 5.499\n",
      "Epoch 4101 : Total Loss 5.499\n",
      "Epoch 4111 : Total Loss 5.499\n",
      "Epoch 4121 : Total Loss 5.498\n",
      "Epoch 4131 : Total Loss 5.498\n",
      "Epoch 4141 : Total Loss 5.498\n",
      "Epoch 4151 : Total Loss 5.498\n",
      "Epoch 4161 : Total Loss 5.498\n",
      "Epoch 4171 : Total Loss 5.497\n",
      "Epoch 4181 : Total Loss 5.497\n",
      "Epoch 4191 : Total Loss 5.497\n",
      "Epoch 4201 : Total Loss 5.497\n",
      "Epoch 4211 : Total Loss 5.497\n",
      "Epoch 4221 : Total Loss 5.497\n",
      "Epoch 4231 : Total Loss 5.496\n",
      "Epoch 4241 : Total Loss 5.496\n",
      "Epoch 4251 : Total Loss 5.496\n",
      "Epoch 4261 : Total Loss 5.496\n",
      "Epoch 4271 : Total Loss 5.496\n",
      "Epoch 4281 : Total Loss 5.495\n",
      "Epoch 4291 : Total Loss 5.495\n",
      "Epoch 4301 : Total Loss 5.495\n",
      "Epoch 4311 : Total Loss 5.495\n",
      "Epoch 4321 : Total Loss 5.495\n",
      "Epoch 4331 : Total Loss 5.495\n",
      "Epoch 4341 : Total Loss 5.494\n",
      "Epoch 4351 : Total Loss 5.494\n",
      "Epoch 4361 : Total Loss 5.494\n",
      "Epoch 4371 : Total Loss 5.494\n",
      "Epoch 4381 : Total Loss 5.494\n",
      "Epoch 4391 : Total Loss 5.494\n",
      "Epoch 4401 : Total Loss 5.493\n",
      "Epoch 4411 : Total Loss 5.493\n",
      "Epoch 4421 : Total Loss 5.493\n",
      "Epoch 4431 : Total Loss 5.493\n",
      "Epoch 4441 : Total Loss 5.493\n",
      "Epoch 4451 : Total Loss 5.493\n",
      "Epoch 4461 : Total Loss 5.492\n",
      "Epoch 4471 : Total Loss 5.492\n",
      "Epoch 4481 : Total Loss 5.492\n",
      "Epoch 4491 : Total Loss 5.492\n",
      "Epoch 4501 : Total Loss 5.492\n",
      "Epoch 4511 : Total Loss 5.492\n",
      "Epoch 4521 : Total Loss 5.492\n",
      "Epoch 4531 : Total Loss 5.491\n",
      "Epoch 4541 : Total Loss 5.491\n",
      "Epoch 4551 : Total Loss 5.491\n",
      "Epoch 4561 : Total Loss 5.491\n",
      "Epoch 4571 : Total Loss 5.491\n",
      "Epoch 4581 : Total Loss 5.491\n",
      "Epoch 4591 : Total Loss 5.490\n",
      "Epoch 4601 : Total Loss 5.490\n",
      "Epoch 4611 : Total Loss 5.490\n",
      "Epoch 4621 : Total Loss 5.490\n",
      "Epoch 4631 : Total Loss 5.490\n",
      "Epoch 4641 : Total Loss 5.490\n",
      "Epoch 4651 : Total Loss 5.490\n",
      "Epoch 4661 : Total Loss 5.489\n",
      "Epoch 4671 : Total Loss 5.489\n",
      "Epoch 4681 : Total Loss 5.489\n",
      "Epoch 4691 : Total Loss 5.489\n",
      "Epoch 4701 : Total Loss 5.489\n",
      "Epoch 4711 : Total Loss 5.489\n",
      "Epoch 4721 : Total Loss 5.489\n",
      "Epoch 4731 : Total Loss 5.488\n",
      "Epoch 4741 : Total Loss 5.488\n",
      "Epoch 4751 : Total Loss 5.488\n",
      "Epoch 4761 : Total Loss 5.488\n",
      "Epoch 4771 : Total Loss 5.488\n",
      "Epoch 4781 : Total Loss 5.488\n",
      "Epoch 4791 : Total Loss 5.488\n",
      "Epoch 4801 : Total Loss 5.487\n",
      "Epoch 4811 : Total Loss 5.487\n",
      "Epoch 4821 : Total Loss 5.487\n",
      "Epoch 4831 : Total Loss 5.487\n",
      "Epoch 4841 : Total Loss 5.487\n",
      "Epoch 4851 : Total Loss 5.487\n",
      "Epoch 4861 : Total Loss 5.487\n",
      "Epoch 4871 : Total Loss 5.486\n",
      "Epoch 4881 : Total Loss 5.486\n",
      "Epoch 4891 : Total Loss 5.486\n",
      "Epoch 4901 : Total Loss 5.486\n",
      "Epoch 4911 : Total Loss 5.486\n",
      "Epoch 4921 : Total Loss 5.486\n",
      "Epoch 4931 : Total Loss 5.486\n",
      "Epoch 4941 : Total Loss 5.485\n",
      "Epoch 4951 : Total Loss 5.485\n",
      "Epoch 4961 : Total Loss 5.485\n",
      "Epoch 4971 : Total Loss 5.485\n",
      "Epoch 4981 : Total Loss 5.485\n",
      "Epoch 4991 : Total Loss 5.485\n",
      "U_hat is:\n",
      " [[0.87462714 1.56945946]\n",
      " [1.95683158 0.05820418]\n",
      " [0.57834637 1.3279668 ]\n",
      " [1.84999735 0.76483079]\n",
      " [1.08993819 1.42735819]]\n",
      "V_hat is:\n",
      " [[1.46221361 2.36133541]\n",
      " [1.04992459 1.30756873]\n",
      " [1.61603897 0.08680954]\n",
      " [1.46369941 0.13841368]\n",
      " [0.47557959 0.36977277]]\n",
      "A_hat is:\n",
      " [[4.98491191 2.97046866 1.54967559 1.49742589 0.99629819]\n",
      " [2.99874535 2.13063156 3.16736876 2.87226948 0.95215147]\n",
      " [3.98144094 2.34362793 1.04991045 1.03033401 0.76609569]\n",
      " [4.51111332 2.94242653 3.05606241 2.81370307 1.16263457]\n",
      " [4.96419389 3.01072185 1.88529089 1.79290779 1.04615055]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [5,3,2,1,0],\n",
    "    [3,0,4,2,1],\n",
    "    [4,0,0,1,0],\n",
    "    [0,0,2,4,0],\n",
    "    [5,3,0,0,1]\n",
    "])\n",
    "\n",
    "m = A.shape[0]\n",
    "n = A.shape[1]\n",
    "user_features = 2\n",
    "\n",
    "np.random.seed(24)\n",
    "\n",
    "# generate matrices with entries of random numbers 0 - 1\n",
    "U = np.random.rand(m, user_features)\n",
    "V = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U)\n",
    "print('V is:\\n', V)\n",
    "\n",
    "# perform factorization\n",
    "\n",
    "U_hat, V_hat = matrix_factorization_sparse(A, U, V, user_features, 5000)\n",
    "\n",
    "print('U_hat is:\\n', U_hat)\n",
    "print('V_hat is:\\n', V_hat)\n",
    "\n",
    "## check if the product results in something close to A\n",
    "\n",
    "print('A_hat is:\\n', np.dot(U_hat, V_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc081cc3",
   "metadata": {},
   "source": [
    "### Test Runtime\n",
    "\n",
    "We will test the runtime of the code we implemented for the sparse data.\n",
    "\n",
    "We will test both methods for the same amount of epochs and latent features, learning rate, etc. on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6b770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit as t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee22777",
   "metadata": {},
   "source": [
    "**Regular Matrix Factorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b281371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 5.473\n",
      "Epoch 11 : Total Loss 5.473\n",
      "Epoch 21 : Total Loss 5.473\n",
      "Epoch 31 : Total Loss 5.473\n",
      "Epoch 41 : Total Loss 5.473\n",
      "Epoch 51 : Total Loss 5.473\n",
      "Epoch 61 : Total Loss 5.473\n",
      "Epoch 71 : Total Loss 5.473\n",
      "Epoch 81 : Total Loss 5.472\n",
      "Epoch 91 : Total Loss 5.472\n",
      "Epoch 101 : Total Loss 5.472\n",
      "Epoch 111 : Total Loss 5.472\n",
      "Epoch 121 : Total Loss 5.472\n",
      "Epoch 131 : Total Loss 5.472\n",
      "Epoch 141 : Total Loss 5.472\n",
      "Epoch 151 : Total Loss 5.471\n",
      "Epoch 161 : Total Loss 5.471\n",
      "Epoch 171 : Total Loss 5.471\n",
      "Epoch 181 : Total Loss 5.471\n",
      "Epoch 191 : Total Loss 5.471\n",
      "Epoch 201 : Total Loss 5.471\n",
      "Epoch 211 : Total Loss 5.471\n",
      "Epoch 221 : Total Loss 5.470\n",
      "Epoch 231 : Total Loss 5.470\n",
      "Epoch 241 : Total Loss 5.470\n",
      "Epoch 251 : Total Loss 5.470\n",
      "Epoch 261 : Total Loss 5.470\n",
      "Epoch 271 : Total Loss 5.470\n",
      "Epoch 281 : Total Loss 5.470\n",
      "Epoch 291 : Total Loss 5.469\n",
      "Epoch 301 : Total Loss 5.469\n",
      "Epoch 311 : Total Loss 5.469\n",
      "Epoch 321 : Total Loss 5.469\n",
      "Epoch 331 : Total Loss 5.469\n",
      "Epoch 341 : Total Loss 5.469\n",
      "Epoch 351 : Total Loss 5.468\n",
      "Epoch 361 : Total Loss 5.468\n",
      "Epoch 371 : Total Loss 5.468\n",
      "Epoch 381 : Total Loss 5.468\n",
      "Epoch 391 : Total Loss 5.468\n",
      "Epoch 401 : Total Loss 5.468\n",
      "Epoch 411 : Total Loss 5.468\n",
      "Epoch 421 : Total Loss 5.467\n",
      "Epoch 431 : Total Loss 5.467\n",
      "Epoch 441 : Total Loss 5.467\n",
      "Epoch 451 : Total Loss 5.467\n",
      "Epoch 461 : Total Loss 5.467\n",
      "Epoch 471 : Total Loss 5.467\n",
      "Epoch 481 : Total Loss 5.466\n",
      "Epoch 491 : Total Loss 5.466\n",
      "Epoch 501 : Total Loss 5.466\n",
      "Epoch 511 : Total Loss 5.466\n",
      "Epoch 521 : Total Loss 5.466\n",
      "Epoch 531 : Total Loss 5.466\n",
      "Epoch 541 : Total Loss 5.466\n",
      "Epoch 551 : Total Loss 5.465\n",
      "Epoch 561 : Total Loss 5.465\n",
      "Epoch 571 : Total Loss 5.465\n",
      "Epoch 581 : Total Loss 5.465\n",
      "Epoch 591 : Total Loss 5.465\n",
      "Epoch 601 : Total Loss 5.465\n",
      "Epoch 611 : Total Loss 5.464\n",
      "Epoch 621 : Total Loss 5.464\n",
      "Epoch 631 : Total Loss 5.464\n",
      "Epoch 641 : Total Loss 5.464\n",
      "Epoch 651 : Total Loss 5.464\n",
      "Epoch 661 : Total Loss 5.464\n",
      "Epoch 671 : Total Loss 5.463\n",
      "Epoch 681 : Total Loss 5.463\n",
      "Epoch 691 : Total Loss 5.463\n",
      "Epoch 701 : Total Loss 5.463\n",
      "Epoch 711 : Total Loss 5.463\n",
      "Epoch 721 : Total Loss 5.463\n",
      "Epoch 731 : Total Loss 5.462\n",
      "Epoch 741 : Total Loss 5.462\n",
      "Epoch 751 : Total Loss 5.462\n",
      "Epoch 761 : Total Loss 5.462\n",
      "Epoch 771 : Total Loss 5.462\n",
      "Epoch 781 : Total Loss 5.461\n",
      "Epoch 791 : Total Loss 5.461\n",
      "Epoch 801 : Total Loss 5.461\n",
      "Epoch 811 : Total Loss 5.461\n",
      "Epoch 821 : Total Loss 5.461\n",
      "Epoch 831 : Total Loss 5.461\n",
      "Epoch 841 : Total Loss 5.460\n",
      "Epoch 851 : Total Loss 5.460\n",
      "Epoch 861 : Total Loss 5.460\n",
      "Epoch 871 : Total Loss 5.460\n",
      "Epoch 881 : Total Loss 5.460\n",
      "Epoch 891 : Total Loss 5.459\n",
      "Epoch 901 : Total Loss 5.459\n",
      "Epoch 911 : Total Loss 5.459\n",
      "Epoch 921 : Total Loss 5.459\n",
      "Epoch 931 : Total Loss 5.459\n",
      "Epoch 941 : Total Loss 5.458\n",
      "Epoch 951 : Total Loss 5.458\n",
      "Epoch 961 : Total Loss 5.458\n",
      "Epoch 971 : Total Loss 5.458\n",
      "Epoch 981 : Total Loss 5.458\n",
      "Epoch 991 : Total Loss 5.457\n",
      "Epoch 1001 : Total Loss 5.457\n",
      "Epoch 1011 : Total Loss 5.457\n",
      "Epoch 1021 : Total Loss 5.457\n",
      "Epoch 1031 : Total Loss 5.457\n",
      "Epoch 1041 : Total Loss 5.456\n",
      "Epoch 1051 : Total Loss 5.456\n",
      "Epoch 1061 : Total Loss 5.456\n",
      "Epoch 1071 : Total Loss 5.456\n",
      "Epoch 1081 : Total Loss 5.455\n",
      "Epoch 1091 : Total Loss 5.455\n",
      "Epoch 1101 : Total Loss 5.455\n",
      "Epoch 1111 : Total Loss 5.455\n",
      "Epoch 1121 : Total Loss 5.455\n",
      "Epoch 1131 : Total Loss 5.454\n",
      "Epoch 1141 : Total Loss 5.454\n",
      "Epoch 1151 : Total Loss 5.454\n",
      "Epoch 1161 : Total Loss 5.454\n",
      "Epoch 1171 : Total Loss 5.453\n",
      "Epoch 1181 : Total Loss 5.453\n",
      "Epoch 1191 : Total Loss 5.453\n",
      "Epoch 1201 : Total Loss 5.453\n",
      "Epoch 1211 : Total Loss 5.452\n",
      "Epoch 1221 : Total Loss 5.452\n",
      "Epoch 1231 : Total Loss 5.452\n",
      "Epoch 1241 : Total Loss 5.452\n",
      "Epoch 1251 : Total Loss 5.451\n",
      "Epoch 1261 : Total Loss 5.451\n",
      "Epoch 1271 : Total Loss 5.451\n",
      "Epoch 1281 : Total Loss 5.451\n",
      "Epoch 1291 : Total Loss 5.450\n",
      "Epoch 1301 : Total Loss 5.450\n",
      "Epoch 1311 : Total Loss 5.450\n",
      "Epoch 1321 : Total Loss 5.450\n",
      "Epoch 1331 : Total Loss 5.449\n",
      "Epoch 1341 : Total Loss 5.449\n",
      "Epoch 1351 : Total Loss 5.449\n",
      "Epoch 1361 : Total Loss 5.449\n",
      "Epoch 1371 : Total Loss 5.448\n",
      "Epoch 1381 : Total Loss 5.448\n",
      "Epoch 1391 : Total Loss 5.448\n",
      "Epoch 1401 : Total Loss 5.448\n",
      "Epoch 1411 : Total Loss 5.447\n",
      "Epoch 1421 : Total Loss 5.447\n",
      "Epoch 1431 : Total Loss 5.447\n",
      "Epoch 1441 : Total Loss 5.446\n",
      "Epoch 1451 : Total Loss 5.446\n",
      "Epoch 1461 : Total Loss 5.446\n",
      "Epoch 1471 : Total Loss 5.446\n",
      "Epoch 1481 : Total Loss 5.445\n",
      "Epoch 1491 : Total Loss 5.445\n",
      "Epoch 1501 : Total Loss 5.445\n",
      "Epoch 1511 : Total Loss 5.444\n",
      "Epoch 1521 : Total Loss 5.444\n",
      "Epoch 1531 : Total Loss 5.444\n",
      "Epoch 1541 : Total Loss 5.443\n",
      "Epoch 1551 : Total Loss 5.443\n",
      "Epoch 1561 : Total Loss 5.443\n",
      "Epoch 1571 : Total Loss 5.442\n",
      "Epoch 1581 : Total Loss 5.442\n",
      "Epoch 1591 : Total Loss 5.442\n",
      "Epoch 1601 : Total Loss 5.442\n",
      "Epoch 1611 : Total Loss 5.441\n",
      "Epoch 1621 : Total Loss 5.441\n",
      "Epoch 1631 : Total Loss 5.441\n",
      "Epoch 1641 : Total Loss 5.440\n",
      "Epoch 1651 : Total Loss 5.440\n",
      "Epoch 1661 : Total Loss 5.440\n",
      "Epoch 1671 : Total Loss 5.439\n",
      "Epoch 1681 : Total Loss 5.439\n",
      "Epoch 1691 : Total Loss 5.438\n",
      "Epoch 1701 : Total Loss 5.438\n",
      "Epoch 1711 : Total Loss 5.438\n",
      "Epoch 1721 : Total Loss 5.437\n",
      "Epoch 1731 : Total Loss 5.437\n",
      "Epoch 1741 : Total Loss 5.437\n",
      "Epoch 1751 : Total Loss 5.436\n",
      "Epoch 1761 : Total Loss 5.436\n",
      "Epoch 1771 : Total Loss 5.436\n",
      "Epoch 1781 : Total Loss 5.435\n",
      "Epoch 1791 : Total Loss 5.435\n",
      "Epoch 1801 : Total Loss 5.434\n",
      "Epoch 1811 : Total Loss 5.434\n",
      "Epoch 1821 : Total Loss 5.434\n",
      "Epoch 1831 : Total Loss 5.433\n",
      "Epoch 1841 : Total Loss 5.433\n",
      "Epoch 1851 : Total Loss 5.433\n",
      "Epoch 1861 : Total Loss 5.432\n",
      "Epoch 1871 : Total Loss 5.432\n",
      "Epoch 1881 : Total Loss 5.431\n",
      "Epoch 1891 : Total Loss 5.431\n",
      "Epoch 1901 : Total Loss 5.431\n",
      "Epoch 1911 : Total Loss 5.430\n",
      "Epoch 1921 : Total Loss 5.430\n",
      "Epoch 1931 : Total Loss 5.429\n",
      "Epoch 1941 : Total Loss 5.429\n",
      "Epoch 1951 : Total Loss 5.428\n",
      "Epoch 1961 : Total Loss 5.428\n",
      "Epoch 1971 : Total Loss 5.428\n",
      "Epoch 1981 : Total Loss 5.427\n",
      "Epoch 1991 : Total Loss 5.427\n",
      "Epoch 2001 : Total Loss 5.426\n",
      "Epoch 2011 : Total Loss 5.426\n",
      "Epoch 2021 : Total Loss 5.425\n",
      "Epoch 2031 : Total Loss 5.425\n",
      "Epoch 2041 : Total Loss 5.424\n",
      "Epoch 2051 : Total Loss 5.424\n",
      "Epoch 2061 : Total Loss 5.423\n",
      "Epoch 2071 : Total Loss 5.423\n",
      "Epoch 2081 : Total Loss 5.423\n",
      "Epoch 2091 : Total Loss 5.422\n",
      "Epoch 2101 : Total Loss 5.422\n",
      "Epoch 2111 : Total Loss 5.421\n",
      "Epoch 2121 : Total Loss 5.421\n",
      "Epoch 2131 : Total Loss 5.420\n",
      "Epoch 2141 : Total Loss 5.420\n",
      "Epoch 2151 : Total Loss 5.419\n",
      "Epoch 2161 : Total Loss 5.419\n",
      "Epoch 2171 : Total Loss 5.418\n",
      "Epoch 2181 : Total Loss 5.418\n",
      "Epoch 2191 : Total Loss 5.417\n",
      "Epoch 2201 : Total Loss 5.416\n",
      "Epoch 2211 : Total Loss 5.416\n",
      "Epoch 2221 : Total Loss 5.415\n",
      "Epoch 2231 : Total Loss 5.415\n",
      "Epoch 2241 : Total Loss 5.414\n",
      "Epoch 2251 : Total Loss 5.414\n",
      "Epoch 2261 : Total Loss 5.413\n",
      "Epoch 2271 : Total Loss 5.413\n",
      "Epoch 2281 : Total Loss 5.412\n",
      "Epoch 2291 : Total Loss 5.412\n",
      "Epoch 2301 : Total Loss 5.411\n",
      "Epoch 2311 : Total Loss 5.410\n",
      "Epoch 2321 : Total Loss 5.410\n",
      "Epoch 2331 : Total Loss 5.409\n",
      "Epoch 2341 : Total Loss 5.409\n",
      "Epoch 2351 : Total Loss 5.408\n",
      "Epoch 2361 : Total Loss 5.407\n",
      "Epoch 2371 : Total Loss 5.407\n",
      "Epoch 2381 : Total Loss 5.406\n",
      "Epoch 2391 : Total Loss 5.406\n",
      "Epoch 2401 : Total Loss 5.405\n",
      "Epoch 2411 : Total Loss 5.404\n",
      "Epoch 2421 : Total Loss 5.404\n",
      "Epoch 2431 : Total Loss 5.403\n",
      "Epoch 2441 : Total Loss 5.403\n",
      "Epoch 2451 : Total Loss 5.402\n",
      "Epoch 2461 : Total Loss 5.401\n",
      "Epoch 2471 : Total Loss 5.401\n",
      "Epoch 2481 : Total Loss 5.400\n",
      "Epoch 2491 : Total Loss 5.399\n",
      "Epoch 2501 : Total Loss 5.399\n",
      "Epoch 2511 : Total Loss 5.398\n",
      "Epoch 2521 : Total Loss 5.397\n",
      "Epoch 2531 : Total Loss 5.397\n",
      "Epoch 2541 : Total Loss 5.396\n",
      "Epoch 2551 : Total Loss 5.395\n",
      "Epoch 2561 : Total Loss 5.394\n",
      "Epoch 2571 : Total Loss 5.394\n",
      "Epoch 2581 : Total Loss 5.393\n",
      "Epoch 2591 : Total Loss 5.392\n",
      "Epoch 2601 : Total Loss 5.392\n",
      "Epoch 2611 : Total Loss 5.391\n",
      "Epoch 2621 : Total Loss 5.390\n",
      "Epoch 2631 : Total Loss 5.389\n",
      "Epoch 2641 : Total Loss 5.389\n",
      "Epoch 2651 : Total Loss 5.388\n",
      "Epoch 2661 : Total Loss 5.387\n",
      "Epoch 2671 : Total Loss 5.386\n",
      "Epoch 2681 : Total Loss 5.385\n",
      "Epoch 2691 : Total Loss 5.385\n",
      "Epoch 2701 : Total Loss 5.384\n",
      "Epoch 2711 : Total Loss 5.383\n",
      "Epoch 2721 : Total Loss 5.382\n",
      "Epoch 2731 : Total Loss 5.381\n",
      "Epoch 2741 : Total Loss 5.381\n",
      "Epoch 2751 : Total Loss 5.380\n",
      "Epoch 2761 : Total Loss 5.379\n",
      "Epoch 2771 : Total Loss 5.378\n",
      "Epoch 2781 : Total Loss 5.377\n",
      "Epoch 2791 : Total Loss 5.376\n",
      "Epoch 2801 : Total Loss 5.376\n",
      "Epoch 2811 : Total Loss 5.375\n",
      "Epoch 2821 : Total Loss 5.374\n",
      "Epoch 2831 : Total Loss 5.373\n",
      "Epoch 2841 : Total Loss 5.372\n",
      "Epoch 2851 : Total Loss 5.371\n",
      "Epoch 2861 : Total Loss 5.370\n",
      "Epoch 2871 : Total Loss 5.369\n",
      "Epoch 2881 : Total Loss 5.369\n",
      "Epoch 2891 : Total Loss 5.368\n",
      "Epoch 2901 : Total Loss 5.367\n",
      "Epoch 2911 : Total Loss 5.366\n",
      "Epoch 2921 : Total Loss 5.365\n",
      "Epoch 2931 : Total Loss 5.364\n",
      "Epoch 2941 : Total Loss 5.363\n",
      "Epoch 2951 : Total Loss 5.362\n",
      "Epoch 2961 : Total Loss 5.361\n",
      "Epoch 2971 : Total Loss 5.360\n",
      "Epoch 2981 : Total Loss 5.359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2991 : Total Loss 5.358\n",
      "Epoch 3001 : Total Loss 5.357\n",
      "Epoch 3011 : Total Loss 5.356\n",
      "Epoch 3021 : Total Loss 5.355\n",
      "Epoch 3031 : Total Loss 5.354\n",
      "Epoch 3041 : Total Loss 5.353\n",
      "Epoch 3051 : Total Loss 5.352\n",
      "Epoch 3061 : Total Loss 5.351\n",
      "Epoch 3071 : Total Loss 5.350\n",
      "Epoch 3081 : Total Loss 5.349\n",
      "Epoch 3091 : Total Loss 5.347\n",
      "Epoch 3101 : Total Loss 5.346\n",
      "Epoch 3111 : Total Loss 5.345\n",
      "Epoch 3121 : Total Loss 5.344\n",
      "Epoch 3131 : Total Loss 5.343\n",
      "Epoch 3141 : Total Loss 5.342\n",
      "Epoch 3151 : Total Loss 5.341\n",
      "Epoch 3161 : Total Loss 5.340\n",
      "Epoch 3171 : Total Loss 5.338\n",
      "Epoch 3181 : Total Loss 5.337\n",
      "Epoch 3191 : Total Loss 5.336\n",
      "Epoch 3201 : Total Loss 5.335\n",
      "Epoch 3211 : Total Loss 5.334\n",
      "Epoch 3221 : Total Loss 5.333\n",
      "Epoch 3231 : Total Loss 5.331\n",
      "Epoch 3241 : Total Loss 5.330\n",
      "Epoch 3251 : Total Loss 5.329\n",
      "Epoch 3261 : Total Loss 5.328\n",
      "Epoch 3271 : Total Loss 5.326\n",
      "Epoch 3281 : Total Loss 5.325\n",
      "Epoch 3291 : Total Loss 5.324\n",
      "Epoch 3301 : Total Loss 5.323\n",
      "Epoch 3311 : Total Loss 5.321\n",
      "Epoch 3321 : Total Loss 5.320\n",
      "Epoch 3331 : Total Loss 5.319\n",
      "Epoch 3341 : Total Loss 5.317\n",
      "Epoch 3351 : Total Loss 5.316\n",
      "Epoch 3361 : Total Loss 5.315\n",
      "Epoch 3371 : Total Loss 5.313\n",
      "Epoch 3381 : Total Loss 5.312\n",
      "Epoch 3391 : Total Loss 5.311\n",
      "Epoch 3401 : Total Loss 5.309\n",
      "Epoch 3411 : Total Loss 5.308\n",
      "Epoch 3421 : Total Loss 5.306\n",
      "Epoch 3431 : Total Loss 5.305\n",
      "Epoch 3441 : Total Loss 5.303\n",
      "Epoch 3451 : Total Loss 5.302\n",
      "Epoch 3461 : Total Loss 5.301\n",
      "Epoch 3471 : Total Loss 5.299\n",
      "Epoch 3481 : Total Loss 5.298\n",
      "Epoch 3491 : Total Loss 5.296\n",
      "Epoch 3501 : Total Loss 5.295\n",
      "Epoch 3511 : Total Loss 5.293\n",
      "Epoch 3521 : Total Loss 5.292\n",
      "Epoch 3531 : Total Loss 5.290\n",
      "Epoch 3541 : Total Loss 5.289\n",
      "Epoch 3551 : Total Loss 5.287\n",
      "Epoch 3561 : Total Loss 5.285\n",
      "Epoch 3571 : Total Loss 5.284\n",
      "Epoch 3581 : Total Loss 5.282\n",
      "Epoch 3591 : Total Loss 5.281\n",
      "Epoch 3601 : Total Loss 5.279\n",
      "Epoch 3611 : Total Loss 5.277\n",
      "Epoch 3621 : Total Loss 5.276\n",
      "Epoch 3631 : Total Loss 5.274\n",
      "Epoch 3641 : Total Loss 5.272\n",
      "Epoch 3651 : Total Loss 5.271\n",
      "Epoch 3661 : Total Loss 5.269\n",
      "Epoch 3671 : Total Loss 5.267\n",
      "Epoch 3681 : Total Loss 5.266\n",
      "Epoch 3691 : Total Loss 5.264\n",
      "Epoch 3701 : Total Loss 5.262\n",
      "Epoch 3711 : Total Loss 5.260\n",
      "Epoch 3721 : Total Loss 5.259\n",
      "Epoch 3731 : Total Loss 5.257\n",
      "Epoch 3741 : Total Loss 5.255\n",
      "Epoch 3751 : Total Loss 5.253\n",
      "Epoch 3761 : Total Loss 5.251\n",
      "Epoch 3771 : Total Loss 5.249\n",
      "Epoch 3781 : Total Loss 5.248\n",
      "Epoch 3791 : Total Loss 5.246\n",
      "Epoch 3801 : Total Loss 5.244\n",
      "Epoch 3811 : Total Loss 5.242\n",
      "Epoch 3821 : Total Loss 5.240\n",
      "Epoch 3831 : Total Loss 5.238\n",
      "Epoch 3841 : Total Loss 5.236\n",
      "Epoch 3851 : Total Loss 5.234\n",
      "Epoch 3861 : Total Loss 5.232\n",
      "Epoch 3871 : Total Loss 5.230\n",
      "Epoch 3881 : Total Loss 5.228\n",
      "Epoch 3891 : Total Loss 5.226\n",
      "Epoch 3901 : Total Loss 5.224\n",
      "Epoch 3911 : Total Loss 5.222\n",
      "Epoch 3921 : Total Loss 5.220\n",
      "Epoch 3931 : Total Loss 5.218\n",
      "Epoch 3941 : Total Loss 5.216\n",
      "Epoch 3951 : Total Loss 5.214\n",
      "Epoch 3961 : Total Loss 5.212\n",
      "Epoch 3971 : Total Loss 5.209\n",
      "Epoch 3981 : Total Loss 5.207\n",
      "Epoch 3991 : Total Loss 5.205\n",
      "Epoch 4001 : Total Loss 5.203\n",
      "Epoch 4011 : Total Loss 5.201\n",
      "Epoch 4021 : Total Loss 5.199\n",
      "Epoch 4031 : Total Loss 5.196\n",
      "Epoch 4041 : Total Loss 5.194\n",
      "Epoch 4051 : Total Loss 5.192\n",
      "Epoch 4061 : Total Loss 5.189\n",
      "Epoch 4071 : Total Loss 5.187\n",
      "Epoch 4081 : Total Loss 5.185\n",
      "Epoch 4091 : Total Loss 5.183\n",
      "Epoch 4101 : Total Loss 5.180\n",
      "Epoch 4111 : Total Loss 5.178\n",
      "Epoch 4121 : Total Loss 5.175\n",
      "Epoch 4131 : Total Loss 5.173\n",
      "Epoch 4141 : Total Loss 5.171\n",
      "Epoch 4151 : Total Loss 5.168\n",
      "Epoch 4161 : Total Loss 5.166\n",
      "Epoch 4171 : Total Loss 5.163\n",
      "Epoch 4181 : Total Loss 5.161\n",
      "Epoch 4191 : Total Loss 5.158\n",
      "Epoch 4201 : Total Loss 5.156\n",
      "Epoch 4211 : Total Loss 5.153\n",
      "Epoch 4221 : Total Loss 5.151\n",
      "Epoch 4231 : Total Loss 5.148\n",
      "Epoch 4241 : Total Loss 5.145\n",
      "Epoch 4251 : Total Loss 5.143\n",
      "Epoch 4261 : Total Loss 5.140\n",
      "Epoch 4271 : Total Loss 5.138\n",
      "Epoch 4281 : Total Loss 5.135\n",
      "Epoch 4291 : Total Loss 5.132\n",
      "Epoch 4301 : Total Loss 5.129\n",
      "Epoch 4311 : Total Loss 5.127\n",
      "Epoch 4321 : Total Loss 5.124\n",
      "Epoch 4331 : Total Loss 5.121\n",
      "Epoch 4341 : Total Loss 5.118\n",
      "Epoch 4351 : Total Loss 5.116\n",
      "Epoch 4361 : Total Loss 5.113\n",
      "Epoch 4371 : Total Loss 5.110\n",
      "Epoch 4381 : Total Loss 5.107\n",
      "Epoch 4391 : Total Loss 5.104\n",
      "Epoch 4401 : Total Loss 5.101\n",
      "Epoch 4411 : Total Loss 5.098\n",
      "Epoch 4421 : Total Loss 5.095\n",
      "Epoch 4431 : Total Loss 5.093\n",
      "Epoch 4441 : Total Loss 5.090\n",
      "Epoch 4451 : Total Loss 5.087\n",
      "Epoch 4461 : Total Loss 5.084\n",
      "Epoch 4471 : Total Loss 5.080\n",
      "Epoch 4481 : Total Loss 5.077\n",
      "Epoch 4491 : Total Loss 5.074\n",
      "Epoch 4501 : Total Loss 5.071\n",
      "Epoch 4511 : Total Loss 5.068\n",
      "Epoch 4521 : Total Loss 5.065\n",
      "Epoch 4531 : Total Loss 5.062\n",
      "Epoch 4541 : Total Loss 5.059\n",
      "Epoch 4551 : Total Loss 5.055\n",
      "Epoch 4561 : Total Loss 5.052\n",
      "Epoch 4571 : Total Loss 5.049\n",
      "Epoch 4581 : Total Loss 5.046\n",
      "Epoch 4591 : Total Loss 5.042\n",
      "Epoch 4601 : Total Loss 5.039\n",
      "Epoch 4611 : Total Loss 5.036\n",
      "Epoch 4621 : Total Loss 5.032\n",
      "Epoch 4631 : Total Loss 5.029\n",
      "Epoch 4641 : Total Loss 5.026\n",
      "Epoch 4651 : Total Loss 5.022\n",
      "Epoch 4661 : Total Loss 5.019\n",
      "Epoch 4671 : Total Loss 5.015\n",
      "Epoch 4681 : Total Loss 5.012\n",
      "Epoch 4691 : Total Loss 5.008\n",
      "Epoch 4701 : Total Loss 5.005\n",
      "Epoch 4711 : Total Loss 5.001\n",
      "Epoch 4721 : Total Loss 4.998\n",
      "Epoch 4731 : Total Loss 4.994\n",
      "Epoch 4741 : Total Loss 4.991\n",
      "Epoch 4751 : Total Loss 4.987\n",
      "Epoch 4761 : Total Loss 4.983\n",
      "Epoch 4771 : Total Loss 4.980\n",
      "Epoch 4781 : Total Loss 4.976\n",
      "Epoch 4791 : Total Loss 4.972\n",
      "Epoch 4801 : Total Loss 4.968\n",
      "Epoch 4811 : Total Loss 4.965\n",
      "Epoch 4821 : Total Loss 4.961\n",
      "Epoch 4831 : Total Loss 4.957\n",
      "Epoch 4841 : Total Loss 4.953\n",
      "Epoch 4851 : Total Loss 4.949\n",
      "Epoch 4861 : Total Loss 4.946\n",
      "Epoch 4871 : Total Loss 4.942\n",
      "Epoch 4881 : Total Loss 4.938\n",
      "Epoch 4891 : Total Loss 4.934\n",
      "Epoch 4901 : Total Loss 4.930\n",
      "Epoch 4911 : Total Loss 4.926\n",
      "Epoch 4921 : Total Loss 4.922\n",
      "Epoch 4931 : Total Loss 4.918\n",
      "Epoch 4941 : Total Loss 4.914\n",
      "Epoch 4951 : Total Loss 4.910\n",
      "Epoch 4961 : Total Loss 4.905\n",
      "Epoch 4971 : Total Loss 4.901\n",
      "Epoch 4981 : Total Loss 4.897\n",
      "Epoch 4991 : Total Loss 4.893\n",
      "The runtime of the regular matrix factorization is  2.539413799997419 seconds.\n"
     ]
    }
   ],
   "source": [
    "start1 = t.default_timer()\n",
    "\n",
    "U_hat, V_hat = matrix_factorization(A, U, V, user_features, 5000)\n",
    "\n",
    "end1 = t.default_timer()\n",
    "\n",
    "print('The runtime of the regular matrix factorization is ', end1 - start1, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6c3f3",
   "metadata": {},
   "source": [
    "**Sparse Matrix Factorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dac08b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 4.899\n",
      "Epoch 11 : Total Loss 4.895\n",
      "Epoch 21 : Total Loss 4.891\n",
      "Epoch 31 : Total Loss 4.886\n",
      "Epoch 41 : Total Loss 4.882\n",
      "Epoch 51 : Total Loss 4.878\n",
      "Epoch 61 : Total Loss 4.873\n",
      "Epoch 71 : Total Loss 4.869\n",
      "Epoch 81 : Total Loss 4.864\n",
      "Epoch 91 : Total Loss 4.860\n",
      "Epoch 101 : Total Loss 4.856\n",
      "Epoch 111 : Total Loss 4.851\n",
      "Epoch 121 : Total Loss 4.847\n",
      "Epoch 131 : Total Loss 4.842\n",
      "Epoch 141 : Total Loss 4.838\n",
      "Epoch 151 : Total Loss 4.833\n",
      "Epoch 161 : Total Loss 4.828\n",
      "Epoch 171 : Total Loss 4.824\n",
      "Epoch 181 : Total Loss 4.819\n",
      "Epoch 191 : Total Loss 4.815\n",
      "Epoch 201 : Total Loss 4.810\n",
      "Epoch 211 : Total Loss 4.805\n",
      "Epoch 221 : Total Loss 4.800\n",
      "Epoch 231 : Total Loss 4.796\n",
      "Epoch 241 : Total Loss 4.791\n",
      "Epoch 251 : Total Loss 4.786\n",
      "Epoch 261 : Total Loss 4.781\n",
      "Epoch 271 : Total Loss 4.776\n",
      "Epoch 281 : Total Loss 4.772\n",
      "Epoch 291 : Total Loss 4.767\n",
      "Epoch 301 : Total Loss 4.762\n",
      "Epoch 311 : Total Loss 4.757\n",
      "Epoch 321 : Total Loss 4.752\n",
      "Epoch 331 : Total Loss 4.747\n",
      "Epoch 341 : Total Loss 4.742\n",
      "Epoch 351 : Total Loss 4.737\n",
      "Epoch 361 : Total Loss 4.732\n",
      "Epoch 371 : Total Loss 4.727\n",
      "Epoch 381 : Total Loss 4.721\n",
      "Epoch 391 : Total Loss 4.716\n",
      "Epoch 401 : Total Loss 4.711\n",
      "Epoch 411 : Total Loss 4.706\n",
      "Epoch 421 : Total Loss 4.701\n",
      "Epoch 431 : Total Loss 4.696\n",
      "Epoch 441 : Total Loss 4.690\n",
      "Epoch 451 : Total Loss 4.685\n",
      "Epoch 461 : Total Loss 4.680\n",
      "Epoch 471 : Total Loss 4.674\n",
      "Epoch 481 : Total Loss 4.669\n",
      "Epoch 491 : Total Loss 4.664\n",
      "Epoch 501 : Total Loss 4.658\n",
      "Epoch 511 : Total Loss 4.653\n",
      "Epoch 521 : Total Loss 4.648\n",
      "Epoch 531 : Total Loss 4.642\n",
      "Epoch 541 : Total Loss 4.637\n",
      "Epoch 551 : Total Loss 4.631\n",
      "Epoch 561 : Total Loss 4.626\n",
      "Epoch 571 : Total Loss 4.620\n",
      "Epoch 581 : Total Loss 4.614\n",
      "Epoch 591 : Total Loss 4.609\n",
      "Epoch 601 : Total Loss 4.603\n",
      "Epoch 611 : Total Loss 4.598\n",
      "Epoch 621 : Total Loss 4.592\n",
      "Epoch 631 : Total Loss 4.586\n",
      "Epoch 641 : Total Loss 4.581\n",
      "Epoch 651 : Total Loss 4.575\n",
      "Epoch 661 : Total Loss 4.569\n",
      "Epoch 671 : Total Loss 4.563\n",
      "Epoch 681 : Total Loss 4.558\n",
      "Epoch 691 : Total Loss 4.552\n",
      "Epoch 701 : Total Loss 4.546\n",
      "Epoch 711 : Total Loss 4.540\n",
      "Epoch 721 : Total Loss 4.534\n",
      "Epoch 731 : Total Loss 4.528\n",
      "Epoch 741 : Total Loss 4.522\n",
      "Epoch 751 : Total Loss 4.517\n",
      "Epoch 761 : Total Loss 4.511\n",
      "Epoch 771 : Total Loss 4.505\n",
      "Epoch 781 : Total Loss 4.499\n",
      "Epoch 791 : Total Loss 4.493\n",
      "Epoch 801 : Total Loss 4.487\n",
      "Epoch 811 : Total Loss 4.481\n",
      "Epoch 821 : Total Loss 4.474\n",
      "Epoch 831 : Total Loss 4.468\n",
      "Epoch 841 : Total Loss 4.462\n",
      "Epoch 851 : Total Loss 4.456\n",
      "Epoch 861 : Total Loss 4.450\n",
      "Epoch 871 : Total Loss 4.444\n",
      "Epoch 881 : Total Loss 4.438\n",
      "Epoch 891 : Total Loss 4.431\n",
      "Epoch 901 : Total Loss 4.425\n",
      "Epoch 911 : Total Loss 4.419\n",
      "Epoch 921 : Total Loss 4.413\n",
      "Epoch 931 : Total Loss 4.406\n",
      "Epoch 941 : Total Loss 4.400\n",
      "Epoch 951 : Total Loss 4.394\n",
      "Epoch 961 : Total Loss 4.388\n",
      "Epoch 971 : Total Loss 4.381\n",
      "Epoch 981 : Total Loss 4.375\n",
      "Epoch 991 : Total Loss 4.368\n",
      "Epoch 1001 : Total Loss 4.362\n",
      "Epoch 1011 : Total Loss 4.356\n",
      "Epoch 1021 : Total Loss 4.349\n",
      "Epoch 1031 : Total Loss 4.343\n",
      "Epoch 1041 : Total Loss 4.336\n",
      "Epoch 1051 : Total Loss 4.330\n",
      "Epoch 1061 : Total Loss 4.323\n",
      "Epoch 1071 : Total Loss 4.317\n",
      "Epoch 1081 : Total Loss 4.310\n",
      "Epoch 1091 : Total Loss 4.304\n",
      "Epoch 1101 : Total Loss 4.297\n",
      "Epoch 1111 : Total Loss 4.291\n",
      "Epoch 1121 : Total Loss 4.284\n",
      "Epoch 1131 : Total Loss 4.278\n",
      "Epoch 1141 : Total Loss 4.271\n",
      "Epoch 1151 : Total Loss 4.264\n",
      "Epoch 1161 : Total Loss 4.258\n",
      "Epoch 1171 : Total Loss 4.251\n",
      "Epoch 1181 : Total Loss 4.244\n",
      "Epoch 1191 : Total Loss 4.238\n",
      "Epoch 1201 : Total Loss 4.231\n",
      "Epoch 1211 : Total Loss 4.224\n",
      "Epoch 1221 : Total Loss 4.218\n",
      "Epoch 1231 : Total Loss 4.211\n",
      "Epoch 1241 : Total Loss 4.204\n",
      "Epoch 1251 : Total Loss 4.198\n",
      "Epoch 1261 : Total Loss 4.191\n",
      "Epoch 1271 : Total Loss 4.184\n",
      "Epoch 1281 : Total Loss 4.177\n",
      "Epoch 1291 : Total Loss 4.170\n",
      "Epoch 1301 : Total Loss 4.164\n",
      "Epoch 1311 : Total Loss 4.157\n",
      "Epoch 1321 : Total Loss 4.150\n",
      "Epoch 1331 : Total Loss 4.143\n",
      "Epoch 1341 : Total Loss 4.136\n",
      "Epoch 1351 : Total Loss 4.130\n",
      "Epoch 1361 : Total Loss 4.123\n",
      "Epoch 1371 : Total Loss 4.116\n",
      "Epoch 1381 : Total Loss 4.109\n",
      "Epoch 1391 : Total Loss 4.102\n",
      "Epoch 1401 : Total Loss 4.095\n",
      "Epoch 1411 : Total Loss 4.088\n",
      "Epoch 1421 : Total Loss 4.082\n",
      "Epoch 1431 : Total Loss 4.075\n",
      "Epoch 1441 : Total Loss 4.068\n",
      "Epoch 1451 : Total Loss 4.061\n",
      "Epoch 1461 : Total Loss 4.054\n",
      "Epoch 1471 : Total Loss 4.047\n",
      "Epoch 1481 : Total Loss 4.040\n",
      "Epoch 1491 : Total Loss 4.033\n",
      "Epoch 1501 : Total Loss 4.026\n",
      "Epoch 1511 : Total Loss 4.019\n",
      "Epoch 1521 : Total Loss 4.012\n",
      "Epoch 1531 : Total Loss 4.005\n",
      "Epoch 1541 : Total Loss 3.998\n",
      "Epoch 1551 : Total Loss 3.991\n",
      "Epoch 1561 : Total Loss 3.985\n",
      "Epoch 1571 : Total Loss 3.978\n",
      "Epoch 1581 : Total Loss 3.971\n",
      "Epoch 1591 : Total Loss 3.964\n",
      "Epoch 1601 : Total Loss 3.957\n",
      "Epoch 1611 : Total Loss 3.950\n",
      "Epoch 1621 : Total Loss 3.943\n",
      "Epoch 1631 : Total Loss 3.936\n",
      "Epoch 1641 : Total Loss 3.929\n",
      "Epoch 1651 : Total Loss 3.922\n",
      "Epoch 1661 : Total Loss 3.915\n",
      "Epoch 1671 : Total Loss 3.908\n",
      "Epoch 1681 : Total Loss 3.901\n",
      "Epoch 1691 : Total Loss 3.894\n",
      "Epoch 1701 : Total Loss 3.887\n",
      "Epoch 1711 : Total Loss 3.880\n",
      "Epoch 1721 : Total Loss 3.873\n",
      "Epoch 1731 : Total Loss 3.866\n",
      "Epoch 1741 : Total Loss 3.859\n",
      "Epoch 1751 : Total Loss 3.852\n",
      "Epoch 1761 : Total Loss 3.845\n",
      "Epoch 1771 : Total Loss 3.838\n",
      "Epoch 1781 : Total Loss 3.831\n",
      "Epoch 1791 : Total Loss 3.824\n",
      "Epoch 1801 : Total Loss 3.817\n",
      "Epoch 1811 : Total Loss 3.810\n",
      "Epoch 1821 : Total Loss 3.803\n",
      "Epoch 1831 : Total Loss 3.796\n",
      "Epoch 1841 : Total Loss 3.790\n",
      "Epoch 1851 : Total Loss 3.783\n",
      "Epoch 1861 : Total Loss 3.776\n",
      "Epoch 1871 : Total Loss 3.769\n",
      "Epoch 1881 : Total Loss 3.762\n",
      "Epoch 1891 : Total Loss 3.755\n",
      "Epoch 1901 : Total Loss 3.748\n",
      "Epoch 1911 : Total Loss 3.741\n",
      "Epoch 1921 : Total Loss 3.734\n",
      "Epoch 1931 : Total Loss 3.727\n",
      "Epoch 1941 : Total Loss 3.720\n",
      "Epoch 1951 : Total Loss 3.714\n",
      "Epoch 1961 : Total Loss 3.707\n",
      "Epoch 1971 : Total Loss 3.700\n",
      "Epoch 1981 : Total Loss 3.693\n",
      "Epoch 1991 : Total Loss 3.686\n",
      "Epoch 2001 : Total Loss 3.679\n",
      "Epoch 2011 : Total Loss 3.673\n",
      "Epoch 2021 : Total Loss 3.666\n",
      "Epoch 2031 : Total Loss 3.659\n",
      "Epoch 2041 : Total Loss 3.652\n",
      "Epoch 2051 : Total Loss 3.645\n",
      "Epoch 2061 : Total Loss 3.639\n",
      "Epoch 2071 : Total Loss 3.632\n",
      "Epoch 2081 : Total Loss 3.625\n",
      "Epoch 2091 : Total Loss 3.618\n",
      "Epoch 2101 : Total Loss 3.612\n",
      "Epoch 2111 : Total Loss 3.605\n",
      "Epoch 2121 : Total Loss 3.598\n",
      "Epoch 2131 : Total Loss 3.591\n",
      "Epoch 2141 : Total Loss 3.585\n",
      "Epoch 2151 : Total Loss 3.578\n",
      "Epoch 2161 : Total Loss 3.571\n",
      "Epoch 2171 : Total Loss 3.565\n",
      "Epoch 2181 : Total Loss 3.558\n",
      "Epoch 2191 : Total Loss 3.551\n",
      "Epoch 2201 : Total Loss 3.545\n",
      "Epoch 2211 : Total Loss 3.538\n",
      "Epoch 2221 : Total Loss 3.532\n",
      "Epoch 2231 : Total Loss 3.525\n",
      "Epoch 2241 : Total Loss 3.518\n",
      "Epoch 2251 : Total Loss 3.512\n",
      "Epoch 2261 : Total Loss 3.505\n",
      "Epoch 2271 : Total Loss 3.499\n",
      "Epoch 2281 : Total Loss 3.492\n",
      "Epoch 2291 : Total Loss 3.486\n",
      "Epoch 2301 : Total Loss 3.479\n",
      "Epoch 2311 : Total Loss 3.473\n",
      "Epoch 2321 : Total Loss 3.467\n",
      "Epoch 2331 : Total Loss 3.460\n",
      "Epoch 2341 : Total Loss 3.454\n",
      "Epoch 2351 : Total Loss 3.447\n",
      "Epoch 2361 : Total Loss 3.441\n",
      "Epoch 2371 : Total Loss 3.435\n",
      "Epoch 2381 : Total Loss 3.428\n",
      "Epoch 2391 : Total Loss 3.422\n",
      "Epoch 2401 : Total Loss 3.416\n",
      "Epoch 2411 : Total Loss 3.409\n",
      "Epoch 2421 : Total Loss 3.403\n",
      "Epoch 2431 : Total Loss 3.397\n",
      "Epoch 2441 : Total Loss 3.390\n",
      "Epoch 2451 : Total Loss 3.384\n",
      "Epoch 2461 : Total Loss 3.378\n",
      "Epoch 2471 : Total Loss 3.372\n",
      "Epoch 2481 : Total Loss 3.366\n",
      "Epoch 2491 : Total Loss 3.359\n",
      "Epoch 2501 : Total Loss 3.353\n",
      "Epoch 2511 : Total Loss 3.347\n",
      "Epoch 2521 : Total Loss 3.341\n",
      "Epoch 2531 : Total Loss 3.335\n",
      "Epoch 2541 : Total Loss 3.329\n",
      "Epoch 2551 : Total Loss 3.323\n",
      "Epoch 2561 : Total Loss 3.317\n",
      "Epoch 2571 : Total Loss 3.311\n",
      "Epoch 2581 : Total Loss 3.305\n",
      "Epoch 2591 : Total Loss 3.299\n",
      "Epoch 2601 : Total Loss 3.293\n",
      "Epoch 2611 : Total Loss 3.287\n",
      "Epoch 2621 : Total Loss 3.281\n",
      "Epoch 2631 : Total Loss 3.275\n",
      "Epoch 2641 : Total Loss 3.269\n",
      "Epoch 2651 : Total Loss 3.264\n",
      "Epoch 2661 : Total Loss 3.258\n",
      "Epoch 2671 : Total Loss 3.252\n",
      "Epoch 2681 : Total Loss 3.246\n",
      "Epoch 2691 : Total Loss 3.240\n",
      "Epoch 2701 : Total Loss 3.235\n",
      "Epoch 2711 : Total Loss 3.229\n",
      "Epoch 2721 : Total Loss 3.223\n",
      "Epoch 2731 : Total Loss 3.217\n",
      "Epoch 2741 : Total Loss 3.212\n",
      "Epoch 2751 : Total Loss 3.206\n",
      "Epoch 2761 : Total Loss 3.200\n",
      "Epoch 2771 : Total Loss 3.195\n",
      "Epoch 2781 : Total Loss 3.189\n",
      "Epoch 2791 : Total Loss 3.184\n",
      "Epoch 2801 : Total Loss 3.178\n",
      "Epoch 2811 : Total Loss 3.173\n",
      "Epoch 2821 : Total Loss 3.167\n",
      "Epoch 2831 : Total Loss 3.162\n",
      "Epoch 2841 : Total Loss 3.156\n",
      "Epoch 2851 : Total Loss 3.151\n",
      "Epoch 2861 : Total Loss 3.145\n",
      "Epoch 2871 : Total Loss 3.140\n",
      "Epoch 2881 : Total Loss 3.135\n",
      "Epoch 2891 : Total Loss 3.129\n",
      "Epoch 2901 : Total Loss 3.124\n",
      "Epoch 2911 : Total Loss 3.119\n",
      "Epoch 2921 : Total Loss 3.113\n",
      "Epoch 2931 : Total Loss 3.108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2941 : Total Loss 3.103\n",
      "Epoch 2951 : Total Loss 3.098\n",
      "Epoch 2961 : Total Loss 3.092\n",
      "Epoch 2971 : Total Loss 3.087\n",
      "Epoch 2981 : Total Loss 3.082\n",
      "Epoch 2991 : Total Loss 3.077\n",
      "Epoch 3001 : Total Loss 3.072\n",
      "Epoch 3011 : Total Loss 3.067\n",
      "Epoch 3021 : Total Loss 3.062\n",
      "Epoch 3031 : Total Loss 3.057\n",
      "Epoch 3041 : Total Loss 3.052\n",
      "Epoch 3051 : Total Loss 3.047\n",
      "Epoch 3061 : Total Loss 3.042\n",
      "Epoch 3071 : Total Loss 3.037\n",
      "Epoch 3081 : Total Loss 3.032\n",
      "Epoch 3091 : Total Loss 3.027\n",
      "Epoch 3101 : Total Loss 3.022\n",
      "Epoch 3111 : Total Loss 3.017\n",
      "Epoch 3121 : Total Loss 3.012\n",
      "Epoch 3131 : Total Loss 3.007\n",
      "Epoch 3141 : Total Loss 3.002\n",
      "Epoch 3151 : Total Loss 2.998\n",
      "Epoch 3161 : Total Loss 2.993\n",
      "Epoch 3171 : Total Loss 2.988\n",
      "Epoch 3181 : Total Loss 2.983\n",
      "Epoch 3191 : Total Loss 2.979\n",
      "Epoch 3201 : Total Loss 2.974\n",
      "Epoch 3211 : Total Loss 2.969\n",
      "Epoch 3221 : Total Loss 2.965\n",
      "Epoch 3231 : Total Loss 2.960\n",
      "Epoch 3241 : Total Loss 2.956\n",
      "Epoch 3251 : Total Loss 2.951\n",
      "Epoch 3261 : Total Loss 2.946\n",
      "Epoch 3271 : Total Loss 2.942\n",
      "Epoch 3281 : Total Loss 2.937\n",
      "Epoch 3291 : Total Loss 2.933\n",
      "Epoch 3301 : Total Loss 2.928\n",
      "Epoch 3311 : Total Loss 2.924\n",
      "Epoch 3321 : Total Loss 2.920\n",
      "Epoch 3331 : Total Loss 2.915\n",
      "Epoch 3341 : Total Loss 2.911\n",
      "Epoch 3351 : Total Loss 2.907\n",
      "Epoch 3361 : Total Loss 2.902\n",
      "Epoch 3371 : Total Loss 2.898\n",
      "Epoch 3381 : Total Loss 2.894\n",
      "Epoch 3391 : Total Loss 2.889\n",
      "Epoch 3401 : Total Loss 2.885\n",
      "Epoch 3411 : Total Loss 2.881\n",
      "Epoch 3421 : Total Loss 2.877\n",
      "Epoch 3431 : Total Loss 2.872\n",
      "Epoch 3441 : Total Loss 2.868\n",
      "Epoch 3451 : Total Loss 2.864\n",
      "Epoch 3461 : Total Loss 2.860\n",
      "Epoch 3471 : Total Loss 2.856\n",
      "Epoch 3481 : Total Loss 2.852\n",
      "Epoch 3491 : Total Loss 2.848\n",
      "Epoch 3501 : Total Loss 2.844\n",
      "Epoch 3511 : Total Loss 2.840\n",
      "Epoch 3521 : Total Loss 2.836\n",
      "Epoch 3531 : Total Loss 2.832\n",
      "Epoch 3541 : Total Loss 2.828\n",
      "Epoch 3551 : Total Loss 2.824\n",
      "Epoch 3561 : Total Loss 2.820\n",
      "Epoch 3571 : Total Loss 2.816\n",
      "Epoch 3581 : Total Loss 2.812\n",
      "Epoch 3591 : Total Loss 2.808\n",
      "Epoch 3601 : Total Loss 2.804\n",
      "Epoch 3611 : Total Loss 2.801\n",
      "Epoch 3621 : Total Loss 2.797\n",
      "Epoch 3631 : Total Loss 2.793\n",
      "Epoch 3641 : Total Loss 2.789\n",
      "Epoch 3651 : Total Loss 2.786\n",
      "Epoch 3661 : Total Loss 2.782\n",
      "Epoch 3671 : Total Loss 2.778\n",
      "Epoch 3681 : Total Loss 2.774\n",
      "Epoch 3691 : Total Loss 2.771\n",
      "Epoch 3701 : Total Loss 2.767\n",
      "Epoch 3711 : Total Loss 2.763\n",
      "Epoch 3721 : Total Loss 2.760\n",
      "Epoch 3731 : Total Loss 2.756\n",
      "Epoch 3741 : Total Loss 2.753\n",
      "Epoch 3751 : Total Loss 2.749\n",
      "Epoch 3761 : Total Loss 2.746\n",
      "Epoch 3771 : Total Loss 2.742\n",
      "Epoch 3781 : Total Loss 2.739\n",
      "Epoch 3791 : Total Loss 2.735\n",
      "Epoch 3801 : Total Loss 2.732\n",
      "Epoch 3811 : Total Loss 2.728\n",
      "Epoch 3821 : Total Loss 2.725\n",
      "Epoch 3831 : Total Loss 2.721\n",
      "Epoch 3841 : Total Loss 2.718\n",
      "Epoch 3851 : Total Loss 2.715\n",
      "Epoch 3861 : Total Loss 2.711\n",
      "Epoch 3871 : Total Loss 2.708\n",
      "Epoch 3881 : Total Loss 2.705\n",
      "Epoch 3891 : Total Loss 2.701\n",
      "Epoch 3901 : Total Loss 2.698\n",
      "Epoch 3911 : Total Loss 2.695\n",
      "Epoch 3921 : Total Loss 2.692\n",
      "Epoch 3931 : Total Loss 2.688\n",
      "Epoch 3941 : Total Loss 2.685\n",
      "Epoch 3951 : Total Loss 2.682\n",
      "Epoch 3961 : Total Loss 2.679\n",
      "Epoch 3971 : Total Loss 2.676\n",
      "Epoch 3981 : Total Loss 2.673\n",
      "Epoch 3991 : Total Loss 2.669\n",
      "Epoch 4001 : Total Loss 2.666\n",
      "Epoch 4011 : Total Loss 2.663\n",
      "Epoch 4021 : Total Loss 2.660\n",
      "Epoch 4031 : Total Loss 2.657\n",
      "Epoch 4041 : Total Loss 2.654\n",
      "Epoch 4051 : Total Loss 2.651\n",
      "Epoch 4061 : Total Loss 2.648\n",
      "Epoch 4071 : Total Loss 2.645\n",
      "Epoch 4081 : Total Loss 2.642\n",
      "Epoch 4091 : Total Loss 2.639\n",
      "Epoch 4101 : Total Loss 2.636\n",
      "Epoch 4111 : Total Loss 2.633\n",
      "Epoch 4121 : Total Loss 2.630\n",
      "Epoch 4131 : Total Loss 2.627\n",
      "Epoch 4141 : Total Loss 2.625\n",
      "Epoch 4151 : Total Loss 2.622\n",
      "Epoch 4161 : Total Loss 2.619\n",
      "Epoch 4171 : Total Loss 2.616\n",
      "Epoch 4181 : Total Loss 2.613\n",
      "Epoch 4191 : Total Loss 2.610\n",
      "Epoch 4201 : Total Loss 2.608\n",
      "Epoch 4211 : Total Loss 2.605\n",
      "Epoch 4221 : Total Loss 2.602\n",
      "Epoch 4231 : Total Loss 2.599\n",
      "Epoch 4241 : Total Loss 2.597\n",
      "Epoch 4251 : Total Loss 2.594\n",
      "Epoch 4261 : Total Loss 2.591\n",
      "Epoch 4271 : Total Loss 2.589\n",
      "Epoch 4281 : Total Loss 2.586\n",
      "Epoch 4291 : Total Loss 2.583\n",
      "Epoch 4301 : Total Loss 2.581\n",
      "Epoch 4311 : Total Loss 2.578\n",
      "Epoch 4321 : Total Loss 2.575\n",
      "Epoch 4331 : Total Loss 2.573\n",
      "Epoch 4341 : Total Loss 2.570\n",
      "Epoch 4351 : Total Loss 2.568\n",
      "Epoch 4361 : Total Loss 2.565\n",
      "Epoch 4371 : Total Loss 2.563\n",
      "Epoch 4381 : Total Loss 2.560\n",
      "Epoch 4391 : Total Loss 2.558\n",
      "Epoch 4401 : Total Loss 2.555\n",
      "Epoch 4411 : Total Loss 2.553\n",
      "Epoch 4421 : Total Loss 2.550\n",
      "Epoch 4431 : Total Loss 2.548\n",
      "Epoch 4441 : Total Loss 2.545\n",
      "Epoch 4451 : Total Loss 2.543\n",
      "Epoch 4461 : Total Loss 2.540\n",
      "Epoch 4471 : Total Loss 2.538\n",
      "Epoch 4481 : Total Loss 2.536\n",
      "Epoch 4491 : Total Loss 2.533\n",
      "Epoch 4501 : Total Loss 2.531\n",
      "Epoch 4511 : Total Loss 2.528\n",
      "Epoch 4521 : Total Loss 2.526\n",
      "Epoch 4531 : Total Loss 2.524\n",
      "Epoch 4541 : Total Loss 2.521\n",
      "Epoch 4551 : Total Loss 2.519\n",
      "Epoch 4561 : Total Loss 2.517\n",
      "Epoch 4571 : Total Loss 2.515\n",
      "Epoch 4581 : Total Loss 2.512\n",
      "Epoch 4591 : Total Loss 2.510\n",
      "Epoch 4601 : Total Loss 2.508\n",
      "Epoch 4611 : Total Loss 2.506\n",
      "Epoch 4621 : Total Loss 2.503\n",
      "Epoch 4631 : Total Loss 2.501\n",
      "Epoch 4641 : Total Loss 2.499\n",
      "Epoch 4651 : Total Loss 2.497\n",
      "Epoch 4661 : Total Loss 2.495\n",
      "Epoch 4671 : Total Loss 2.493\n",
      "Epoch 4681 : Total Loss 2.490\n",
      "Epoch 4691 : Total Loss 2.488\n",
      "Epoch 4701 : Total Loss 2.486\n",
      "Epoch 4711 : Total Loss 2.484\n",
      "Epoch 4721 : Total Loss 2.482\n",
      "Epoch 4731 : Total Loss 2.480\n",
      "Epoch 4741 : Total Loss 2.478\n",
      "Epoch 4751 : Total Loss 2.476\n",
      "Epoch 4761 : Total Loss 2.474\n",
      "Epoch 4771 : Total Loss 2.472\n",
      "Epoch 4781 : Total Loss 2.470\n",
      "Epoch 4791 : Total Loss 2.468\n",
      "Epoch 4801 : Total Loss 2.466\n",
      "Epoch 4811 : Total Loss 2.464\n",
      "Epoch 4821 : Total Loss 2.462\n",
      "Epoch 4831 : Total Loss 2.460\n",
      "Epoch 4841 : Total Loss 2.458\n",
      "Epoch 4851 : Total Loss 2.456\n",
      "Epoch 4861 : Total Loss 2.454\n",
      "Epoch 4871 : Total Loss 2.452\n",
      "Epoch 4881 : Total Loss 2.450\n",
      "Epoch 4891 : Total Loss 2.448\n",
      "Epoch 4901 : Total Loss 2.446\n",
      "Epoch 4911 : Total Loss 2.444\n",
      "Epoch 4921 : Total Loss 2.442\n",
      "Epoch 4931 : Total Loss 2.440\n",
      "Epoch 4941 : Total Loss 2.438\n",
      "Epoch 4951 : Total Loss 2.437\n",
      "Epoch 4961 : Total Loss 2.435\n",
      "Epoch 4971 : Total Loss 2.433\n",
      "Epoch 4981 : Total Loss 2.431\n",
      "Epoch 4991 : Total Loss 2.429\n",
      "The runtime of the sparse matrix factorization is  2.464422699995339 seconds.\n"
     ]
    }
   ],
   "source": [
    "start2 = t.default_timer()\n",
    "\n",
    "U_hat_sparse, V_hat_sparse = matrix_factorization_sparse(A, U, V, user_features, 5000)\n",
    "\n",
    "end2 = t.default_timer()\n",
    "\n",
    "print('The runtime of the sparse matrix factorization is ', end2 - start2, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86909283",
   "metadata": {},
   "source": [
    "There is not a significant difference on these, however, the $A$ that we used in the **above example is not sparse at all.** 14/25 entries are nonzero, so the sparsity is only 44%. **We will try this method on a significantly more sparse matrix.**\n",
    "\n",
    "## Test on a Large Sparse Matrix\n",
    "\n",
    "**This will be a 100 x 100 sparse matrix with 2 latent features, and it will be 99% sparse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f57a545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate random sparse matrix\n",
    "from scipy.sparse import random\n",
    "from scipy.stats import randint\n",
    "arr_rvs = randint(0, 6).rvs\n",
    "\n",
    "x = random(100,100, density = 0.01, data_rvs = arr_rvs)\n",
    "\n",
    "arr_100_100 = np.array(x.toarray())\n",
    "\n",
    "arr_100_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd8d275e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4.0,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  5.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  3.0],\n",
       " 84)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_ind = [(i,j) for i in range(arr_100_100.shape[0]) for j in range(arr_100_100.shape[1]) if arr_100_100[i,j]!=0]\n",
    "\n",
    "li_arr_100_100 = [arr_100_100[i,j] for i,j in li_ind]\n",
    "\n",
    "li_arr_100_100, len(li_arr_100_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85a5c1",
   "metadata": {},
   "source": [
    "### Run the Sparse Matrix Factorization on our Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b300ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[4.17022005e-01 7.20324493e-01]\n",
      " [1.14374817e-04 3.02332573e-01]\n",
      " [1.46755891e-01 9.23385948e-02]\n",
      " [1.86260211e-01 3.45560727e-01]\n",
      " [3.96767474e-01 5.38816734e-01]\n",
      " [4.19194514e-01 6.85219500e-01]\n",
      " [2.04452250e-01 8.78117436e-01]\n",
      " [2.73875932e-02 6.70467510e-01]\n",
      " [4.17304802e-01 5.58689828e-01]\n",
      " [1.40386939e-01 1.98101489e-01]\n",
      " [8.00744569e-01 9.68261576e-01]\n",
      " [3.13424178e-01 6.92322616e-01]\n",
      " [8.76389152e-01 8.94606664e-01]\n",
      " [8.50442114e-02 3.90547832e-02]\n",
      " [1.69830420e-01 8.78142503e-01]\n",
      " [9.83468338e-02 4.21107625e-01]\n",
      " [9.57889530e-01 5.33165285e-01]\n",
      " [6.91877114e-01 3.15515631e-01]\n",
      " [6.86500928e-01 8.34625672e-01]\n",
      " [1.82882773e-02 7.50144315e-01]\n",
      " [9.88861089e-01 7.48165654e-01]\n",
      " [2.80443992e-01 7.89279328e-01]\n",
      " [1.03226007e-01 4.47893526e-01]\n",
      " [9.08595503e-01 2.93614148e-01]\n",
      " [2.87775339e-01 1.30028572e-01]\n",
      " [1.93669579e-02 6.78835533e-01]\n",
      " [2.11628116e-01 2.65546659e-01]\n",
      " [4.91573159e-01 5.33625451e-02]\n",
      " [5.74117605e-01 1.46728575e-01]\n",
      " [5.89305537e-01 6.99758360e-01]\n",
      " [1.02334429e-01 4.14055988e-01]\n",
      " [6.94400158e-01 4.14179270e-01]\n",
      " [4.99534589e-02 5.35896406e-01]\n",
      " [6.63794645e-01 5.14889112e-01]\n",
      " [9.44594756e-01 5.86555041e-01]\n",
      " [9.03401915e-01 1.37474704e-01]\n",
      " [1.39276347e-01 8.07391289e-01]\n",
      " [3.97676837e-01 1.65354197e-01]\n",
      " [9.27508580e-01 3.47765860e-01]\n",
      " [7.50812103e-01 7.25997985e-01]\n",
      " [8.83306091e-01 6.23672207e-01]\n",
      " [7.50942434e-01 3.48898342e-01]\n",
      " [2.69927892e-01 8.95886218e-01]\n",
      " [4.28091190e-01 9.64840047e-01]\n",
      " [6.63441498e-01 6.21695720e-01]\n",
      " [1.14745973e-01 9.49489259e-01]\n",
      " [4.49912133e-01 5.78389614e-01]\n",
      " [4.08136803e-01 2.37026980e-01]\n",
      " [9.03379521e-01 5.73679487e-01]\n",
      " [2.87032703e-03 6.17144914e-01]\n",
      " [3.26644902e-01 5.27058102e-01]\n",
      " [8.85942099e-01 3.57269760e-01]\n",
      " [9.08535151e-01 6.23360116e-01]\n",
      " [1.58212428e-02 9.29437234e-01]\n",
      " [6.90896918e-01 9.97322850e-01]\n",
      " [1.72340508e-01 1.37135750e-01]\n",
      " [9.32595463e-01 6.96818161e-01]\n",
      " [6.60001727e-02 7.55463053e-01]\n",
      " [7.53876188e-01 9.23024536e-01]\n",
      " [7.11524759e-01 1.24270962e-01]\n",
      " [1.98801338e-02 2.62109869e-02]\n",
      " [2.83064880e-02 2.46211068e-01]\n",
      " [8.60027949e-01 5.38831064e-01]\n",
      " [5.52821979e-01 8.42030892e-01]\n",
      " [1.24173315e-01 2.79183679e-01]\n",
      " [5.85759271e-01 9.69595748e-01]\n",
      " [5.61030219e-01 1.86472894e-02]\n",
      " [8.00632673e-01 2.32974274e-01]\n",
      " [8.07105196e-01 3.87860644e-01]\n",
      " [8.63541855e-01 7.47121643e-01]\n",
      " [5.56240234e-01 1.36455226e-01]\n",
      " [5.99176895e-02 1.21343456e-01]\n",
      " [4.45518785e-02 1.07494129e-01]\n",
      " [2.25709339e-01 7.12988980e-01]\n",
      " [5.59716982e-01 1.25559802e-02]\n",
      " [7.19742797e-02 9.67276330e-01]\n",
      " [5.68100462e-01 2.03293235e-01]\n",
      " [2.52325745e-01 7.43825854e-01]\n",
      " [1.95429481e-01 5.81358927e-01]\n",
      " [9.70019989e-01 8.46828801e-01]\n",
      " [2.39847759e-01 4.93769714e-01]\n",
      " [6.19955718e-01 8.28980900e-01]\n",
      " [1.56791395e-01 1.85762022e-02]\n",
      " [7.00221437e-02 4.86345111e-01]\n",
      " [6.06329462e-01 5.68851437e-01]\n",
      " [3.17362409e-01 9.88616154e-01]\n",
      " [5.79745219e-01 3.80141173e-01]\n",
      " [5.50948219e-01 7.45334431e-01]\n",
      " [6.69232893e-01 2.64919558e-01]\n",
      " [6.63348344e-02 3.70084198e-01]\n",
      " [6.29717507e-01 2.10174010e-01]\n",
      " [7.52755554e-01 6.65364814e-02]\n",
      " [2.60315099e-01 8.04754564e-01]\n",
      " [1.93434283e-01 6.39460881e-01]\n",
      " [5.24670309e-01 9.24807970e-01]\n",
      " [2.63296770e-01 6.59610907e-02]\n",
      " [7.35065963e-01 7.72178030e-01]\n",
      " [9.07815853e-01 9.31972069e-01]\n",
      " [1.39515730e-02 2.34362086e-01]\n",
      " [6.16778357e-01 9.49016321e-01]]\n",
      "V is:\n",
      " [[9.50176119e-01 5.56653188e-01]\n",
      " [9.15606350e-01 6.41566209e-01]\n",
      " [3.90007714e-01 4.85990667e-01]\n",
      " [6.04310483e-01 5.49547922e-01]\n",
      " [9.26181427e-01 9.18733436e-01]\n",
      " [3.94875613e-01 9.63262528e-01]\n",
      " [1.73955667e-01 1.26329519e-01]\n",
      " [1.35079158e-01 5.05662166e-01]\n",
      " [2.15248053e-02 9.47970211e-01]\n",
      " [8.27115471e-01 1.50189807e-02]\n",
      " [1.76196256e-01 3.32063574e-01]\n",
      " [1.30996845e-01 8.09490692e-01]\n",
      " [3.44736653e-01 9.40107482e-01]\n",
      " [5.82014180e-01 8.78831984e-01]\n",
      " [8.44734445e-01 9.05392319e-01]\n",
      " [4.59880266e-01 5.46346816e-01]\n",
      " [7.98603591e-01 2.85718852e-01]\n",
      " [4.90253523e-01 5.99110308e-01]\n",
      " [1.55332756e-02 5.93481408e-01]\n",
      " [4.33676349e-01 8.07360529e-01]\n",
      " [3.15244803e-01 8.92888709e-01]\n",
      " [5.77857215e-01 1.84010202e-01]\n",
      " [7.87929234e-01 6.12031177e-01]\n",
      " [5.39092721e-02 4.20193680e-01]\n",
      " [6.79068837e-01 9.18601778e-01]\n",
      " [4.02024891e-04 9.76759149e-01]\n",
      " [3.76580315e-01 9.73783538e-01]\n",
      " [6.04716101e-01 8.28845808e-01]\n",
      " [5.74711505e-01 6.28076198e-01]\n",
      " [2.85576282e-01 5.86833341e-01]\n",
      " [7.50021764e-01 8.58313836e-01]\n",
      " [7.55082188e-01 6.98057248e-01]\n",
      " [8.64479430e-01 3.22680997e-01]\n",
      " [6.70788791e-01 4.50873936e-01]\n",
      " [3.82102752e-01 4.10811350e-01]\n",
      " [4.01479583e-01 3.17383946e-01]\n",
      " [6.21919368e-01 4.30247271e-01]\n",
      " [9.73802078e-01 6.77800891e-01]\n",
      " [1.98569888e-01 4.26701009e-01]\n",
      " [3.43346240e-01 7.97638804e-01]\n",
      " [8.79998289e-01 9.03841956e-01]\n",
      " [6.62719812e-01 2.70208262e-01]\n",
      " [2.52366702e-01 8.54897943e-01]\n",
      " [5.27714646e-01 8.02161084e-01]\n",
      " [5.72488517e-01 7.33142525e-01]\n",
      " [5.19011627e-01 7.70883911e-01]\n",
      " [5.68857991e-01 4.65709879e-01]\n",
      " [3.42688908e-01 6.82093484e-02]\n",
      " [3.77924179e-01 7.96260777e-02]\n",
      " [9.82817114e-01 1.81612851e-01]\n",
      " [8.11858698e-01 8.74961645e-01]\n",
      " [6.88413252e-01 5.69494413e-01]\n",
      " [1.60971437e-01 4.66880023e-01]\n",
      " [3.45172051e-01 2.25039958e-01]\n",
      " [5.92511869e-01 3.12269838e-01]\n",
      " [9.16305553e-01 9.09635525e-01]\n",
      " [2.57118294e-01 1.10891301e-01]\n",
      " [1.92962732e-01 4.99584171e-01]\n",
      " [7.28585668e-01 2.08194438e-01]\n",
      " [2.48033558e-01 8.51671875e-01]\n",
      " [4.15848718e-01 6.16685067e-01]\n",
      " [2.33666139e-01 1.01967259e-01]\n",
      " [5.15857017e-01 4.77140987e-01]\n",
      " [1.52671644e-01 6.21806232e-01]\n",
      " [5.44010119e-01 6.54137347e-01]\n",
      " [1.44545540e-01 7.51527817e-01]\n",
      " [2.22049140e-01 5.19351824e-01]\n",
      " [7.85296028e-01 2.23304280e-02]\n",
      " [3.24362460e-01 8.72922376e-01]\n",
      " [8.44709608e-01 5.38440593e-01]\n",
      " [8.66608274e-01 9.49805991e-01]\n",
      " [8.26406998e-01 8.54115444e-01]\n",
      " [9.87434018e-02 6.51304332e-01]\n",
      " [7.03516988e-01 6.10240813e-01]\n",
      " [7.99615262e-01 3.45712199e-02]\n",
      " [7.70238735e-01 7.31728601e-01]\n",
      " [2.59698393e-01 2.57069299e-01]\n",
      " [6.32303317e-01 3.45297462e-01]\n",
      " [7.96588678e-01 4.46146232e-01]\n",
      " [7.82749415e-01 9.90471784e-01]\n",
      " [3.00248340e-01 1.43005828e-01]\n",
      " [9.01308436e-01 5.41559379e-01]\n",
      " [9.74740371e-01 6.36604400e-01]\n",
      " [9.93913025e-01 5.46070804e-01]\n",
      " [5.26425934e-01 1.35427903e-01]\n",
      " [3.55705171e-01 2.62185673e-02]\n",
      " [1.60395180e-01 7.45637193e-01]\n",
      " [3.03996899e-02 3.66543097e-01]\n",
      " [8.62346253e-01 6.92677718e-01]\n",
      " [6.90942142e-01 1.88636801e-01]\n",
      " [4.41904281e-01 5.81577407e-01]\n",
      " [9.89751708e-01 2.03906225e-01]\n",
      " [2.47732902e-01 2.62173084e-01]\n",
      " [7.50172413e-01 4.56975327e-01]\n",
      " [5.69294384e-02 5.08516241e-01]\n",
      " [2.11960165e-01 7.98604245e-01]\n",
      " [2.97331382e-01 2.76060120e-02]\n",
      " [5.93432449e-01 8.43840429e-01]\n",
      " [3.81016124e-01 7.49858311e-01]\n",
      " [5.11141478e-01 5.40951805e-01]]\n",
      "Epoch 1 : Total Loss 598.880\n",
      "Epoch 11 : Total Loss 590.512\n",
      "Epoch 21 : Total Loss 582.027\n",
      "Epoch 31 : Total Loss 573.428\n",
      "Epoch 41 : Total Loss 564.718\n",
      "Epoch 51 : Total Loss 555.900\n",
      "Epoch 61 : Total Loss 546.979\n",
      "Epoch 71 : Total Loss 537.961\n",
      "Epoch 81 : Total Loss 528.851\n",
      "Epoch 91 : Total Loss 519.656\n",
      "Epoch 101 : Total Loss 510.385\n",
      "Epoch 111 : Total Loss 501.045\n",
      "Epoch 121 : Total Loss 491.646\n",
      "Epoch 131 : Total Loss 482.197\n",
      "Epoch 141 : Total Loss 472.709\n",
      "Epoch 151 : Total Loss 463.193\n",
      "Epoch 161 : Total Loss 453.660\n",
      "Epoch 171 : Total Loss 444.122\n",
      "Epoch 181 : Total Loss 434.591\n",
      "Epoch 191 : Total Loss 425.079\n",
      "Epoch 201 : Total Loss 415.598\n",
      "Epoch 211 : Total Loss 406.161\n",
      "Epoch 221 : Total Loss 396.780\n",
      "Epoch 231 : Total Loss 387.465\n",
      "Epoch 241 : Total Loss 378.230\n",
      "Epoch 251 : Total Loss 369.084\n",
      "Epoch 261 : Total Loss 360.038\n",
      "Epoch 271 : Total Loss 351.103\n",
      "Epoch 281 : Total Loss 342.286\n",
      "Epoch 291 : Total Loss 333.597\n",
      "Epoch 301 : Total Loss 325.042\n",
      "Epoch 311 : Total Loss 316.630\n",
      "Epoch 321 : Total Loss 308.365\n",
      "Epoch 331 : Total Loss 300.253\n",
      "Epoch 341 : Total Loss 292.298\n",
      "Epoch 351 : Total Loss 284.504\n",
      "Epoch 361 : Total Loss 276.873\n",
      "Epoch 371 : Total Loss 269.408\n",
      "Epoch 381 : Total Loss 262.110\n",
      "Epoch 391 : Total Loss 254.979\n",
      "Epoch 401 : Total Loss 248.016\n",
      "Epoch 411 : Total Loss 241.221\n",
      "Epoch 421 : Total Loss 234.594\n",
      "Epoch 431 : Total Loss 228.132\n",
      "Epoch 441 : Total Loss 221.835\n",
      "Epoch 451 : Total Loss 215.701\n",
      "Epoch 461 : Total Loss 209.729\n",
      "Epoch 471 : Total Loss 203.915\n",
      "Epoch 481 : Total Loss 198.259\n",
      "Epoch 491 : Total Loss 192.757\n",
      "Epoch 501 : Total Loss 187.408\n",
      "Epoch 511 : Total Loss 182.208\n",
      "Epoch 521 : Total Loss 177.155\n",
      "Epoch 531 : Total Loss 172.247\n",
      "Epoch 541 : Total Loss 167.481\n",
      "Epoch 551 : Total Loss 162.854\n",
      "Epoch 561 : Total Loss 158.364\n",
      "Epoch 571 : Total Loss 154.008\n",
      "Epoch 581 : Total Loss 149.783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591 : Total Loss 145.687\n",
      "Epoch 601 : Total Loss 141.717\n",
      "Epoch 611 : Total Loss 137.869\n",
      "Epoch 621 : Total Loss 134.143\n",
      "Epoch 631 : Total Loss 130.534\n",
      "Epoch 641 : Total Loss 127.039\n",
      "Epoch 651 : Total Loss 123.657\n",
      "Epoch 661 : Total Loss 120.384\n",
      "Epoch 671 : Total Loss 117.218\n",
      "Epoch 681 : Total Loss 114.156\n",
      "Epoch 691 : Total Loss 111.194\n",
      "Epoch 701 : Total Loss 108.331\n",
      "Epoch 711 : Total Loss 105.563\n",
      "Epoch 721 : Total Loss 102.887\n",
      "Epoch 731 : Total Loss 100.301\n",
      "Epoch 741 : Total Loss 97.802\n",
      "Epoch 751 : Total Loss 95.387\n",
      "Epoch 761 : Total Loss 93.053\n",
      "Epoch 771 : Total Loss 90.798\n",
      "Epoch 781 : Total Loss 88.618\n",
      "Epoch 791 : Total Loss 86.512\n",
      "Epoch 801 : Total Loss 84.476\n",
      "Epoch 811 : Total Loss 82.508\n",
      "Epoch 821 : Total Loss 80.606\n",
      "Epoch 831 : Total Loss 78.766\n",
      "Epoch 841 : Total Loss 76.987\n",
      "Epoch 851 : Total Loss 75.265\n",
      "Epoch 861 : Total Loss 73.600\n",
      "Epoch 871 : Total Loss 71.988\n",
      "Epoch 881 : Total Loss 70.427\n",
      "Epoch 891 : Total Loss 68.916\n",
      "Epoch 901 : Total Loss 67.451\n",
      "Epoch 911 : Total Loss 66.032\n",
      "Epoch 921 : Total Loss 64.657\n",
      "Epoch 931 : Total Loss 63.323\n",
      "Epoch 941 : Total Loss 62.029\n",
      "Epoch 951 : Total Loss 60.773\n",
      "Epoch 961 : Total Loss 59.553\n",
      "Epoch 971 : Total Loss 58.369\n",
      "Epoch 981 : Total Loss 57.218\n",
      "Epoch 991 : Total Loss 56.100\n",
      "Epoch 1001 : Total Loss 55.013\n",
      "Epoch 1011 : Total Loss 53.955\n",
      "Epoch 1021 : Total Loss 52.926\n",
      "Epoch 1031 : Total Loss 51.924\n",
      "Epoch 1041 : Total Loss 50.949\n",
      "Epoch 1051 : Total Loss 49.999\n",
      "Epoch 1061 : Total Loss 49.074\n",
      "Epoch 1071 : Total Loss 48.172\n",
      "Epoch 1081 : Total Loss 47.292\n",
      "Epoch 1091 : Total Loss 46.435\n",
      "Epoch 1101 : Total Loss 45.599\n",
      "Epoch 1111 : Total Loss 44.783\n",
      "Epoch 1121 : Total Loss 43.987\n",
      "Epoch 1131 : Total Loss 43.209\n",
      "Epoch 1141 : Total Loss 42.451\n",
      "Epoch 1151 : Total Loss 41.710\n",
      "Epoch 1161 : Total Loss 40.986\n",
      "Epoch 1171 : Total Loss 40.280\n",
      "Epoch 1181 : Total Loss 39.589\n",
      "Epoch 1191 : Total Loss 38.915\n",
      "Epoch 1201 : Total Loss 38.256\n",
      "Epoch 1211 : Total Loss 37.612\n",
      "Epoch 1221 : Total Loss 36.982\n",
      "Epoch 1231 : Total Loss 36.367\n",
      "Epoch 1241 : Total Loss 35.765\n",
      "Epoch 1251 : Total Loss 35.177\n",
      "Epoch 1261 : Total Loss 34.602\n",
      "Epoch 1271 : Total Loss 34.040\n",
      "Epoch 1281 : Total Loss 33.491\n",
      "Epoch 1291 : Total Loss 32.953\n",
      "Epoch 1301 : Total Loss 32.428\n",
      "Epoch 1311 : Total Loss 31.914\n",
      "Epoch 1321 : Total Loss 31.412\n",
      "Epoch 1331 : Total Loss 30.921\n",
      "Epoch 1341 : Total Loss 30.440\n",
      "Epoch 1351 : Total Loss 29.970\n",
      "Epoch 1361 : Total Loss 29.511\n",
      "Epoch 1371 : Total Loss 29.062\n",
      "Epoch 1381 : Total Loss 28.622\n",
      "Epoch 1391 : Total Loss 28.193\n",
      "Epoch 1401 : Total Loss 27.772\n",
      "Epoch 1411 : Total Loss 27.362\n",
      "Epoch 1421 : Total Loss 26.960\n",
      "Epoch 1431 : Total Loss 26.567\n",
      "Epoch 1441 : Total Loss 26.183\n",
      "Epoch 1451 : Total Loss 25.807\n",
      "Epoch 1461 : Total Loss 25.440\n",
      "Epoch 1471 : Total Loss 25.080\n",
      "Epoch 1481 : Total Loss 24.729\n",
      "Epoch 1491 : Total Loss 24.386\n",
      "Epoch 1501 : Total Loss 24.050\n",
      "Epoch 1511 : Total Loss 23.721\n",
      "Epoch 1521 : Total Loss 23.400\n",
      "Epoch 1531 : Total Loss 23.086\n",
      "Epoch 1541 : Total Loss 22.779\n",
      "Epoch 1551 : Total Loss 22.479\n",
      "Epoch 1561 : Total Loss 22.185\n",
      "Epoch 1571 : Total Loss 21.898\n",
      "Epoch 1581 : Total Loss 21.617\n",
      "Epoch 1591 : Total Loss 21.343\n",
      "Epoch 1601 : Total Loss 21.075\n",
      "Epoch 1611 : Total Loss 20.812\n",
      "Epoch 1621 : Total Loss 20.556\n",
      "Epoch 1631 : Total Loss 20.305\n",
      "Epoch 1641 : Total Loss 20.059\n",
      "Epoch 1651 : Total Loss 19.819\n",
      "Epoch 1661 : Total Loss 19.585\n",
      "Epoch 1671 : Total Loss 19.355\n",
      "Epoch 1681 : Total Loss 19.131\n",
      "Epoch 1691 : Total Loss 18.911\n",
      "Epoch 1701 : Total Loss 18.697\n",
      "Epoch 1711 : Total Loss 18.487\n",
      "Epoch 1721 : Total Loss 18.282\n",
      "Epoch 1731 : Total Loss 18.081\n",
      "Epoch 1741 : Total Loss 17.885\n",
      "Epoch 1751 : Total Loss 17.693\n",
      "Epoch 1761 : Total Loss 17.505\n",
      "Epoch 1771 : Total Loss 17.321\n",
      "Epoch 1781 : Total Loss 17.141\n",
      "Epoch 1791 : Total Loss 16.966\n",
      "Epoch 1801 : Total Loss 16.794\n",
      "Epoch 1811 : Total Loss 16.625\n",
      "Epoch 1821 : Total Loss 16.461\n",
      "Epoch 1831 : Total Loss 16.300\n",
      "Epoch 1841 : Total Loss 16.142\n",
      "Epoch 1851 : Total Loss 15.988\n",
      "Epoch 1861 : Total Loss 15.837\n",
      "Epoch 1871 : Total Loss 15.689\n",
      "Epoch 1881 : Total Loss 15.545\n",
      "Epoch 1891 : Total Loss 15.403\n",
      "Epoch 1901 : Total Loss 15.265\n",
      "Epoch 1911 : Total Loss 15.130\n",
      "Epoch 1921 : Total Loss 14.997\n",
      "Epoch 1931 : Total Loss 14.867\n",
      "Epoch 1941 : Total Loss 14.740\n",
      "Epoch 1951 : Total Loss 14.615\n",
      "Epoch 1961 : Total Loss 14.493\n",
      "Epoch 1971 : Total Loss 14.374\n",
      "Epoch 1981 : Total Loss 14.257\n",
      "Epoch 1991 : Total Loss 14.142\n",
      "Epoch 2001 : Total Loss 14.030\n",
      "Epoch 2011 : Total Loss 13.920\n",
      "Epoch 2021 : Total Loss 13.812\n",
      "Epoch 2031 : Total Loss 13.707\n",
      "Epoch 2041 : Total Loss 13.603\n",
      "Epoch 2051 : Total Loss 13.502\n",
      "Epoch 2061 : Total Loss 13.403\n",
      "Epoch 2071 : Total Loss 13.305\n",
      "Epoch 2081 : Total Loss 13.210\n",
      "Epoch 2091 : Total Loss 13.116\n",
      "Epoch 2101 : Total Loss 13.024\n",
      "Epoch 2111 : Total Loss 12.934\n",
      "Epoch 2121 : Total Loss 12.846\n",
      "Epoch 2131 : Total Loss 12.759\n",
      "Epoch 2141 : Total Loss 12.674\n",
      "Epoch 2151 : Total Loss 12.590\n",
      "Epoch 2161 : Total Loss 12.508\n",
      "Epoch 2171 : Total Loss 12.428\n",
      "Epoch 2181 : Total Loss 12.349\n",
      "Epoch 2191 : Total Loss 12.272\n",
      "Epoch 2201 : Total Loss 12.195\n",
      "Epoch 2211 : Total Loss 12.121\n",
      "Epoch 2221 : Total Loss 12.047\n",
      "Epoch 2231 : Total Loss 11.975\n",
      "Epoch 2241 : Total Loss 11.904\n",
      "Epoch 2251 : Total Loss 11.835\n",
      "Epoch 2261 : Total Loss 11.766\n",
      "Epoch 2271 : Total Loss 11.699\n",
      "Epoch 2281 : Total Loss 11.633\n",
      "Epoch 2291 : Total Loss 11.568\n",
      "Epoch 2301 : Total Loss 11.504\n",
      "Epoch 2311 : Total Loss 11.441\n",
      "Epoch 2321 : Total Loss 11.379\n",
      "Epoch 2331 : Total Loss 11.318\n",
      "Epoch 2341 : Total Loss 11.258\n",
      "Epoch 2351 : Total Loss 11.199\n",
      "Epoch 2361 : Total Loss 11.141\n",
      "Epoch 2371 : Total Loss 11.084\n",
      "Epoch 2381 : Total Loss 11.028\n",
      "Epoch 2391 : Total Loss 10.973\n",
      "Epoch 2401 : Total Loss 10.918\n",
      "Epoch 2411 : Total Loss 10.865\n",
      "Epoch 2421 : Total Loss 10.812\n",
      "Epoch 2431 : Total Loss 10.760\n",
      "Epoch 2441 : Total Loss 10.708\n",
      "Epoch 2451 : Total Loss 10.658\n",
      "Epoch 2461 : Total Loss 10.608\n",
      "Epoch 2471 : Total Loss 10.559\n",
      "Epoch 2481 : Total Loss 10.510\n",
      "Epoch 2491 : Total Loss 10.462\n",
      "Epoch 2501 : Total Loss 10.415\n",
      "Epoch 2511 : Total Loss 10.369\n",
      "Epoch 2521 : Total Loss 10.323\n",
      "Epoch 2531 : Total Loss 10.278\n",
      "Epoch 2541 : Total Loss 10.233\n",
      "Epoch 2551 : Total Loss 10.189\n",
      "Epoch 2561 : Total Loss 10.145\n",
      "Epoch 2571 : Total Loss 10.102\n",
      "Epoch 2581 : Total Loss 10.060\n",
      "Epoch 2591 : Total Loss 10.018\n",
      "Epoch 2601 : Total Loss 9.977\n",
      "Epoch 2611 : Total Loss 9.936\n",
      "Epoch 2621 : Total Loss 9.896\n",
      "Epoch 2631 : Total Loss 9.856\n",
      "Epoch 2641 : Total Loss 9.817\n",
      "Epoch 2651 : Total Loss 9.778\n",
      "Epoch 2661 : Total Loss 9.739\n",
      "Epoch 2671 : Total Loss 9.702\n",
      "Epoch 2681 : Total Loss 9.664\n",
      "Epoch 2691 : Total Loss 9.627\n",
      "Epoch 2701 : Total Loss 9.590\n",
      "Epoch 2711 : Total Loss 9.554\n",
      "Epoch 2721 : Total Loss 9.518\n",
      "Epoch 2731 : Total Loss 9.483\n",
      "Epoch 2741 : Total Loss 9.448\n",
      "Epoch 2751 : Total Loss 9.413\n",
      "Epoch 2761 : Total Loss 9.379\n",
      "Epoch 2771 : Total Loss 9.345\n",
      "Epoch 2781 : Total Loss 9.312\n",
      "Epoch 2791 : Total Loss 9.279\n",
      "Epoch 2801 : Total Loss 9.246\n",
      "Epoch 2811 : Total Loss 9.213\n",
      "Epoch 2821 : Total Loss 9.181\n",
      "Epoch 2831 : Total Loss 9.150\n",
      "Epoch 2841 : Total Loss 9.118\n",
      "Epoch 2851 : Total Loss 9.087\n",
      "Epoch 2861 : Total Loss 9.056\n",
      "Epoch 2871 : Total Loss 9.026\n",
      "Epoch 2881 : Total Loss 8.996\n",
      "Epoch 2891 : Total Loss 8.966\n",
      "Epoch 2901 : Total Loss 8.936\n",
      "Epoch 2911 : Total Loss 8.907\n",
      "Epoch 2921 : Total Loss 8.878\n",
      "Epoch 2931 : Total Loss 8.849\n",
      "Epoch 2941 : Total Loss 8.821\n",
      "Epoch 2951 : Total Loss 8.793\n",
      "Epoch 2961 : Total Loss 8.765\n",
      "Epoch 2971 : Total Loss 8.737\n",
      "Epoch 2981 : Total Loss 8.710\n",
      "Epoch 2991 : Total Loss 8.683\n",
      "Epoch 3001 : Total Loss 8.656\n",
      "Epoch 3011 : Total Loss 8.629\n",
      "Epoch 3021 : Total Loss 8.603\n",
      "Epoch 3031 : Total Loss 8.577\n",
      "Epoch 3041 : Total Loss 8.551\n",
      "Epoch 3051 : Total Loss 8.526\n",
      "Epoch 3061 : Total Loss 8.500\n",
      "Epoch 3071 : Total Loss 8.475\n",
      "Epoch 3081 : Total Loss 8.450\n",
      "Epoch 3091 : Total Loss 8.426\n",
      "Epoch 3101 : Total Loss 8.401\n",
      "Epoch 3111 : Total Loss 8.377\n",
      "Epoch 3121 : Total Loss 8.353\n",
      "Epoch 3131 : Total Loss 8.329\n",
      "Epoch 3141 : Total Loss 8.306\n",
      "Epoch 3151 : Total Loss 8.283\n",
      "Epoch 3161 : Total Loss 8.260\n",
      "Epoch 3171 : Total Loss 8.237\n",
      "Epoch 3181 : Total Loss 8.214\n",
      "Epoch 3191 : Total Loss 8.191\n",
      "Epoch 3201 : Total Loss 8.169\n",
      "Epoch 3211 : Total Loss 8.147\n",
      "Epoch 3221 : Total Loss 8.125\n",
      "Epoch 3231 : Total Loss 8.103\n",
      "Epoch 3241 : Total Loss 8.082\n",
      "Epoch 3251 : Total Loss 8.061\n",
      "Epoch 3261 : Total Loss 8.040\n",
      "Epoch 3271 : Total Loss 8.019\n",
      "Epoch 3281 : Total Loss 7.998\n",
      "Epoch 3291 : Total Loss 7.977\n",
      "Epoch 3301 : Total Loss 7.957\n",
      "Epoch 3311 : Total Loss 7.937\n",
      "Epoch 3321 : Total Loss 7.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3331 : Total Loss 7.897\n",
      "Epoch 3341 : Total Loss 7.877\n",
      "Epoch 3351 : Total Loss 7.858\n",
      "Epoch 3361 : Total Loss 7.838\n",
      "Epoch 3371 : Total Loss 7.819\n",
      "Epoch 3381 : Total Loss 7.800\n",
      "Epoch 3391 : Total Loss 7.781\n",
      "Epoch 3401 : Total Loss 7.763\n",
      "Epoch 3411 : Total Loss 7.744\n",
      "Epoch 3421 : Total Loss 7.726\n",
      "Epoch 3431 : Total Loss 7.708\n",
      "Epoch 3441 : Total Loss 7.690\n",
      "Epoch 3451 : Total Loss 7.672\n",
      "Epoch 3461 : Total Loss 7.654\n",
      "Epoch 3471 : Total Loss 7.636\n",
      "Epoch 3481 : Total Loss 7.619\n",
      "Epoch 3491 : Total Loss 7.602\n",
      "Epoch 3501 : Total Loss 7.585\n",
      "Epoch 3511 : Total Loss 7.568\n",
      "Epoch 3521 : Total Loss 7.551\n",
      "Epoch 3531 : Total Loss 7.534\n",
      "Epoch 3541 : Total Loss 7.518\n",
      "Epoch 3551 : Total Loss 7.501\n",
      "Epoch 3561 : Total Loss 7.485\n",
      "Epoch 3571 : Total Loss 7.469\n",
      "Epoch 3581 : Total Loss 7.453\n",
      "Epoch 3591 : Total Loss 7.437\n",
      "Epoch 3601 : Total Loss 7.422\n",
      "Epoch 3611 : Total Loss 7.406\n",
      "Epoch 3621 : Total Loss 7.391\n",
      "Epoch 3631 : Total Loss 7.376\n",
      "Epoch 3641 : Total Loss 7.361\n",
      "Epoch 3651 : Total Loss 7.346\n",
      "Epoch 3661 : Total Loss 7.331\n",
      "Epoch 3671 : Total Loss 7.316\n",
      "Epoch 3681 : Total Loss 7.301\n",
      "Epoch 3691 : Total Loss 7.287\n",
      "Epoch 3701 : Total Loss 7.273\n",
      "Epoch 3711 : Total Loss 7.258\n",
      "Epoch 3721 : Total Loss 7.244\n",
      "Epoch 3731 : Total Loss 7.230\n",
      "Epoch 3741 : Total Loss 7.217\n",
      "Epoch 3751 : Total Loss 7.203\n",
      "Epoch 3761 : Total Loss 7.189\n",
      "Epoch 3771 : Total Loss 7.176\n",
      "Epoch 3781 : Total Loss 7.162\n",
      "Epoch 3791 : Total Loss 7.149\n",
      "Epoch 3801 : Total Loss 7.136\n",
      "Epoch 3811 : Total Loss 7.123\n",
      "Epoch 3821 : Total Loss 7.110\n",
      "Epoch 3831 : Total Loss 7.098\n",
      "Epoch 3841 : Total Loss 7.085\n",
      "Epoch 3851 : Total Loss 7.072\n",
      "Epoch 3861 : Total Loss 7.060\n",
      "Epoch 3871 : Total Loss 7.048\n",
      "Epoch 3881 : Total Loss 7.035\n",
      "Epoch 3891 : Total Loss 7.023\n",
      "Epoch 3901 : Total Loss 7.011\n",
      "Epoch 3911 : Total Loss 7.000\n",
      "Epoch 3921 : Total Loss 6.988\n",
      "Epoch 3931 : Total Loss 6.976\n",
      "Epoch 3941 : Total Loss 6.965\n",
      "Epoch 3951 : Total Loss 6.953\n",
      "Epoch 3961 : Total Loss 6.942\n",
      "Epoch 3971 : Total Loss 6.931\n",
      "Epoch 3981 : Total Loss 6.919\n",
      "Epoch 3991 : Total Loss 6.908\n",
      "Epoch 4001 : Total Loss 6.897\n",
      "Epoch 4011 : Total Loss 6.887\n",
      "Epoch 4021 : Total Loss 6.876\n",
      "Epoch 4031 : Total Loss 6.865\n",
      "Epoch 4041 : Total Loss 6.855\n",
      "Epoch 4051 : Total Loss 6.844\n",
      "Epoch 4061 : Total Loss 6.834\n",
      "Epoch 4071 : Total Loss 6.824\n",
      "Epoch 4081 : Total Loss 6.813\n",
      "Epoch 4091 : Total Loss 6.803\n",
      "Epoch 4101 : Total Loss 6.793\n",
      "Epoch 4111 : Total Loss 6.783\n",
      "Epoch 4121 : Total Loss 6.774\n",
      "Epoch 4131 : Total Loss 6.764\n",
      "Epoch 4141 : Total Loss 6.754\n",
      "Epoch 4151 : Total Loss 6.745\n",
      "Epoch 4161 : Total Loss 6.735\n",
      "Epoch 4171 : Total Loss 6.726\n",
      "Epoch 4181 : Total Loss 6.717\n",
      "Epoch 4191 : Total Loss 6.707\n",
      "Epoch 4201 : Total Loss 6.698\n",
      "Epoch 4211 : Total Loss 6.689\n",
      "Epoch 4221 : Total Loss 6.680\n",
      "Epoch 4231 : Total Loss 6.672\n",
      "Epoch 4241 : Total Loss 6.663\n",
      "Epoch 4251 : Total Loss 6.654\n",
      "Epoch 4261 : Total Loss 6.645\n",
      "Epoch 4271 : Total Loss 6.637\n",
      "Epoch 4281 : Total Loss 6.628\n",
      "Epoch 4291 : Total Loss 6.620\n",
      "Epoch 4301 : Total Loss 6.612\n",
      "Epoch 4311 : Total Loss 6.604\n",
      "Epoch 4321 : Total Loss 6.595\n",
      "Epoch 4331 : Total Loss 6.587\n",
      "Epoch 4341 : Total Loss 6.579\n",
      "Epoch 4351 : Total Loss 6.571\n",
      "Epoch 4361 : Total Loss 6.564\n",
      "Epoch 4371 : Total Loss 6.556\n",
      "Epoch 4381 : Total Loss 6.548\n",
      "Epoch 4391 : Total Loss 6.540\n",
      "Epoch 4401 : Total Loss 6.533\n",
      "Epoch 4411 : Total Loss 6.525\n",
      "Epoch 4421 : Total Loss 6.518\n",
      "Epoch 4431 : Total Loss 6.510\n",
      "Epoch 4441 : Total Loss 6.503\n",
      "Epoch 4451 : Total Loss 6.496\n",
      "Epoch 4461 : Total Loss 6.489\n",
      "Epoch 4471 : Total Loss 6.482\n",
      "Epoch 4481 : Total Loss 6.475\n",
      "Epoch 4491 : Total Loss 6.468\n",
      "Epoch 4501 : Total Loss 6.461\n",
      "Epoch 4511 : Total Loss 6.454\n",
      "Epoch 4521 : Total Loss 6.447\n",
      "Epoch 4531 : Total Loss 6.440\n",
      "Epoch 4541 : Total Loss 6.434\n",
      "Epoch 4551 : Total Loss 6.427\n",
      "Epoch 4561 : Total Loss 6.421\n",
      "Epoch 4571 : Total Loss 6.414\n",
      "Epoch 4581 : Total Loss 6.408\n",
      "Epoch 4591 : Total Loss 6.401\n",
      "Epoch 4601 : Total Loss 6.395\n",
      "Epoch 4611 : Total Loss 6.389\n",
      "Epoch 4621 : Total Loss 6.383\n",
      "Epoch 4631 : Total Loss 6.377\n",
      "Epoch 4641 : Total Loss 6.371\n",
      "Epoch 4651 : Total Loss 6.365\n",
      "Epoch 4661 : Total Loss 6.359\n",
      "Epoch 4671 : Total Loss 6.353\n",
      "Epoch 4681 : Total Loss 6.347\n",
      "Epoch 4691 : Total Loss 6.341\n",
      "Epoch 4701 : Total Loss 6.335\n",
      "Epoch 4711 : Total Loss 6.330\n",
      "Epoch 4721 : Total Loss 6.324\n",
      "Epoch 4731 : Total Loss 6.318\n",
      "Epoch 4741 : Total Loss 6.313\n",
      "Epoch 4751 : Total Loss 6.307\n",
      "Epoch 4761 : Total Loss 6.302\n",
      "Epoch 4771 : Total Loss 6.297\n",
      "Epoch 4781 : Total Loss 6.291\n",
      "Epoch 4791 : Total Loss 6.286\n",
      "Epoch 4801 : Total Loss 6.281\n",
      "Epoch 4811 : Total Loss 6.276\n",
      "Epoch 4821 : Total Loss 6.270\n",
      "Epoch 4831 : Total Loss 6.265\n",
      "Epoch 4841 : Total Loss 6.260\n",
      "Epoch 4851 : Total Loss 6.255\n",
      "Epoch 4861 : Total Loss 6.250\n",
      "Epoch 4871 : Total Loss 6.245\n",
      "Epoch 4881 : Total Loss 6.240\n",
      "Epoch 4891 : Total Loss 6.236\n",
      "Epoch 4901 : Total Loss 6.231\n",
      "Epoch 4911 : Total Loss 6.226\n",
      "Epoch 4921 : Total Loss 6.221\n",
      "Epoch 4931 : Total Loss 6.217\n",
      "Epoch 4941 : Total Loss 6.212\n",
      "Epoch 4951 : Total Loss 6.208\n",
      "Epoch 4961 : Total Loss 6.203\n",
      "Epoch 4971 : Total Loss 6.199\n",
      "Epoch 4981 : Total Loss 6.194\n",
      "Epoch 4991 : Total Loss 6.190\n",
      "U_hat is:\n",
      " [[ 0.417022    0.72032449]\n",
      " [ 1.28692278  1.1330305 ]\n",
      " [ 1.55601561  1.47861269]\n",
      " [ 1.05261729  0.76524973]\n",
      " [ 0.39676747  0.53881673]\n",
      " [ 0.25243383  0.49387642]\n",
      " [ 0.70419227  1.63261983]\n",
      " [ 0.02738759  0.67046751]\n",
      " [ 0.48106314  0.40329958]\n",
      " [ 0.63886828  0.56652471]\n",
      " [ 0.46617673  0.84367056]\n",
      " [ 0.31342418  0.69232262]\n",
      " [ 1.91050334  1.25942089]\n",
      " [ 0.08504421  0.03905478]\n",
      " [ 0.16983042  0.8781425 ]\n",
      " [ 0.09834683  0.42110763]\n",
      " [ 1.4029      1.09632446]\n",
      " [ 1.86128636  1.06956791]\n",
      " [ 1.03575136  1.05508035]\n",
      " [ 0.28392703  2.2855634 ]\n",
      " [ 1.28453935  1.07294701]\n",
      " [ 0.10405118  0.49500656]\n",
      " [ 0.42457478  1.59543607]\n",
      " [ 0.9085955   0.29361415]\n",
      " [ 0.28777534  0.13002857]\n",
      " [ 0.01936696  0.67883553]\n",
      " [ 1.8657183   1.3822476 ]\n",
      " [ 0.49157316  0.05336255]\n",
      " [ 0.57411761  0.14672857]\n",
      " [ 0.58930554  0.69975836]\n",
      " [ 0.10233443  0.41405599]\n",
      " [ 1.21818381  0.68491836]\n",
      " [ 0.29504064  0.91413613]\n",
      " [ 0.66379465  0.51488911]\n",
      " [ 0.94459476  0.58655504]\n",
      " [ 0.90340192  0.1374747 ]\n",
      " [ 0.13927635  0.80739129]\n",
      " [ 1.61729538  1.38212401]\n",
      " [ 0.92750858  0.34776586]\n",
      " [ 2.06564072  0.62311505]\n",
      " [ 1.0640812   0.80316477]\n",
      " [ 0.75094243  0.34889834]\n",
      " [ 0.26992789  0.89588622]\n",
      " [ 0.40673013  1.65466556]\n",
      " [ 0.94461947  1.45924824]\n",
      " [ 0.32676473  1.50241495]\n",
      " [ 0.44991213  0.57838961]\n",
      " [ 0.88271888  0.97261761]\n",
      " [ 1.38354137  1.20880636]\n",
      " [ 0.42394024  0.89209248]\n",
      " [ 0.07509598  1.36835289]\n",
      " [ 0.8859421   0.35726976]\n",
      " [ 0.90853515  0.62336012]\n",
      " [ 0.33908944  1.28933725]\n",
      " [ 1.42302227  1.77134113]\n",
      " [ 0.17234051  0.13713575]\n",
      " [ 1.48538128  1.86025487]\n",
      " [ 0.06600017  0.75546305]\n",
      " [ 0.75387619  0.92302454]\n",
      " [ 2.39851408  1.18331289]\n",
      " [ 0.01988013  0.02621099]\n",
      " [ 1.40378484  0.90193457]\n",
      " [ 1.65570628  1.35730753]\n",
      " [ 0.95035317  1.87372426]\n",
      " [ 1.38540727  1.03536961]\n",
      " [ 0.26712594  1.79320336]\n",
      " [ 2.09559912  1.088364  ]\n",
      " [ 0.80063267  0.23297427]\n",
      " [ 1.88692403  1.06073522]\n",
      " [ 1.84768011  1.88202561]\n",
      " [ 0.56659326  0.12410921]\n",
      " [ 0.05991769  0.12134346]\n",
      " [ 0.63181341  1.69758637]\n",
      " [ 0.55519398  1.85171265]\n",
      " [ 0.71858814  0.07780781]\n",
      " [ 0.07197428  0.96727633]\n",
      " [ 0.56810046  0.20329323]\n",
      " [ 1.06930401  1.65952789]\n",
      " [ 0.53191537  0.7809097 ]\n",
      " [ 1.53516781  1.31129374]\n",
      " [ 0.23984776  0.49376971]\n",
      " [ 1.28803     1.2540855 ]\n",
      " [ 0.15679139  0.0185762 ]\n",
      " [ 0.22867141  1.8867234 ]\n",
      " [ 1.02924862  1.62628839]\n",
      " [ 0.31736241  0.98861615]\n",
      " [ 0.57974522  0.38014117]\n",
      " [ 0.75753511  0.98072379]\n",
      " [ 1.42718221 -0.1723215 ]\n",
      " [ 0.47810088  1.64060754]\n",
      " [ 2.2036811   0.82189592]\n",
      " [ 0.95241835  0.23779183]\n",
      " [ 0.2603151   0.80475456]\n",
      " [ 0.58354611  0.88015632]\n",
      " [ 0.7300612   1.03624816]\n",
      " [ 1.12263178  0.62535154]\n",
      " [ 0.73506596  0.77217803]\n",
      " [ 0.90781585  0.93197207]\n",
      " [ 0.01395157  0.23436209]\n",
      " [ 0.7027521   1.29973424]]\n",
      "V_hat is:\n",
      " [[0.95017612 0.55665319]\n",
      " [0.91560635 0.64156621]\n",
      " [0.48042926 0.88697674]\n",
      " [0.60431048 0.54954792]\n",
      " [0.92618143 0.91873344]\n",
      " [0.39487561 0.96326253]\n",
      " [1.88438166 2.18805555]\n",
      " [0.27444612 2.14706995]\n",
      " [0.02152481 0.94797021]\n",
      " [0.82711547 0.01501898]\n",
      " [0.17619626 0.33206357]\n",
      " [1.13104085 1.77688205]\n",
      " [0.34473665 0.94010748]\n",
      " [0.58201418 0.87883198]\n",
      " [1.93346657 0.28718988]\n",
      " [0.45988027 0.54634682]\n",
      " [1.27027653 0.65610033]\n",
      " [0.49025352 0.59911031]\n",
      " [0.58235181 0.67780029]\n",
      " [0.89444977 1.23231565]\n",
      " [0.3152448  0.89288871]\n",
      " [1.30823262 1.01917221]\n",
      " [0.78792923 0.61203118]\n",
      " [0.78284707 1.73241927]\n",
      " [0.67906884 0.91860178]\n",
      " [0.38507012 2.16517352]\n",
      " [0.37658031 0.97378354]\n",
      " [0.88901066 0.35299529]\n",
      " [0.75949395 0.66014622]\n",
      " [1.09956527 1.21507206]\n",
      " [0.75002176 0.85831384]\n",
      " [0.95410909 1.29201516]\n",
      " [1.33907295 0.75569198]\n",
      " [0.67078879 0.45087394]\n",
      " [1.75204171 1.47451157]\n",
      " [0.40147958 0.31738395]\n",
      " [0.87230507 0.67908025]\n",
      " [1.97938308 1.22090252]\n",
      " [1.83542929 1.04394131]\n",
      " [0.34334624 0.7976388 ]\n",
      " [0.87999829 0.90384196]\n",
      " [0.77572612 0.70719252]\n",
      " [0.2523667  0.85489794]\n",
      " [0.52771465 0.80216108]\n",
      " [0.57248852 0.73314253]\n",
      " [0.51901163 0.77088391]\n",
      " [0.56885799 0.46570988]\n",
      " [0.61755552 0.50657425]\n",
      " [1.59215752 0.56889442]\n",
      " [0.98281711 0.18161285]\n",
      " [0.8118587  0.87496164]\n",
      " [0.68841325 0.56949441]\n",
      " [0.16097144 0.46688002]\n",
      " [0.34517205 0.22503996]\n",
      " [0.59251187 0.31226984]\n",
      " [1.09058211 1.0326581 ]\n",
      " [1.57651487 0.66211006]\n",
      " [0.51015126 1.6736762 ]\n",
      " [1.87757928 0.14478839]\n",
      " [0.47877359 1.74665002]\n",
      " [1.32798621 1.30584536]\n",
      " [1.05152617 0.74546705]\n",
      " [0.51585702 0.47714099]\n",
      " [0.15267164 0.62180623]\n",
      " [1.76915382 2.26978845]\n",
      " [0.3090055  2.07653188]\n",
      " [0.22204914 0.51935182]\n",
      " [0.78529603 0.02233043]\n",
      " [0.987697   1.25900172]\n",
      " [0.84470961 0.53844059]\n",
      " [0.86660827 0.94980599]\n",
      " [0.43857524 1.80310536]\n",
      " [0.0987434  0.65130433]\n",
      " [0.70351699 0.61024081]\n",
      " [0.79961526 0.03457122]\n",
      " [1.46201684 1.64256087]\n",
      " [0.25969839 0.2570693 ]\n",
      " [0.63230332 0.34529746]\n",
      " [1.10895233 0.79702541]\n",
      " [0.78274941 0.99047178]\n",
      " [1.31922243 2.01656263]\n",
      " [1.95540025 1.22213532]\n",
      " [0.97474037 0.6366044 ]\n",
      " [1.9156052  1.34404524]\n",
      " [0.52642593 0.1354279 ]\n",
      " [0.65283948 0.61851633]\n",
      " [0.16039518 0.74563719]\n",
      " [0.03039969 0.3665431 ]\n",
      " [2.06229477 0.58882883]\n",
      " [1.55877213 0.88873561]\n",
      " [1.01875832 1.67096268]\n",
      " [1.38734933 0.22085069]\n",
      " [1.10410839 0.62504914]\n",
      " [1.74601975 1.25131454]\n",
      " [0.61459846 1.96933946]\n",
      " [0.21196016 0.79860424]\n",
      " [1.41087877 0.99921471]\n",
      " [0.85518621 1.54645533]\n",
      " [0.2179855  0.51089184]\n",
      " [1.43417443 2.01863838]]\n",
      "A is:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "A_hat is:\n",
      " [[0.79721528 0.84396385 0.83926065 ... 1.47058112 0.45891265 2.05215697]\n",
      " [1.85350834 1.90522875 1.62324707 ... 2.85273967 0.85938653 4.13285061]\n",
      " [2.30156334 2.37332571 2.0590505  ... 3.61729157 1.09459999 5.21638214]\n",
      " ...\n",
      " [1.38137017 1.42912375 1.26277885 ... 2.21760477 0.67402761 3.18328088]\n",
      " [0.14371485 0.16313294 0.21457646 ... 0.37436169 0.12277492 0.49310129]\n",
      " [1.39123947 1.47730985 1.49045671 ... 2.61096484 0.81721338 3.63156251]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "m, n = arr_100_100.shape[0], arr_100_100.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "U_3 = np.random.rand(m, user_features)\n",
    "V_3 = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U_3)\n",
    "print('V is:\\n', V_3)\n",
    "\n",
    "# perform factorization\n",
    "start3 = t.default_timer()\n",
    "\n",
    "U_hat, V_hat = matrix_factorization_sparse(arr_100_100, U_3, V_3, user_features, 5000)\n",
    "\n",
    "end3 = t.default_timer()\n",
    "\n",
    "A_hat = np.dot(U_hat, V_hat.T)\n",
    "\n",
    "print('U_hat is:\\n', U_hat)\n",
    "print('V_hat is:\\n', V_hat)\n",
    "\n",
    "## check if the product results in something close to A\n",
    "print('A is:\\n', arr_100_100)\n",
    "print('A_hat is:\\n', A_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c57a5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The runtime of the sparse matrix factorization on a 100 x 100, 99% sparse matrix is 6.4532393999397755 seconds.\n",
      "The mean squared error of all predictions is 0.008.\n",
      "The root mean squared error of all predictions is 0.089.\n"
     ]
    }
   ],
   "source": [
    "print(f'The runtime of the sparse matrix factorization on a 100 x 100, 99% sparse matrix is {end3 - start3} seconds.')\n",
    "\n",
    "mse, rmse = evaluation_metrics(arr_100_100, A_hat)\n",
    "\n",
    "print(f'The mean squared error of all predictions is {mse:.3f}.')\n",
    "print(f'The root mean squared error of all predictions is {rmse:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07af24",
   "metadata": {},
   "source": [
    "### Run Regular Matrix Factorization on Sparse Matrix\n",
    "\n",
    "Just for comparison, let us check how much more efficient our sparse matrix factorization is compared to the regular method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4315a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[4.17022005e-01 7.20324493e-01]\n",
      " [1.14374817e-04 3.02332573e-01]\n",
      " [1.46755891e-01 9.23385948e-02]\n",
      " [1.86260211e-01 3.45560727e-01]\n",
      " [3.96767474e-01 5.38816734e-01]\n",
      " [4.19194514e-01 6.85219500e-01]\n",
      " [2.04452250e-01 8.78117436e-01]\n",
      " [2.73875932e-02 6.70467510e-01]\n",
      " [4.17304802e-01 5.58689828e-01]\n",
      " [1.40386939e-01 1.98101489e-01]\n",
      " [8.00744569e-01 9.68261576e-01]\n",
      " [3.13424178e-01 6.92322616e-01]\n",
      " [8.76389152e-01 8.94606664e-01]\n",
      " [8.50442114e-02 3.90547832e-02]\n",
      " [1.69830420e-01 8.78142503e-01]\n",
      " [9.83468338e-02 4.21107625e-01]\n",
      " [9.57889530e-01 5.33165285e-01]\n",
      " [6.91877114e-01 3.15515631e-01]\n",
      " [6.86500928e-01 8.34625672e-01]\n",
      " [1.82882773e-02 7.50144315e-01]\n",
      " [9.88861089e-01 7.48165654e-01]\n",
      " [2.80443992e-01 7.89279328e-01]\n",
      " [1.03226007e-01 4.47893526e-01]\n",
      " [9.08595503e-01 2.93614148e-01]\n",
      " [2.87775339e-01 1.30028572e-01]\n",
      " [1.93669579e-02 6.78835533e-01]\n",
      " [2.11628116e-01 2.65546659e-01]\n",
      " [4.91573159e-01 5.33625451e-02]\n",
      " [5.74117605e-01 1.46728575e-01]\n",
      " [5.89305537e-01 6.99758360e-01]\n",
      " [1.02334429e-01 4.14055988e-01]\n",
      " [6.94400158e-01 4.14179270e-01]\n",
      " [4.99534589e-02 5.35896406e-01]\n",
      " [6.63794645e-01 5.14889112e-01]\n",
      " [9.44594756e-01 5.86555041e-01]\n",
      " [9.03401915e-01 1.37474704e-01]\n",
      " [1.39276347e-01 8.07391289e-01]\n",
      " [3.97676837e-01 1.65354197e-01]\n",
      " [9.27508580e-01 3.47765860e-01]\n",
      " [7.50812103e-01 7.25997985e-01]\n",
      " [8.83306091e-01 6.23672207e-01]\n",
      " [7.50942434e-01 3.48898342e-01]\n",
      " [2.69927892e-01 8.95886218e-01]\n",
      " [4.28091190e-01 9.64840047e-01]\n",
      " [6.63441498e-01 6.21695720e-01]\n",
      " [1.14745973e-01 9.49489259e-01]\n",
      " [4.49912133e-01 5.78389614e-01]\n",
      " [4.08136803e-01 2.37026980e-01]\n",
      " [9.03379521e-01 5.73679487e-01]\n",
      " [2.87032703e-03 6.17144914e-01]\n",
      " [3.26644902e-01 5.27058102e-01]\n",
      " [8.85942099e-01 3.57269760e-01]\n",
      " [9.08535151e-01 6.23360116e-01]\n",
      " [1.58212428e-02 9.29437234e-01]\n",
      " [6.90896918e-01 9.97322850e-01]\n",
      " [1.72340508e-01 1.37135750e-01]\n",
      " [9.32595463e-01 6.96818161e-01]\n",
      " [6.60001727e-02 7.55463053e-01]\n",
      " [7.53876188e-01 9.23024536e-01]\n",
      " [7.11524759e-01 1.24270962e-01]\n",
      " [1.98801338e-02 2.62109869e-02]\n",
      " [2.83064880e-02 2.46211068e-01]\n",
      " [8.60027949e-01 5.38831064e-01]\n",
      " [5.52821979e-01 8.42030892e-01]\n",
      " [1.24173315e-01 2.79183679e-01]\n",
      " [5.85759271e-01 9.69595748e-01]\n",
      " [5.61030219e-01 1.86472894e-02]\n",
      " [8.00632673e-01 2.32974274e-01]\n",
      " [8.07105196e-01 3.87860644e-01]\n",
      " [8.63541855e-01 7.47121643e-01]\n",
      " [5.56240234e-01 1.36455226e-01]\n",
      " [5.99176895e-02 1.21343456e-01]\n",
      " [4.45518785e-02 1.07494129e-01]\n",
      " [2.25709339e-01 7.12988980e-01]\n",
      " [5.59716982e-01 1.25559802e-02]\n",
      " [7.19742797e-02 9.67276330e-01]\n",
      " [5.68100462e-01 2.03293235e-01]\n",
      " [2.52325745e-01 7.43825854e-01]\n",
      " [1.95429481e-01 5.81358927e-01]\n",
      " [9.70019989e-01 8.46828801e-01]\n",
      " [2.39847759e-01 4.93769714e-01]\n",
      " [6.19955718e-01 8.28980900e-01]\n",
      " [1.56791395e-01 1.85762022e-02]\n",
      " [7.00221437e-02 4.86345111e-01]\n",
      " [6.06329462e-01 5.68851437e-01]\n",
      " [3.17362409e-01 9.88616154e-01]\n",
      " [5.79745219e-01 3.80141173e-01]\n",
      " [5.50948219e-01 7.45334431e-01]\n",
      " [6.69232893e-01 2.64919558e-01]\n",
      " [6.63348344e-02 3.70084198e-01]\n",
      " [6.29717507e-01 2.10174010e-01]\n",
      " [7.52755554e-01 6.65364814e-02]\n",
      " [2.60315099e-01 8.04754564e-01]\n",
      " [1.93434283e-01 6.39460881e-01]\n",
      " [5.24670309e-01 9.24807970e-01]\n",
      " [2.63296770e-01 6.59610907e-02]\n",
      " [7.35065963e-01 7.72178030e-01]\n",
      " [9.07815853e-01 9.31972069e-01]\n",
      " [1.39515730e-02 2.34362086e-01]\n",
      " [6.16778357e-01 9.49016321e-01]]\n",
      "V is:\n",
      " [[9.50176119e-01 5.56653188e-01]\n",
      " [9.15606350e-01 6.41566209e-01]\n",
      " [3.90007714e-01 4.85990667e-01]\n",
      " [6.04310483e-01 5.49547922e-01]\n",
      " [9.26181427e-01 9.18733436e-01]\n",
      " [3.94875613e-01 9.63262528e-01]\n",
      " [1.73955667e-01 1.26329519e-01]\n",
      " [1.35079158e-01 5.05662166e-01]\n",
      " [2.15248053e-02 9.47970211e-01]\n",
      " [8.27115471e-01 1.50189807e-02]\n",
      " [1.76196256e-01 3.32063574e-01]\n",
      " [1.30996845e-01 8.09490692e-01]\n",
      " [3.44736653e-01 9.40107482e-01]\n",
      " [5.82014180e-01 8.78831984e-01]\n",
      " [8.44734445e-01 9.05392319e-01]\n",
      " [4.59880266e-01 5.46346816e-01]\n",
      " [7.98603591e-01 2.85718852e-01]\n",
      " [4.90253523e-01 5.99110308e-01]\n",
      " [1.55332756e-02 5.93481408e-01]\n",
      " [4.33676349e-01 8.07360529e-01]\n",
      " [3.15244803e-01 8.92888709e-01]\n",
      " [5.77857215e-01 1.84010202e-01]\n",
      " [7.87929234e-01 6.12031177e-01]\n",
      " [5.39092721e-02 4.20193680e-01]\n",
      " [6.79068837e-01 9.18601778e-01]\n",
      " [4.02024891e-04 9.76759149e-01]\n",
      " [3.76580315e-01 9.73783538e-01]\n",
      " [6.04716101e-01 8.28845808e-01]\n",
      " [5.74711505e-01 6.28076198e-01]\n",
      " [2.85576282e-01 5.86833341e-01]\n",
      " [7.50021764e-01 8.58313836e-01]\n",
      " [7.55082188e-01 6.98057248e-01]\n",
      " [8.64479430e-01 3.22680997e-01]\n",
      " [6.70788791e-01 4.50873936e-01]\n",
      " [3.82102752e-01 4.10811350e-01]\n",
      " [4.01479583e-01 3.17383946e-01]\n",
      " [6.21919368e-01 4.30247271e-01]\n",
      " [9.73802078e-01 6.77800891e-01]\n",
      " [1.98569888e-01 4.26701009e-01]\n",
      " [3.43346240e-01 7.97638804e-01]\n",
      " [8.79998289e-01 9.03841956e-01]\n",
      " [6.62719812e-01 2.70208262e-01]\n",
      " [2.52366702e-01 8.54897943e-01]\n",
      " [5.27714646e-01 8.02161084e-01]\n",
      " [5.72488517e-01 7.33142525e-01]\n",
      " [5.19011627e-01 7.70883911e-01]\n",
      " [5.68857991e-01 4.65709879e-01]\n",
      " [3.42688908e-01 6.82093484e-02]\n",
      " [3.77924179e-01 7.96260777e-02]\n",
      " [9.82817114e-01 1.81612851e-01]\n",
      " [8.11858698e-01 8.74961645e-01]\n",
      " [6.88413252e-01 5.69494413e-01]\n",
      " [1.60971437e-01 4.66880023e-01]\n",
      " [3.45172051e-01 2.25039958e-01]\n",
      " [5.92511869e-01 3.12269838e-01]\n",
      " [9.16305553e-01 9.09635525e-01]\n",
      " [2.57118294e-01 1.10891301e-01]\n",
      " [1.92962732e-01 4.99584171e-01]\n",
      " [7.28585668e-01 2.08194438e-01]\n",
      " [2.48033558e-01 8.51671875e-01]\n",
      " [4.15848718e-01 6.16685067e-01]\n",
      " [2.33666139e-01 1.01967259e-01]\n",
      " [5.15857017e-01 4.77140987e-01]\n",
      " [1.52671644e-01 6.21806232e-01]\n",
      " [5.44010119e-01 6.54137347e-01]\n",
      " [1.44545540e-01 7.51527817e-01]\n",
      " [2.22049140e-01 5.19351824e-01]\n",
      " [7.85296028e-01 2.23304280e-02]\n",
      " [3.24362460e-01 8.72922376e-01]\n",
      " [8.44709608e-01 5.38440593e-01]\n",
      " [8.66608274e-01 9.49805991e-01]\n",
      " [8.26406998e-01 8.54115444e-01]\n",
      " [9.87434018e-02 6.51304332e-01]\n",
      " [7.03516988e-01 6.10240813e-01]\n",
      " [7.99615262e-01 3.45712199e-02]\n",
      " [7.70238735e-01 7.31728601e-01]\n",
      " [2.59698393e-01 2.57069299e-01]\n",
      " [6.32303317e-01 3.45297462e-01]\n",
      " [7.96588678e-01 4.46146232e-01]\n",
      " [7.82749415e-01 9.90471784e-01]\n",
      " [3.00248340e-01 1.43005828e-01]\n",
      " [9.01308436e-01 5.41559379e-01]\n",
      " [9.74740371e-01 6.36604400e-01]\n",
      " [9.93913025e-01 5.46070804e-01]\n",
      " [5.26425934e-01 1.35427903e-01]\n",
      " [3.55705171e-01 2.62185673e-02]\n",
      " [1.60395180e-01 7.45637193e-01]\n",
      " [3.03996899e-02 3.66543097e-01]\n",
      " [8.62346253e-01 6.92677718e-01]\n",
      " [6.90942142e-01 1.88636801e-01]\n",
      " [4.41904281e-01 5.81577407e-01]\n",
      " [9.89751708e-01 2.03906225e-01]\n",
      " [2.47732902e-01 2.62173084e-01]\n",
      " [7.50172413e-01 4.56975327e-01]\n",
      " [5.69294384e-02 5.08516241e-01]\n",
      " [2.11960165e-01 7.98604245e-01]\n",
      " [2.97331382e-01 2.76060120e-02]\n",
      " [5.93432449e-01 8.43840429e-01]\n",
      " [3.81016124e-01 7.49858311e-01]\n",
      " [5.11141478e-01 5.40951805e-01]]\n",
      "Epoch 1 : Total Loss 598.166\n",
      "Epoch 11 : Total Loss 589.788\n",
      "Epoch 21 : Total Loss 581.294\n",
      "Epoch 31 : Total Loss 572.685\n",
      "Epoch 41 : Total Loss 563.966\n",
      "Epoch 51 : Total Loss 555.139\n",
      "Epoch 61 : Total Loss 546.209\n",
      "Epoch 71 : Total Loss 537.182\n",
      "Epoch 81 : Total Loss 528.064\n",
      "Epoch 91 : Total Loss 518.862\n",
      "Epoch 101 : Total Loss 509.584\n",
      "Epoch 111 : Total Loss 500.238\n",
      "Epoch 121 : Total Loss 490.834\n",
      "Epoch 131 : Total Loss 481.381\n",
      "Epoch 141 : Total Loss 471.889\n",
      "Epoch 151 : Total Loss 462.370\n",
      "Epoch 161 : Total Loss 452.835\n",
      "Epoch 171 : Total Loss 443.296\n",
      "Epoch 181 : Total Loss 433.765\n",
      "Epoch 191 : Total Loss 424.254\n",
      "Epoch 201 : Total Loss 414.775\n",
      "Epoch 211 : Total Loss 405.340\n",
      "Epoch 221 : Total Loss 395.962\n",
      "Epoch 231 : Total Loss 386.653\n",
      "Epoch 241 : Total Loss 377.422\n",
      "Epoch 251 : Total Loss 368.283\n",
      "Epoch 261 : Total Loss 359.244\n",
      "Epoch 271 : Total Loss 350.317\n",
      "Epoch 281 : Total Loss 341.508\n",
      "Epoch 291 : Total Loss 332.828\n",
      "Epoch 301 : Total Loss 324.284\n",
      "Epoch 311 : Total Loss 315.882\n",
      "Epoch 321 : Total Loss 307.628\n",
      "Epoch 331 : Total Loss 299.527\n",
      "Epoch 341 : Total Loss 291.584\n",
      "Epoch 351 : Total Loss 283.802\n",
      "Epoch 361 : Total Loss 276.183\n",
      "Epoch 371 : Total Loss 268.730\n",
      "Epoch 381 : Total Loss 261.445\n",
      "Epoch 391 : Total Loss 254.327\n",
      "Epoch 401 : Total Loss 247.377\n",
      "Epoch 411 : Total Loss 240.595\n",
      "Epoch 421 : Total Loss 233.980\n",
      "Epoch 431 : Total Loss 227.531\n",
      "Epoch 441 : Total Loss 221.247\n",
      "Epoch 451 : Total Loss 215.126\n",
      "Epoch 461 : Total Loss 209.166\n",
      "Epoch 471 : Total Loss 203.366\n",
      "Epoch 481 : Total Loss 197.722\n",
      "Epoch 491 : Total Loss 192.232\n",
      "Epoch 501 : Total Loss 186.895\n",
      "Epoch 511 : Total Loss 181.708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 521 : Total Loss 176.667\n",
      "Epoch 531 : Total Loss 171.771\n",
      "Epoch 541 : Total Loss 167.017\n",
      "Epoch 551 : Total Loss 162.402\n",
      "Epoch 561 : Total Loss 157.923\n",
      "Epoch 571 : Total Loss 153.578\n",
      "Epoch 581 : Total Loss 149.364\n",
      "Epoch 591 : Total Loss 145.279\n",
      "Epoch 601 : Total Loss 141.320\n",
      "Epoch 611 : Total Loss 137.483\n",
      "Epoch 621 : Total Loss 133.766\n",
      "Epoch 631 : Total Loss 130.168\n",
      "Epoch 641 : Total Loss 126.683\n",
      "Epoch 651 : Total Loss 123.311\n",
      "Epoch 661 : Total Loss 120.048\n",
      "Epoch 671 : Total Loss 116.891\n",
      "Epoch 681 : Total Loss 113.838\n",
      "Epoch 691 : Total Loss 110.885\n",
      "Epoch 701 : Total Loss 108.031\n",
      "Epoch 711 : Total Loss 105.271\n",
      "Epoch 721 : Total Loss 102.604\n",
      "Epoch 731 : Total Loss 100.026\n",
      "Epoch 741 : Total Loss 97.534\n",
      "Epoch 751 : Total Loss 95.127\n",
      "Epoch 761 : Total Loss 92.800\n",
      "Epoch 771 : Total Loss 90.552\n",
      "Epoch 781 : Total Loss 88.380\n",
      "Epoch 791 : Total Loss 86.280\n",
      "Epoch 801 : Total Loss 84.251\n",
      "Epoch 811 : Total Loss 82.289\n",
      "Epoch 821 : Total Loss 80.392\n",
      "Epoch 831 : Total Loss 78.558\n",
      "Epoch 841 : Total Loss 76.784\n",
      "Epoch 851 : Total Loss 75.069\n",
      "Epoch 861 : Total Loss 73.408\n",
      "Epoch 871 : Total Loss 71.801\n",
      "Epoch 881 : Total Loss 70.245\n",
      "Epoch 891 : Total Loss 68.738\n",
      "Epoch 901 : Total Loss 67.279\n",
      "Epoch 911 : Total Loss 65.864\n",
      "Epoch 921 : Total Loss 64.492\n",
      "Epoch 931 : Total Loss 63.162\n",
      "Epoch 941 : Total Loss 61.872\n",
      "Epoch 951 : Total Loss 60.620\n",
      "Epoch 961 : Total Loss 59.404\n",
      "Epoch 971 : Total Loss 58.223\n",
      "Epoch 981 : Total Loss 57.076\n",
      "Epoch 991 : Total Loss 55.960\n",
      "Epoch 1001 : Total Loss 54.876\n",
      "Epoch 1011 : Total Loss 53.821\n",
      "Epoch 1021 : Total Loss 52.795\n",
      "Epoch 1031 : Total Loss 51.796\n",
      "Epoch 1041 : Total Loss 50.824\n",
      "Epoch 1051 : Total Loss 49.876\n",
      "Epoch 1061 : Total Loss 48.953\n",
      "Epoch 1071 : Total Loss 48.054\n",
      "Epoch 1081 : Total Loss 47.177\n",
      "Epoch 1091 : Total Loss 46.322\n",
      "Epoch 1101 : Total Loss 45.487\n",
      "Epoch 1111 : Total Loss 44.674\n",
      "Epoch 1121 : Total Loss 43.880\n",
      "Epoch 1131 : Total Loss 43.104\n",
      "Epoch 1141 : Total Loss 42.348\n",
      "Epoch 1151 : Total Loss 41.609\n",
      "Epoch 1161 : Total Loss 40.887\n",
      "Epoch 1171 : Total Loss 40.182\n",
      "Epoch 1181 : Total Loss 39.494\n",
      "Epoch 1191 : Total Loss 38.821\n",
      "Epoch 1201 : Total Loss 38.164\n",
      "Epoch 1211 : Total Loss 37.521\n",
      "Epoch 1221 : Total Loss 36.893\n",
      "Epoch 1231 : Total Loss 36.280\n",
      "Epoch 1241 : Total Loss 35.680\n",
      "Epoch 1251 : Total Loss 35.093\n",
      "Epoch 1261 : Total Loss 34.520\n",
      "Epoch 1271 : Total Loss 33.959\n",
      "Epoch 1281 : Total Loss 33.411\n",
      "Epoch 1291 : Total Loss 32.875\n",
      "Epoch 1301 : Total Loss 32.351\n",
      "Epoch 1311 : Total Loss 31.839\n",
      "Epoch 1321 : Total Loss 31.337\n",
      "Epoch 1331 : Total Loss 30.847\n",
      "Epoch 1341 : Total Loss 30.368\n",
      "Epoch 1351 : Total Loss 29.900\n",
      "Epoch 1361 : Total Loss 29.442\n",
      "Epoch 1371 : Total Loss 28.993\n",
      "Epoch 1381 : Total Loss 28.555\n",
      "Epoch 1391 : Total Loss 28.127\n",
      "Epoch 1401 : Total Loss 27.708\n",
      "Epoch 1411 : Total Loss 27.298\n",
      "Epoch 1421 : Total Loss 26.897\n",
      "Epoch 1431 : Total Loss 26.506\n",
      "Epoch 1441 : Total Loss 26.122\n",
      "Epoch 1451 : Total Loss 25.748\n",
      "Epoch 1461 : Total Loss 25.382\n",
      "Epoch 1471 : Total Loss 25.023\n",
      "Epoch 1481 : Total Loss 24.673\n",
      "Epoch 1491 : Total Loss 24.330\n",
      "Epoch 1501 : Total Loss 23.996\n",
      "Epoch 1511 : Total Loss 23.668\n",
      "Epoch 1521 : Total Loss 23.348\n",
      "Epoch 1531 : Total Loss 23.035\n",
      "Epoch 1541 : Total Loss 22.729\n",
      "Epoch 1551 : Total Loss 22.429\n",
      "Epoch 1561 : Total Loss 22.136\n",
      "Epoch 1571 : Total Loss 21.850\n",
      "Epoch 1581 : Total Loss 21.570\n",
      "Epoch 1591 : Total Loss 21.297\n",
      "Epoch 1601 : Total Loss 21.029\n",
      "Epoch 1611 : Total Loss 20.767\n",
      "Epoch 1621 : Total Loss 20.512\n",
      "Epoch 1631 : Total Loss 20.261\n",
      "Epoch 1641 : Total Loss 20.017\n",
      "Epoch 1651 : Total Loss 19.778\n",
      "Epoch 1661 : Total Loss 19.544\n",
      "Epoch 1671 : Total Loss 19.315\n",
      "Epoch 1681 : Total Loss 19.091\n",
      "Epoch 1691 : Total Loss 18.872\n",
      "Epoch 1701 : Total Loss 18.659\n",
      "Epoch 1711 : Total Loss 18.449\n",
      "Epoch 1721 : Total Loss 18.245\n",
      "Epoch 1731 : Total Loss 18.045\n",
      "Epoch 1741 : Total Loss 17.849\n",
      "Epoch 1751 : Total Loss 17.657\n",
      "Epoch 1761 : Total Loss 17.470\n",
      "Epoch 1771 : Total Loss 17.287\n",
      "Epoch 1781 : Total Loss 17.108\n",
      "Epoch 1791 : Total Loss 16.933\n",
      "Epoch 1801 : Total Loss 16.761\n",
      "Epoch 1811 : Total Loss 16.593\n",
      "Epoch 1821 : Total Loss 16.429\n",
      "Epoch 1831 : Total Loss 16.269\n",
      "Epoch 1841 : Total Loss 16.112\n",
      "Epoch 1851 : Total Loss 15.958\n",
      "Epoch 1861 : Total Loss 15.808\n",
      "Epoch 1871 : Total Loss 15.660\n",
      "Epoch 1881 : Total Loss 15.516\n",
      "Epoch 1891 : Total Loss 15.375\n",
      "Epoch 1901 : Total Loss 15.237\n",
      "Epoch 1911 : Total Loss 15.102\n",
      "Epoch 1921 : Total Loss 14.970\n",
      "Epoch 1931 : Total Loss 14.840\n",
      "Epoch 1941 : Total Loss 14.714\n",
      "Epoch 1951 : Total Loss 14.590\n",
      "Epoch 1961 : Total Loss 14.468\n",
      "Epoch 1971 : Total Loss 14.349\n",
      "Epoch 1981 : Total Loss 14.232\n",
      "Epoch 1991 : Total Loss 14.118\n",
      "Epoch 2001 : Total Loss 14.006\n",
      "Epoch 2011 : Total Loss 13.897\n",
      "Epoch 2021 : Total Loss 13.789\n",
      "Epoch 2031 : Total Loss 13.684\n",
      "Epoch 2041 : Total Loss 13.581\n",
      "Epoch 2051 : Total Loss 13.480\n",
      "Epoch 2061 : Total Loss 13.381\n",
      "Epoch 2071 : Total Loss 13.284\n",
      "Epoch 2081 : Total Loss 13.188\n",
      "Epoch 2091 : Total Loss 13.095\n",
      "Epoch 2101 : Total Loss 13.003\n",
      "Epoch 2111 : Total Loss 12.914\n",
      "Epoch 2121 : Total Loss 12.825\n",
      "Epoch 2131 : Total Loss 12.739\n",
      "Epoch 2141 : Total Loss 12.654\n",
      "Epoch 2151 : Total Loss 12.571\n",
      "Epoch 2161 : Total Loss 12.489\n",
      "Epoch 2171 : Total Loss 12.409\n",
      "Epoch 2181 : Total Loss 12.330\n",
      "Epoch 2191 : Total Loss 12.253\n",
      "Epoch 2201 : Total Loss 12.177\n",
      "Epoch 2211 : Total Loss 12.103\n",
      "Epoch 2221 : Total Loss 12.030\n",
      "Epoch 2231 : Total Loss 11.958\n",
      "Epoch 2241 : Total Loss 11.887\n",
      "Epoch 2251 : Total Loss 11.818\n",
      "Epoch 2261 : Total Loss 11.749\n",
      "Epoch 2271 : Total Loss 11.682\n",
      "Epoch 2281 : Total Loss 11.616\n",
      "Epoch 2291 : Total Loss 11.552\n",
      "Epoch 2301 : Total Loss 11.488\n",
      "Epoch 2311 : Total Loss 11.425\n",
      "Epoch 2321 : Total Loss 11.363\n",
      "Epoch 2331 : Total Loss 11.303\n",
      "Epoch 2341 : Total Loss 11.243\n",
      "Epoch 2351 : Total Loss 11.184\n",
      "Epoch 2361 : Total Loss 11.126\n",
      "Epoch 2371 : Total Loss 11.070\n",
      "Epoch 2381 : Total Loss 11.013\n",
      "Epoch 2391 : Total Loss 10.958\n",
      "Epoch 2401 : Total Loss 10.904\n",
      "Epoch 2411 : Total Loss 10.850\n",
      "Epoch 2421 : Total Loss 10.798\n",
      "Epoch 2431 : Total Loss 10.746\n",
      "Epoch 2441 : Total Loss 10.695\n",
      "Epoch 2451 : Total Loss 10.644\n",
      "Epoch 2461 : Total Loss 10.594\n",
      "Epoch 2471 : Total Loss 10.545\n",
      "Epoch 2481 : Total Loss 10.497\n",
      "Epoch 2491 : Total Loss 10.449\n",
      "Epoch 2501 : Total Loss 10.402\n",
      "Epoch 2511 : Total Loss 10.356\n",
      "Epoch 2521 : Total Loss 10.310\n",
      "Epoch 2531 : Total Loss 10.265\n",
      "Epoch 2541 : Total Loss 10.221\n",
      "Epoch 2551 : Total Loss 10.177\n",
      "Epoch 2561 : Total Loss 10.133\n",
      "Epoch 2571 : Total Loss 10.090\n",
      "Epoch 2581 : Total Loss 10.048\n",
      "Epoch 2591 : Total Loss 10.006\n",
      "Epoch 2601 : Total Loss 9.965\n",
      "Epoch 2611 : Total Loss 9.925\n",
      "Epoch 2621 : Total Loss 9.884\n",
      "Epoch 2631 : Total Loss 9.845\n",
      "Epoch 2641 : Total Loss 9.805\n",
      "Epoch 2651 : Total Loss 9.767\n",
      "Epoch 2661 : Total Loss 9.728\n",
      "Epoch 2671 : Total Loss 9.691\n",
      "Epoch 2681 : Total Loss 9.653\n",
      "Epoch 2691 : Total Loss 9.616\n",
      "Epoch 2701 : Total Loss 9.580\n",
      "Epoch 2711 : Total Loss 9.544\n",
      "Epoch 2721 : Total Loss 9.508\n",
      "Epoch 2731 : Total Loss 9.473\n",
      "Epoch 2741 : Total Loss 9.438\n",
      "Epoch 2751 : Total Loss 9.403\n",
      "Epoch 2761 : Total Loss 9.369\n",
      "Epoch 2771 : Total Loss 9.335\n",
      "Epoch 2781 : Total Loss 9.302\n",
      "Epoch 2791 : Total Loss 9.269\n",
      "Epoch 2801 : Total Loss 9.236\n",
      "Epoch 2811 : Total Loss 9.204\n",
      "Epoch 2821 : Total Loss 9.172\n",
      "Epoch 2831 : Total Loss 9.140\n",
      "Epoch 2841 : Total Loss 9.109\n",
      "Epoch 2851 : Total Loss 9.078\n",
      "Epoch 2861 : Total Loss 9.047\n",
      "Epoch 2871 : Total Loss 9.017\n",
      "Epoch 2881 : Total Loss 8.987\n",
      "Epoch 2891 : Total Loss 8.957\n",
      "Epoch 2901 : Total Loss 8.927\n",
      "Epoch 2911 : Total Loss 8.898\n",
      "Epoch 2921 : Total Loss 8.869\n",
      "Epoch 2931 : Total Loss 8.841\n",
      "Epoch 2941 : Total Loss 8.812\n",
      "Epoch 2951 : Total Loss 8.784\n",
      "Epoch 2961 : Total Loss 8.756\n",
      "Epoch 2971 : Total Loss 8.729\n",
      "Epoch 2981 : Total Loss 8.702\n",
      "Epoch 2991 : Total Loss 8.675\n",
      "Epoch 3001 : Total Loss 8.648\n",
      "Epoch 3011 : Total Loss 8.621\n",
      "Epoch 3021 : Total Loss 8.595\n",
      "Epoch 3031 : Total Loss 8.569\n",
      "Epoch 3041 : Total Loss 8.543\n",
      "Epoch 3051 : Total Loss 8.518\n",
      "Epoch 3061 : Total Loss 8.493\n",
      "Epoch 3071 : Total Loss 8.468\n",
      "Epoch 3081 : Total Loss 8.443\n",
      "Epoch 3091 : Total Loss 8.418\n",
      "Epoch 3101 : Total Loss 8.394\n",
      "Epoch 3111 : Total Loss 8.370\n",
      "Epoch 3121 : Total Loss 8.346\n",
      "Epoch 3131 : Total Loss 8.322\n",
      "Epoch 3141 : Total Loss 8.299\n",
      "Epoch 3151 : Total Loss 8.275\n",
      "Epoch 3161 : Total Loss 8.252\n",
      "Epoch 3171 : Total Loss 8.230\n",
      "Epoch 3181 : Total Loss 8.207\n",
      "Epoch 3191 : Total Loss 8.184\n",
      "Epoch 3201 : Total Loss 8.162\n",
      "Epoch 3211 : Total Loss 8.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3221 : Total Loss 8.118\n",
      "Epoch 3231 : Total Loss 8.097\n",
      "Epoch 3241 : Total Loss 8.075\n",
      "Epoch 3251 : Total Loss 8.054\n",
      "Epoch 3261 : Total Loss 8.033\n",
      "Epoch 3271 : Total Loss 8.012\n",
      "Epoch 3281 : Total Loss 7.991\n",
      "Epoch 3291 : Total Loss 7.971\n",
      "Epoch 3301 : Total Loss 7.951\n",
      "Epoch 3311 : Total Loss 7.930\n",
      "Epoch 3321 : Total Loss 7.910\n",
      "Epoch 3331 : Total Loss 7.891\n",
      "Epoch 3341 : Total Loss 7.871\n",
      "Epoch 3351 : Total Loss 7.851\n",
      "Epoch 3361 : Total Loss 7.832\n",
      "Epoch 3371 : Total Loss 7.813\n",
      "Epoch 3381 : Total Loss 7.794\n",
      "Epoch 3391 : Total Loss 7.775\n",
      "Epoch 3401 : Total Loss 7.757\n",
      "Epoch 3411 : Total Loss 7.738\n",
      "Epoch 3421 : Total Loss 7.720\n",
      "Epoch 3431 : Total Loss 7.702\n",
      "Epoch 3441 : Total Loss 7.684\n",
      "Epoch 3451 : Total Loss 7.666\n",
      "Epoch 3461 : Total Loss 7.648\n",
      "Epoch 3471 : Total Loss 7.631\n",
      "Epoch 3481 : Total Loss 7.614\n",
      "Epoch 3491 : Total Loss 7.596\n",
      "Epoch 3501 : Total Loss 7.579\n",
      "Epoch 3511 : Total Loss 7.562\n",
      "Epoch 3521 : Total Loss 7.546\n",
      "Epoch 3531 : Total Loss 7.529\n",
      "Epoch 3541 : Total Loss 7.513\n",
      "Epoch 3551 : Total Loss 7.496\n",
      "Epoch 3561 : Total Loss 7.480\n",
      "Epoch 3571 : Total Loss 7.464\n",
      "Epoch 3581 : Total Loss 7.448\n",
      "Epoch 3591 : Total Loss 7.432\n",
      "Epoch 3601 : Total Loss 7.417\n",
      "Epoch 3611 : Total Loss 7.401\n",
      "Epoch 3621 : Total Loss 7.386\n",
      "Epoch 3631 : Total Loss 7.371\n",
      "Epoch 3641 : Total Loss 7.356\n",
      "Epoch 3651 : Total Loss 7.341\n",
      "Epoch 3661 : Total Loss 7.326\n",
      "Epoch 3671 : Total Loss 7.311\n",
      "Epoch 3681 : Total Loss 7.297\n",
      "Epoch 3691 : Total Loss 7.282\n",
      "Epoch 3701 : Total Loss 7.268\n",
      "Epoch 3711 : Total Loss 7.254\n",
      "Epoch 3721 : Total Loss 7.240\n",
      "Epoch 3731 : Total Loss 7.226\n",
      "Epoch 3741 : Total Loss 7.212\n",
      "Epoch 3751 : Total Loss 7.198\n",
      "Epoch 3761 : Total Loss 7.185\n",
      "Epoch 3771 : Total Loss 7.172\n",
      "Epoch 3781 : Total Loss 7.158\n",
      "Epoch 3791 : Total Loss 7.145\n",
      "Epoch 3801 : Total Loss 7.132\n",
      "Epoch 3811 : Total Loss 7.119\n",
      "Epoch 3821 : Total Loss 7.106\n",
      "Epoch 3831 : Total Loss 7.093\n",
      "Epoch 3841 : Total Loss 7.081\n",
      "Epoch 3851 : Total Loss 7.068\n",
      "Epoch 3861 : Total Loss 7.056\n",
      "Epoch 3871 : Total Loss 7.044\n",
      "Epoch 3881 : Total Loss 7.032\n",
      "Epoch 3891 : Total Loss 7.019\n",
      "Epoch 3901 : Total Loss 7.008\n",
      "Epoch 3911 : Total Loss 6.996\n",
      "Epoch 3921 : Total Loss 6.984\n",
      "Epoch 3931 : Total Loss 6.972\n",
      "Epoch 3941 : Total Loss 6.961\n",
      "Epoch 3951 : Total Loss 6.949\n",
      "Epoch 3961 : Total Loss 6.938\n",
      "Epoch 3971 : Total Loss 6.927\n",
      "Epoch 3981 : Total Loss 6.916\n",
      "Epoch 3991 : Total Loss 6.905\n",
      "Epoch 4001 : Total Loss 6.894\n",
      "Epoch 4011 : Total Loss 6.883\n",
      "Epoch 4021 : Total Loss 6.872\n",
      "Epoch 4031 : Total Loss 6.862\n",
      "Epoch 4041 : Total Loss 6.851\n",
      "Epoch 4051 : Total Loss 6.841\n",
      "Epoch 4061 : Total Loss 6.830\n",
      "Epoch 4071 : Total Loss 6.820\n",
      "Epoch 4081 : Total Loss 6.810\n",
      "Epoch 4091 : Total Loss 6.800\n",
      "Epoch 4101 : Total Loss 6.790\n",
      "Epoch 4111 : Total Loss 6.780\n",
      "Epoch 4121 : Total Loss 6.770\n",
      "Epoch 4131 : Total Loss 6.761\n",
      "Epoch 4141 : Total Loss 6.751\n",
      "Epoch 4151 : Total Loss 6.742\n",
      "Epoch 4161 : Total Loss 6.732\n",
      "Epoch 4171 : Total Loss 6.723\n",
      "Epoch 4181 : Total Loss 6.714\n",
      "Epoch 4191 : Total Loss 6.704\n",
      "Epoch 4201 : Total Loss 6.695\n",
      "Epoch 4211 : Total Loss 6.686\n",
      "Epoch 4221 : Total Loss 6.677\n",
      "Epoch 4231 : Total Loss 6.669\n",
      "Epoch 4241 : Total Loss 6.660\n",
      "Epoch 4251 : Total Loss 6.651\n",
      "Epoch 4261 : Total Loss 6.643\n",
      "Epoch 4271 : Total Loss 6.634\n",
      "Epoch 4281 : Total Loss 6.626\n",
      "Epoch 4291 : Total Loss 6.617\n",
      "Epoch 4301 : Total Loss 6.609\n",
      "Epoch 4311 : Total Loss 6.601\n",
      "Epoch 4321 : Total Loss 6.593\n",
      "Epoch 4331 : Total Loss 6.585\n",
      "Epoch 4341 : Total Loss 6.577\n",
      "Epoch 4351 : Total Loss 6.569\n",
      "Epoch 4361 : Total Loss 6.561\n",
      "Epoch 4371 : Total Loss 6.553\n",
      "Epoch 4381 : Total Loss 6.545\n",
      "Epoch 4391 : Total Loss 6.538\n",
      "Epoch 4401 : Total Loss 6.530\n",
      "Epoch 4411 : Total Loss 6.523\n",
      "Epoch 4421 : Total Loss 6.515\n",
      "Epoch 4431 : Total Loss 6.508\n",
      "Epoch 4441 : Total Loss 6.501\n",
      "Epoch 4451 : Total Loss 6.493\n",
      "Epoch 4461 : Total Loss 6.486\n",
      "Epoch 4471 : Total Loss 6.479\n",
      "Epoch 4481 : Total Loss 6.472\n",
      "Epoch 4491 : Total Loss 6.465\n",
      "Epoch 4501 : Total Loss 6.458\n",
      "Epoch 4511 : Total Loss 6.452\n",
      "Epoch 4521 : Total Loss 6.445\n",
      "Epoch 4531 : Total Loss 6.438\n",
      "Epoch 4541 : Total Loss 6.432\n",
      "Epoch 4551 : Total Loss 6.425\n",
      "Epoch 4561 : Total Loss 6.418\n",
      "Epoch 4571 : Total Loss 6.412\n",
      "Epoch 4581 : Total Loss 6.406\n",
      "Epoch 4591 : Total Loss 6.399\n",
      "Epoch 4601 : Total Loss 6.393\n",
      "Epoch 4611 : Total Loss 6.387\n",
      "Epoch 4621 : Total Loss 6.381\n",
      "Epoch 4631 : Total Loss 6.374\n",
      "Epoch 4641 : Total Loss 6.368\n",
      "Epoch 4651 : Total Loss 6.362\n",
      "Epoch 4661 : Total Loss 6.357\n",
      "Epoch 4671 : Total Loss 6.351\n",
      "Epoch 4681 : Total Loss 6.345\n",
      "Epoch 4691 : Total Loss 6.339\n",
      "Epoch 4701 : Total Loss 6.333\n",
      "Epoch 4711 : Total Loss 6.328\n",
      "Epoch 4721 : Total Loss 6.322\n",
      "Epoch 4731 : Total Loss 6.316\n",
      "Epoch 4741 : Total Loss 6.311\n",
      "Epoch 4751 : Total Loss 6.305\n",
      "Epoch 4761 : Total Loss 6.300\n",
      "Epoch 4771 : Total Loss 6.295\n",
      "Epoch 4781 : Total Loss 6.289\n",
      "Epoch 4791 : Total Loss 6.284\n",
      "Epoch 4801 : Total Loss 6.279\n",
      "Epoch 4811 : Total Loss 6.274\n",
      "Epoch 4821 : Total Loss 6.269\n",
      "Epoch 4831 : Total Loss 6.263\n",
      "Epoch 4841 : Total Loss 6.258\n",
      "Epoch 4851 : Total Loss 6.253\n",
      "Epoch 4861 : Total Loss 6.248\n",
      "Epoch 4871 : Total Loss 6.244\n",
      "Epoch 4881 : Total Loss 6.239\n",
      "Epoch 4891 : Total Loss 6.234\n",
      "Epoch 4901 : Total Loss 6.229\n",
      "Epoch 4911 : Total Loss 6.224\n",
      "Epoch 4921 : Total Loss 6.220\n",
      "Epoch 4931 : Total Loss 6.215\n",
      "Epoch 4941 : Total Loss 6.210\n",
      "Epoch 4951 : Total Loss 6.206\n",
      "Epoch 4961 : Total Loss 6.201\n",
      "Epoch 4971 : Total Loss 6.197\n",
      "Epoch 4981 : Total Loss 6.192\n",
      "Epoch 4991 : Total Loss 6.188\n",
      "U_hat is:\n",
      " [[ 0.417022    0.72032449]\n",
      " [ 1.28692278  1.1330305 ]\n",
      " [ 1.55601561  1.47861269]\n",
      " [ 1.05261729  0.76524973]\n",
      " [ 0.39676747  0.53881673]\n",
      " [ 0.25243383  0.49387642]\n",
      " [ 0.70419227  1.63261983]\n",
      " [ 0.02738759  0.67046751]\n",
      " [ 0.48106314  0.40329958]\n",
      " [ 0.63886828  0.56652471]\n",
      " [ 0.46617673  0.84367056]\n",
      " [ 0.31342418  0.69232262]\n",
      " [ 1.91050334  1.25942089]\n",
      " [ 0.08504421  0.03905478]\n",
      " [ 0.16983042  0.8781425 ]\n",
      " [ 0.09834683  0.42110763]\n",
      " [ 1.4029      1.09632446]\n",
      " [ 1.86128636  1.06956791]\n",
      " [ 1.03575136  1.05508035]\n",
      " [ 0.28392703  2.2855634 ]\n",
      " [ 1.28453935  1.07294701]\n",
      " [ 0.10405118  0.49500656]\n",
      " [ 0.42457478  1.59543607]\n",
      " [ 0.9085955   0.29361415]\n",
      " [ 0.28777534  0.13002857]\n",
      " [ 0.01936696  0.67883553]\n",
      " [ 1.8657183   1.3822476 ]\n",
      " [ 0.49157316  0.05336255]\n",
      " [ 0.57411761  0.14672857]\n",
      " [ 0.58930554  0.69975836]\n",
      " [ 0.10233443  0.41405599]\n",
      " [ 1.21818381  0.68491836]\n",
      " [ 0.29504064  0.91413613]\n",
      " [ 0.66379465  0.51488911]\n",
      " [ 0.94459476  0.58655504]\n",
      " [ 0.90340192  0.1374747 ]\n",
      " [ 0.13927635  0.80739129]\n",
      " [ 1.61729538  1.38212401]\n",
      " [ 0.92750858  0.34776586]\n",
      " [ 2.06564072  0.62311505]\n",
      " [ 1.0640812   0.80316477]\n",
      " [ 0.75094243  0.34889834]\n",
      " [ 0.26992789  0.89588622]\n",
      " [ 0.40673013  1.65466556]\n",
      " [ 0.94461947  1.45924824]\n",
      " [ 0.32676473  1.50241495]\n",
      " [ 0.44991213  0.57838961]\n",
      " [ 0.88271888  0.97261761]\n",
      " [ 1.38354137  1.20880636]\n",
      " [ 0.42394024  0.89209248]\n",
      " [ 0.07509598  1.36835289]\n",
      " [ 0.8859421   0.35726976]\n",
      " [ 0.90853515  0.62336012]\n",
      " [ 0.33908944  1.28933725]\n",
      " [ 1.42302227  1.77134113]\n",
      " [ 0.17234051  0.13713575]\n",
      " [ 1.48538128  1.86025487]\n",
      " [ 0.06600017  0.75546305]\n",
      " [ 0.75387619  0.92302454]\n",
      " [ 2.39851408  1.18331289]\n",
      " [ 0.01988013  0.02621099]\n",
      " [ 1.40378484  0.90193457]\n",
      " [ 1.65570628  1.35730753]\n",
      " [ 0.95035317  1.87372426]\n",
      " [ 1.38540727  1.03536961]\n",
      " [ 0.26712594  1.79320336]\n",
      " [ 2.09559912  1.088364  ]\n",
      " [ 0.80063267  0.23297427]\n",
      " [ 1.88692403  1.06073522]\n",
      " [ 1.84768011  1.88202561]\n",
      " [ 0.56659326  0.12410921]\n",
      " [ 0.05991769  0.12134346]\n",
      " [ 0.63181341  1.69758637]\n",
      " [ 0.55519398  1.85171265]\n",
      " [ 0.71858814  0.07780781]\n",
      " [ 0.07197428  0.96727633]\n",
      " [ 0.56810046  0.20329323]\n",
      " [ 1.06930401  1.65952789]\n",
      " [ 0.53191537  0.7809097 ]\n",
      " [ 1.53516781  1.31129374]\n",
      " [ 0.23984776  0.49376971]\n",
      " [ 1.28803     1.2540855 ]\n",
      " [ 0.15679139  0.0185762 ]\n",
      " [ 0.22867141  1.8867234 ]\n",
      " [ 1.02924862  1.62628839]\n",
      " [ 0.31736241  0.98861615]\n",
      " [ 0.57974522  0.38014117]\n",
      " [ 0.75753511  0.98072379]\n",
      " [ 1.42718221 -0.1723215 ]\n",
      " [ 0.47810088  1.64060754]\n",
      " [ 2.2036811   0.82189592]\n",
      " [ 0.95241835  0.23779183]\n",
      " [ 0.2603151   0.80475456]\n",
      " [ 0.58354611  0.88015632]\n",
      " [ 0.7300612   1.03624816]\n",
      " [ 1.12263178  0.62535154]\n",
      " [ 0.73506596  0.77217803]\n",
      " [ 0.90781585  0.93197207]\n",
      " [ 0.01395157  0.23436209]\n",
      " [ 0.7027521   1.29973424]]\n",
      "V_hat is:\n",
      " [[0.95017612 0.55665319]\n",
      " [0.91560635 0.64156621]\n",
      " [0.48042926 0.88697674]\n",
      " [0.60431048 0.54954792]\n",
      " [0.92618143 0.91873344]\n",
      " [0.39487561 0.96326253]\n",
      " [1.88438166 2.18805555]\n",
      " [0.27444612 2.14706995]\n",
      " [0.02152481 0.94797021]\n",
      " [0.82711547 0.01501898]\n",
      " [0.17619626 0.33206357]\n",
      " [1.13104085 1.77688205]\n",
      " [0.34473665 0.94010748]\n",
      " [0.58201418 0.87883198]\n",
      " [1.93346657 0.28718988]\n",
      " [0.45988027 0.54634682]\n",
      " [1.27027653 0.65610033]\n",
      " [0.49025352 0.59911031]\n",
      " [0.58235181 0.67780029]\n",
      " [0.89444977 1.23231565]\n",
      " [0.3152448  0.89288871]\n",
      " [1.30823262 1.01917221]\n",
      " [0.78792923 0.61203118]\n",
      " [0.78284707 1.73241927]\n",
      " [0.67906884 0.91860178]\n",
      " [0.38507012 2.16517352]\n",
      " [0.37658031 0.97378354]\n",
      " [0.88901066 0.35299529]\n",
      " [0.75949395 0.66014622]\n",
      " [1.09956527 1.21507206]\n",
      " [0.75002176 0.85831384]\n",
      " [0.95410909 1.29201516]\n",
      " [1.33907295 0.75569198]\n",
      " [0.67078879 0.45087394]\n",
      " [1.75204171 1.47451157]\n",
      " [0.40147958 0.31738395]\n",
      " [0.87230507 0.67908025]\n",
      " [1.97938308 1.22090252]\n",
      " [1.83542929 1.04394131]\n",
      " [0.34334624 0.7976388 ]\n",
      " [0.87999829 0.90384196]\n",
      " [0.77572612 0.70719252]\n",
      " [0.2523667  0.85489794]\n",
      " [0.52771465 0.80216108]\n",
      " [0.57248852 0.73314253]\n",
      " [0.51901163 0.77088391]\n",
      " [0.56885799 0.46570988]\n",
      " [0.61755552 0.50657425]\n",
      " [1.59215752 0.56889442]\n",
      " [0.98281711 0.18161285]\n",
      " [0.8118587  0.87496164]\n",
      " [0.68841325 0.56949441]\n",
      " [0.16097144 0.46688002]\n",
      " [0.34517205 0.22503996]\n",
      " [0.59251187 0.31226984]\n",
      " [1.09058211 1.0326581 ]\n",
      " [1.57651487 0.66211006]\n",
      " [0.51015126 1.6736762 ]\n",
      " [1.87757928 0.14478839]\n",
      " [0.47877359 1.74665002]\n",
      " [1.32798621 1.30584536]\n",
      " [1.05152617 0.74546705]\n",
      " [0.51585702 0.47714099]\n",
      " [0.15267164 0.62180623]\n",
      " [1.76915382 2.26978845]\n",
      " [0.3090055  2.07653188]\n",
      " [0.22204914 0.51935182]\n",
      " [0.78529603 0.02233043]\n",
      " [0.987697   1.25900172]\n",
      " [0.84470961 0.53844059]\n",
      " [0.86660827 0.94980599]\n",
      " [0.43857524 1.80310536]\n",
      " [0.0987434  0.65130433]\n",
      " [0.70351699 0.61024081]\n",
      " [0.79961526 0.03457122]\n",
      " [1.46201684 1.64256087]\n",
      " [0.25969839 0.2570693 ]\n",
      " [0.63230332 0.34529746]\n",
      " [1.10895233 0.79702541]\n",
      " [0.78274941 0.99047178]\n",
      " [1.31922243 2.01656263]\n",
      " [1.95540025 1.22213532]\n",
      " [0.97474037 0.6366044 ]\n",
      " [1.9156052  1.34404524]\n",
      " [0.52642593 0.1354279 ]\n",
      " [0.65283948 0.61851633]\n",
      " [0.16039518 0.74563719]\n",
      " [0.03039969 0.3665431 ]\n",
      " [2.06229477 0.58882883]\n",
      " [1.55877213 0.88873561]\n",
      " [1.01875832 1.67096268]\n",
      " [1.38734933 0.22085069]\n",
      " [1.10410839 0.62504914]\n",
      " [1.74601975 1.25131454]\n",
      " [0.61459846 1.96933946]\n",
      " [0.21196016 0.79860424]\n",
      " [1.41087877 0.99921471]\n",
      " [0.85518621 1.54645533]\n",
      " [0.2179855  0.51089184]\n",
      " [1.43417443 2.01863838]]\n",
      "A is:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "A_hat is:\n",
      " [[0.79721528 0.84396385 0.83926065 ... 1.47058112 0.45891265 2.05215697]\n",
      " [1.85350834 1.90522875 1.62324707 ... 2.85273967 0.85938653 4.13285061]\n",
      " [2.30156334 2.37332571 2.0590505  ... 3.61729157 1.09459999 5.21638214]\n",
      " ...\n",
      " [1.38137017 1.42912375 1.26277885 ... 2.21760477 0.67402761 3.18328088]\n",
      " [0.14371485 0.16313294 0.21457646 ... 0.37436169 0.12277492 0.49310129]\n",
      " [1.39123947 1.47730985 1.49045671 ... 2.61096484 0.81721338 3.63156251]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "m, n = arr_100_100.shape[0], arr_100_100.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "U_3 = np.random.rand(m, user_features)\n",
    "V_3 = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U_3)\n",
    "print('V is:\\n', V_3)\n",
    "\n",
    "# perform factorization\n",
    "start4 = t.default_timer()\n",
    "\n",
    "U_hat, V_hat = matrix_factorization(arr_100_100, U_3, V_3, user_features, 5000)\n",
    "\n",
    "end4 = t.default_timer()\n",
    "\n",
    "A_hat = np.dot(U_hat, V_hat.T)\n",
    "\n",
    "print('U_hat is:\\n', U_hat)\n",
    "print('V_hat is:\\n', V_hat)\n",
    "\n",
    "## check if the product results in something close to A\n",
    "print('A is:\\n', arr_100_100)\n",
    "print('A_hat is:\\n', A_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ef73faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The runtime of the regular matrix factorization on a 100 x 100, 99% sparse matrix is 52.94888380006887 seconds.\n",
      "The mean squared error of all predictions is 0.008.\n",
      "The root mean squared error of all predictions is 0.089.\n"
     ]
    }
   ],
   "source": [
    "# get our evaluation metrics\n",
    "\n",
    "print(f'The runtime of the regular matrix factorization on a 100 x 100, 99% sparse matrix is {end4 - start4} seconds.')\n",
    "\n",
    "mse, rmse = evaluation_metrics(arr_100_100, A_hat)\n",
    "\n",
    "print(f'The mean squared error of all predictions is {mse:.3f}.')\n",
    "print(f'The root mean squared error of all predictions is {rmse:.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf1e91a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get our predictions\n",
    "\n",
    "mat_predict(arr_100_100, A_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52843964",
   "metadata": {},
   "source": [
    "This is almost a 900% improvement in runtime! To see why, we can do a brief complexity analysis. In this case, looking at $\\mathcal{O}$ is helpful because we are looking to factorize huge, sparse matrices.\n",
    "\n",
    "### Complexity Analysis\n",
    "\n",
    "The time complexity of the matrix factorization methods are not entirely clear as it depends on the cases that we are looking at.\n",
    "\n",
    "Let us lay down some definitions:\n",
    "\n",
    "**Sparsity (s)**:\n",
    "\n",
    "$s$ is a parameter denoting the proportion of 0's in a matrix compared to the total number of entries. For an $m\\times n$ interaction matrix, $A$, we see that the **sparsity of $A$** is:\n",
    "\n",
    "$$s = \\frac{\\mathrm{Number \\; of \\; 0's}}{nm}$$\n",
    "\n",
    "**Density (d)**:\n",
    "\n",
    "Likewise, $d$ is a parameter denoting the proportion of nonzero's compared to the total number of entries. For $A$ indicated above, the **density of $A$** is:\n",
    "\n",
    "$$d = \\frac{\\mathrm{Number \\;of\\;nonzero's}}{nm}$$\n",
    "\n",
    "Sparsity and density are related by the following expression:\n",
    "\n",
    "$$s = 1 - d$$\n",
    "\n",
    "(rearrange to see density in terms of sparsity)\n",
    "\n",
    "Now onto our analysis:\n",
    "\n",
    "1) **Case 1: Dense Matrices**\n",
    "\n",
    "There is no clear definition, but for our purposes, we consider a **dense matrix**, $A$ (assume $m\\times n$) as one in which there is a **density** of at least $\\frac{nm}{2}$, where the nonzero entries are distributed with little bias across all rows and columns.\n",
    "\n",
    "Let $k$ denote the **number of latent features**, $n$ denote the **number of columns**, and $m$ the **number of rows**. Assume that our number of latent features is *fixed*, and is far smaller than the dimensions of our interaction table.  Assume that our training loop persists for $e$ **epochs** and that $e$ is *fixed* (in practice, the number of epochs can get large, but, in principle, it is fixed before we run our training loop).\n",
    "\n",
    "Let $N := \\max(m,n)$. For a general $m\\times n$ ground truth, interaction matrix, $A$, the *regular matrix factorization* implementation has a big $\\mathcal{O}$ time complexity of $\\mathcal{O}(ekmn)$. In our case, since the latent features and epochs are fixed in size (the dimensions of the matrix are features of our data and model), we can consider $k << N$, $e << N$, and $m$ and $n$ to be arbitrarily large, and we see that the worst-case runtime is actually just $\\mathcal{O}(N^{2})$.\n",
    "\n",
    "Now look at the *sparse matrix factorization* implementation, note that this is no different than the *regular matrix factorization* as when we isolate the indices corresponding to nonzero entries, we, essentially, must traverse at least $0.5nm$ entries in $A$. Therefore, the time complexity of the sparse MF is just $\\mathcal{O}(N^{2})$.\n",
    "\n",
    "2) **Case 2: Sparse Matrices**\n",
    "\n",
    "We consider a **sparse matrix** as one in which the **sparsity** is **at least** $1 - 1/\\min(n,m)$ (i.e. the number of nonzero entries is approximately the number of rows or the number of columns). With notation above, we see that the worst-case runtime for the *sparse matrix factorization* is $\\mathcal{O}(N)$, and the worst-case runtime for the *regular matrix factorization* is $\\mathcal{O}(N^{2})$, and to see this, just recognize that $e$ and $k$ are fixed as above. \n",
    "\n",
    "In our *sparse matrix factorization*, since **we only traverse nonzero entries of the interaction table**, we are only running our training loop on at most $\\max(n,m)$ entries. Therefore, since $k$ and $e$ are fixed, we get a time complexity of $\\sim \\mathcal{O}(N)$.\n",
    "\n",
    "In our *regular matrix factorization*, since **we must traverse ALL entries of the interaction table**, we are running our training loop for $nm$ entries as we **must traverse the zero entries in order to check that they are zero**. Therefore, we have a time complexity of $\\sim \\mathcal{O}(N^{2})$.\n",
    "\n",
    "### Why is Sparsity Important?\n",
    "\n",
    "Sparsity is an important assumption about our input data that is actually true for the business problem. Oftentimes, when we are trying to analyze interactions, we will have many entries missing. Most people either do not leave ratings or the ratings are often not present due to some preprocessing method (like one-hot encoding). A sparse matrix is a very realistic and conservative assumption to make when we are trying to generate recommendations.\n",
    "\n",
    "Not only does sparsity optimize our method above, but it is an assumption that often holds in the business problem where matrix factorization is relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommenders",
   "language": "python",
   "name": "recommenders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
