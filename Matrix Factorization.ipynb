{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2e48f8",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "We will implement matrix factorization here, both manually and maybe through TensorFlow.\n",
    "\n",
    "## Theory Behind Matrix Factorization: Problem Statement\n",
    "\n",
    "We will give a brief rundown, nothing too complicated.\n",
    "\n",
    "The main **mathematical** statement that we will tackle with matrix factorization is as follows:\n",
    "\n",
    "We have some matrix $A$ (this could be a tabular dataset or anything else). Now the *caveat* is that $A$ has **missing entries** that do not know about. We want to find two matrices $U$ and $V$, so that their product:\n",
    "\n",
    "$$U \\cdot V^{T} \\approx A$$\n",
    "\n",
    "So really, in terms of an optimization problem, we want to minimize each entry of the matrix $E$:\n",
    "\n",
    "$$E = A - U\\cdot V^{T}$$\n",
    "\n",
    "It really helps to write this in index notation to represent each component so that we are not dealing with matrices directly. Let $E_{ij} := e_{ij}$, then, writing out the above expression in matrix form, we see that:\n",
    "\n",
    "$$e_{ij} = a_{ij} - \\sum_{k}u_{ik}v_{jk}^{T} = a_{ij} - \\sum_{k}u_{ik}v_{kj}$$\n",
    "\n",
    "We are now only working with real numbers! We can now adopt the traditional MSE loss for each entry.\n",
    "\n",
    "$$e_{ij}^{2} = \\left(a_{ij} - \\sum_{k}u_{ik}v_{kj}\\right)^{2}$$\n",
    "\n",
    "or to make it look simpler, we can write $\\hat{a}_{ij} = \\sum_{k}u_{ik}v_{kj}$, and we now have:\n",
    "\n",
    "$$e_{ij}^{2} = (a_{ij} - \\hat{a}_{ij})^{2}$$\n",
    "\n",
    "We are now minimizing this expression for all $(i,j) \\in rows(A), cols(A))$. **For all intents and purposes**, this is all we really need to know to continue.\n",
    "\n",
    "## Theory of Matrix Factorization: Machine Learning\n",
    "\n",
    "However, we can continue our derivation of our equations, as this will help us in our implementation. We take the gradient (derivative) of our loss above, with respect to the nonzero components of both $U$ and $V$ (**Notice that we are not training weights and biases, but just the entries of the matrices $U$ and $V$!!!**):\n",
    "\n",
    "$$\\frac{\\partial e_{ij}^{2}}{\\partial u_{ik}} = \\frac{\\partial e_{ij}^{2}}{\\partial e_{ij}}\\frac{\\partial e_{ij}}{\\partial u_{ik}} = -2e_{ij}v_{kj} $$\n",
    "\n",
    "Likewise, for the other component:\n",
    "\n",
    "$$\\frac{\\partial e_{ij}^{2}}{\\partial v_{ik}} = -2e_{ij}u_{ik}$$\n",
    "\n",
    "We now do the **optimization step** (i.e. the *Gradient Descent Step*):\n",
    "\n",
    "$$u_{ik} \\longmapsto u_{ik} - \\lambda \\frac{\\partial e_{ij}^{2}}{\\partial u_{ik}} = u_{ik} + 2\\lambda e_{ij}v_{kj}$$\n",
    "\n",
    "$$v_{kj} \\longmapsto v_{kj} - \\lambda \\frac{\\partial e_{ij}^{2}}{\\partial v_{kj}} = v_{kj} + 2\\lambda e_{ij}u_{ik}$$\n",
    "\n",
    "## Business Use of Matrix Factorization\n",
    "\n",
    "The business problem we are trying to solve is the following:\n",
    "\n",
    "We are given a table of **customers** and **products** they interacted with, however, some customers do not leave ratings on the product that they interacted with. In this case, how do we find these missing ratings?\n",
    "\n",
    "Furthermore, let us say that we track certain features for the customers and products. For example:\n",
    "\n",
    "Let $U$ denote the customers, and let $V$ denote items bought at a food mart. Let us say that $U$, $V$ contains **2 features**, sweet and savory, and let $A$ be the rating of sweet or savory items (from 1-5) by a customer that purchased them. Sweet and savory are what we refer to as **latent features**, and they relate customers to products as they are keys that merge customers and products. \n",
    "\n",
    "Now assume $A$ is the following customer-product interaction table...\n",
    "\n",
    "$$A = \\begin{pmatrix} \n",
    "4 & 0 & 3 & 2 & 2 & 1\\\\[3pt]\n",
    "0 & 5 & 3 & 4 & 5 & 1\\\\[3pt]\n",
    "5 & 5 & 0 & 0 & 3 & 5\\\\[3pt]\n",
    "3 & 3 & 4 & 3 & 2 & 0\\\\[3pt]\n",
    "2 & 4 & 2 & 4 & 5 & 1\\\\[3pt]\n",
    "0 & 0 & 5 & 3 & 3 & 4\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then we want to find $U$ and $V$ such that:\n",
    "\n",
    "- $U$ encodes the latent features (sweet or savory) of the customers, and $V$ does the same for the products.\n",
    "- $U \\cdot V^{T} \\approx A$.\n",
    "\n",
    "Thus, we are looking for $U$, $V$ such that:\n",
    "\n",
    "$$A \\approx U\\cdot V^{T}$$\n",
    "\n",
    "Now refer to the previous section for the mathematical details of our machine learning problem.\n",
    "\n",
    "\n",
    "## Manual Implementation of Matrix Factorization\n",
    "\n",
    "By \"manual implementation\", we mean *implementation in python with only numpy*. This is possible because we are using the following loss:\n",
    "\n",
    "$$\\mathrm{Loss} = \\mathrm{MSE} + \\mathrm{Regularization}$$\n",
    "\n",
    "So we have a closed-form expression for the gradient of the loss in this case. If we wanted to implement more complicated loss functions, we will likely need to use an automatic differentiation package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ab2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f4aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARGUMENTS\n",
    "# A - ground truth matrix\n",
    "# U - product 1\n",
    "# V - product 2\n",
    "# number of features\n",
    "# epochs - number of epochs to train\n",
    "# lambda - learning rate\n",
    "# beta - regularization parameter\n",
    "\n",
    "def matrix_factorization(A, U, V, k, steps, lmbda=0.0002, beta = 0.02):\n",
    "    V = V.T\n",
    "\n",
    "    for epoch in range(steps):\n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                # we are only looking to minimize the loss on the already-filled entries\n",
    "                if A[i][j] > 0:\n",
    "                    # error\n",
    "                    eij = A[i][j] - np.dot(U[i, :], V[:, j])\n",
    "                    \n",
    "                    for K in range(k):\n",
    "                        # gradient descent step\n",
    "                        U[i][K] = U[i][K] + lmbda * (2*eij * V[K][j] - beta*U[i][K])\n",
    "                        V[K][j] = V[K][j] + lmbda * (2*eij * U[i][K] - beta*V[K][j])\n",
    "                        \n",
    "        er = np.dot(U,V)\n",
    "        \n",
    "        # e is our loss (i.e. our e_{ij}^{2})\n",
    "        e = 0\n",
    "        \n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                if A[i][j] > 0:\n",
    "                    # update the loss\n",
    "                    e = e + (A[i][j] - np.dot(U[i,:], V[:,j]))**2\n",
    "                    \n",
    "                    for K in range(k):\n",
    "                        # put regularization term on loss so it doesn't overfit on training data\n",
    "                        e = e + (beta/2) * (U[i][K]**2 + V[K][j]**2)\n",
    "                     \n",
    "        # give print output of training (print only every 10 epochs)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} : Total Loss {e:.3f}')\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return U, V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0113a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.9600173  0.69951205]\n",
      " [0.99986729 0.2200673 ]\n",
      " [0.36105635 0.73984099]\n",
      " [0.99645573 0.31634698]\n",
      " [0.13654458 0.38398001]]\n",
      "V is:\n",
      " [[0.32051928 0.36641475]\n",
      " [0.70965156 0.90014243]\n",
      " [0.53411544 0.24729376]\n",
      " [0.67180656 0.56172911]\n",
      " [0.54255988 0.8934476 ]]\n",
      "Epoch 1 : Total Loss 99.275\n",
      "Epoch 11 : Total Loss 96.767\n",
      "Epoch 21 : Total Loss 94.202\n",
      "Epoch 31 : Total Loss 91.584\n",
      "Epoch 41 : Total Loss 88.918\n",
      "Epoch 51 : Total Loss 86.207\n",
      "Epoch 61 : Total Loss 83.459\n",
      "Epoch 71 : Total Loss 80.680\n",
      "Epoch 81 : Total Loss 77.878\n",
      "Epoch 91 : Total Loss 75.059\n",
      "Epoch 101 : Total Loss 72.233\n",
      "Epoch 111 : Total Loss 69.408\n",
      "Epoch 121 : Total Loss 66.595\n",
      "Epoch 131 : Total Loss 63.801\n",
      "Epoch 141 : Total Loss 61.036\n",
      "Epoch 151 : Total Loss 58.311\n",
      "Epoch 161 : Total Loss 55.633\n",
      "Epoch 171 : Total Loss 53.012\n",
      "Epoch 181 : Total Loss 50.455\n",
      "Epoch 191 : Total Loss 47.970\n",
      "U_hat is:\n",
      " [[1.2427563  0.99404657]\n",
      " [1.32958304 0.4449768 ]\n",
      " [0.53087359 0.91554011]\n",
      " [1.23935835 0.47920855]\n",
      " [0.51268872 0.8134178 ]]\n",
      "V_hat is:\n",
      " [[1.02502384 1.0589032 ]\n",
      " [0.87169684 1.08305621]\n",
      " [0.97105056 0.41299391]\n",
      " [0.95914034 0.64674232]\n",
      " [0.55186549 0.90495035]]\n",
      "A_hat is:\n",
      " [[2.32645393 2.15991506 1.61731439 1.83486969 1.58539711]\n",
      " [1.83404167 1.64092823 1.47486507 1.56304206 1.13643291]\n",
      " [1.51362644 1.45434224 0.89361759 1.10130081 1.12148916]\n",
      " [1.77780732 1.59935457 1.40138984 1.49864305 1.11761906]\n",
      " [1.38684887 1.32788634 0.83378327 1.01781215 1.01903794]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [5,3,2,1,0],\n",
    "    [3,0,4,2,1],\n",
    "    [4,0,0,1,0],\n",
    "    [0,0,2,4,0],\n",
    "    [5,3,0,0,1]\n",
    "])\n",
    "\n",
    "m = A.shape[0]\n",
    "n = A.shape[1]\n",
    "user_features = 2\n",
    "\n",
    "np.random.seed(24)\n",
    "\n",
    "# generate matrices with entries of random numbers 0 - 1\n",
    "U = np.random.rand(m, user_features)\n",
    "V = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U)\n",
    "print('V is:\\n', V)\n",
    "\n",
    "# perform factorization\n",
    "\n",
    "U_hat, V_hat = matrix_factorization(A, U, V, user_features, 200)\n",
    "\n",
    "print('U_hat is:\\n', U_hat)\n",
    "print('V_hat is:\\n', V_hat)\n",
    "\n",
    "## check if the product results in something close to A\n",
    "\n",
    "print('A_hat is:\\n', np.dot(U_hat, V_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9e1ff",
   "metadata": {},
   "source": [
    "This does not look good after 200 epochs! Let's try more epochs, and if that doesn't work, let's increase the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a54ec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 45.564\n",
      "Epoch 11 : Total Loss 43.244\n",
      "Epoch 21 : Total Loss 41.013\n",
      "Epoch 31 : Total Loss 38.877\n",
      "Epoch 41 : Total Loss 36.838\n",
      "Epoch 51 : Total Loss 34.898\n",
      "Epoch 61 : Total Loss 33.060\n",
      "Epoch 71 : Total Loss 31.323\n",
      "Epoch 81 : Total Loss 29.687\n",
      "Epoch 91 : Total Loss 28.150\n",
      "Epoch 101 : Total Loss 26.711\n",
      "Epoch 111 : Total Loss 25.366\n",
      "Epoch 121 : Total Loss 24.114\n",
      "Epoch 131 : Total Loss 22.950\n",
      "Epoch 141 : Total Loss 21.870\n",
      "Epoch 151 : Total Loss 20.871\n",
      "Epoch 161 : Total Loss 19.947\n",
      "Epoch 171 : Total Loss 19.095\n",
      "Epoch 181 : Total Loss 18.310\n",
      "Epoch 191 : Total Loss 17.587\n",
      "Epoch 201 : Total Loss 16.923\n",
      "Epoch 211 : Total Loss 16.312\n",
      "Epoch 221 : Total Loss 15.751\n",
      "Epoch 231 : Total Loss 15.236\n",
      "Epoch 241 : Total Loss 14.763\n",
      "Epoch 251 : Total Loss 14.328\n",
      "Epoch 261 : Total Loss 13.929\n",
      "Epoch 271 : Total Loss 13.563\n",
      "Epoch 281 : Total Loss 13.225\n",
      "Epoch 291 : Total Loss 12.914\n",
      "Epoch 301 : Total Loss 12.628\n",
      "Epoch 311 : Total Loss 12.364\n",
      "Epoch 321 : Total Loss 12.120\n",
      "Epoch 331 : Total Loss 11.894\n",
      "Epoch 341 : Total Loss 11.684\n",
      "Epoch 351 : Total Loss 11.490\n",
      "Epoch 361 : Total Loss 11.309\n",
      "Epoch 371 : Total Loss 11.140\n",
      "Epoch 381 : Total Loss 10.983\n",
      "Epoch 391 : Total Loss 10.835\n",
      "Epoch 401 : Total Loss 10.697\n",
      "Epoch 411 : Total Loss 10.568\n",
      "Epoch 421 : Total Loss 10.445\n",
      "Epoch 431 : Total Loss 10.330\n",
      "Epoch 441 : Total Loss 10.221\n",
      "Epoch 451 : Total Loss 10.118\n",
      "Epoch 461 : Total Loss 10.020\n",
      "Epoch 471 : Total Loss 9.927\n",
      "Epoch 481 : Total Loss 9.838\n",
      "Epoch 491 : Total Loss 9.753\n",
      "Epoch 501 : Total Loss 9.672\n",
      "Epoch 511 : Total Loss 9.594\n",
      "Epoch 521 : Total Loss 9.519\n",
      "Epoch 531 : Total Loss 9.448\n",
      "Epoch 541 : Total Loss 9.378\n",
      "Epoch 551 : Total Loss 9.312\n",
      "Epoch 561 : Total Loss 9.247\n",
      "Epoch 571 : Total Loss 9.185\n",
      "Epoch 581 : Total Loss 9.125\n",
      "Epoch 591 : Total Loss 9.066\n",
      "Epoch 601 : Total Loss 9.009\n",
      "Epoch 611 : Total Loss 8.954\n",
      "Epoch 621 : Total Loss 8.900\n",
      "Epoch 631 : Total Loss 8.847\n",
      "Epoch 641 : Total Loss 8.796\n",
      "Epoch 651 : Total Loss 8.746\n",
      "Epoch 661 : Total Loss 8.697\n",
      "Epoch 671 : Total Loss 8.649\n",
      "Epoch 681 : Total Loss 8.602\n",
      "Epoch 691 : Total Loss 8.556\n",
      "Epoch 701 : Total Loss 8.511\n",
      "Epoch 711 : Total Loss 8.467\n",
      "Epoch 721 : Total Loss 8.423\n",
      "Epoch 731 : Total Loss 8.380\n",
      "Epoch 741 : Total Loss 8.338\n",
      "Epoch 751 : Total Loss 8.297\n",
      "Epoch 761 : Total Loss 8.256\n",
      "Epoch 771 : Total Loss 8.216\n",
      "Epoch 781 : Total Loss 8.176\n",
      "Epoch 791 : Total Loss 8.137\n",
      "Epoch 801 : Total Loss 8.099\n",
      "Epoch 811 : Total Loss 8.061\n",
      "Epoch 821 : Total Loss 8.024\n",
      "Epoch 831 : Total Loss 7.987\n",
      "Epoch 841 : Total Loss 7.950\n",
      "Epoch 851 : Total Loss 7.914\n",
      "Epoch 861 : Total Loss 7.878\n",
      "Epoch 871 : Total Loss 7.843\n",
      "Epoch 881 : Total Loss 7.808\n",
      "Epoch 891 : Total Loss 7.774\n",
      "Epoch 901 : Total Loss 7.740\n",
      "Epoch 911 : Total Loss 7.706\n",
      "Epoch 921 : Total Loss 7.673\n",
      "Epoch 931 : Total Loss 7.640\n",
      "Epoch 941 : Total Loss 7.608\n",
      "Epoch 951 : Total Loss 7.575\n",
      "Epoch 961 : Total Loss 7.544\n",
      "Epoch 971 : Total Loss 7.512\n",
      "Epoch 981 : Total Loss 7.481\n",
      "Epoch 991 : Total Loss 7.450\n",
      "Epoch 1001 : Total Loss 7.419\n",
      "Epoch 1011 : Total Loss 7.389\n",
      "Epoch 1021 : Total Loss 7.359\n",
      "Epoch 1031 : Total Loss 7.330\n",
      "Epoch 1041 : Total Loss 7.300\n",
      "Epoch 1051 : Total Loss 7.271\n",
      "Epoch 1061 : Total Loss 7.243\n",
      "Epoch 1071 : Total Loss 7.214\n",
      "Epoch 1081 : Total Loss 7.186\n",
      "Epoch 1091 : Total Loss 7.158\n",
      "Epoch 1101 : Total Loss 7.131\n",
      "Epoch 1111 : Total Loss 7.104\n",
      "Epoch 1121 : Total Loss 7.077\n",
      "Epoch 1131 : Total Loss 7.050\n",
      "Epoch 1141 : Total Loss 7.024\n",
      "Epoch 1151 : Total Loss 6.998\n",
      "Epoch 1161 : Total Loss 6.972\n",
      "Epoch 1171 : Total Loss 6.947\n",
      "Epoch 1181 : Total Loss 6.922\n",
      "Epoch 1191 : Total Loss 6.897\n",
      "Epoch 1201 : Total Loss 6.873\n",
      "Epoch 1211 : Total Loss 6.849\n",
      "Epoch 1221 : Total Loss 6.825\n",
      "Epoch 1231 : Total Loss 6.801\n",
      "Epoch 1241 : Total Loss 6.778\n",
      "Epoch 1251 : Total Loss 6.755\n",
      "Epoch 1261 : Total Loss 6.732\n",
      "Epoch 1271 : Total Loss 6.709\n",
      "Epoch 1281 : Total Loss 6.687\n",
      "Epoch 1291 : Total Loss 6.665\n",
      "Epoch 1301 : Total Loss 6.644\n",
      "Epoch 1311 : Total Loss 6.622\n",
      "Epoch 1321 : Total Loss 6.601\n",
      "Epoch 1331 : Total Loss 6.581\n",
      "Epoch 1341 : Total Loss 6.560\n",
      "Epoch 1351 : Total Loss 6.540\n",
      "Epoch 1361 : Total Loss 6.520\n",
      "Epoch 1371 : Total Loss 6.501\n",
      "Epoch 1381 : Total Loss 6.481\n",
      "Epoch 1391 : Total Loss 6.462\n",
      "Epoch 1401 : Total Loss 6.443\n",
      "Epoch 1411 : Total Loss 6.425\n",
      "Epoch 1421 : Total Loss 6.407\n",
      "Epoch 1431 : Total Loss 6.389\n",
      "Epoch 1441 : Total Loss 6.371\n",
      "Epoch 1451 : Total Loss 6.354\n",
      "Epoch 1461 : Total Loss 6.337\n",
      "Epoch 1471 : Total Loss 6.320\n",
      "Epoch 1481 : Total Loss 6.303\n",
      "Epoch 1491 : Total Loss 6.287\n",
      "Epoch 1501 : Total Loss 6.271\n",
      "Epoch 1511 : Total Loss 6.255\n",
      "Epoch 1521 : Total Loss 6.240\n",
      "Epoch 1531 : Total Loss 6.224\n",
      "Epoch 1541 : Total Loss 6.209\n",
      "Epoch 1551 : Total Loss 6.195\n",
      "Epoch 1561 : Total Loss 6.180\n",
      "Epoch 1571 : Total Loss 6.166\n",
      "Epoch 1581 : Total Loss 6.152\n",
      "Epoch 1591 : Total Loss 6.138\n",
      "Epoch 1601 : Total Loss 6.124\n",
      "Epoch 1611 : Total Loss 6.111\n",
      "Epoch 1621 : Total Loss 6.098\n",
      "Epoch 1631 : Total Loss 6.085\n",
      "Epoch 1641 : Total Loss 6.073\n",
      "Epoch 1651 : Total Loss 6.060\n",
      "Epoch 1661 : Total Loss 6.048\n",
      "Epoch 1671 : Total Loss 6.036\n",
      "Epoch 1681 : Total Loss 6.025\n",
      "Epoch 1691 : Total Loss 6.013\n",
      "Epoch 1701 : Total Loss 6.002\n",
      "Epoch 1711 : Total Loss 5.991\n",
      "Epoch 1721 : Total Loss 5.980\n",
      "Epoch 1731 : Total Loss 5.970\n",
      "Epoch 1741 : Total Loss 5.959\n",
      "Epoch 1751 : Total Loss 5.949\n",
      "Epoch 1761 : Total Loss 5.939\n",
      "Epoch 1771 : Total Loss 5.929\n",
      "Epoch 1781 : Total Loss 5.920\n",
      "Epoch 1791 : Total Loss 5.910\n",
      "Epoch 1801 : Total Loss 5.901\n",
      "Epoch 1811 : Total Loss 5.892\n",
      "Epoch 1821 : Total Loss 5.883\n",
      "Epoch 1831 : Total Loss 5.875\n",
      "Epoch 1841 : Total Loss 5.866\n",
      "Epoch 1851 : Total Loss 5.858\n",
      "Epoch 1861 : Total Loss 5.850\n",
      "Epoch 1871 : Total Loss 5.842\n",
      "Epoch 1881 : Total Loss 5.834\n",
      "Epoch 1891 : Total Loss 5.827\n",
      "Epoch 1901 : Total Loss 5.819\n",
      "Epoch 1911 : Total Loss 5.812\n",
      "Epoch 1921 : Total Loss 5.805\n",
      "Epoch 1931 : Total Loss 5.798\n",
      "Epoch 1941 : Total Loss 5.791\n",
      "Epoch 1951 : Total Loss 5.784\n",
      "Epoch 1961 : Total Loss 5.778\n",
      "Epoch 1971 : Total Loss 5.771\n",
      "Epoch 1981 : Total Loss 5.765\n",
      "Epoch 1991 : Total Loss 5.759\n",
      "Epoch 2001 : Total Loss 5.753\n",
      "Epoch 2011 : Total Loss 5.747\n",
      "Epoch 2021 : Total Loss 5.741\n",
      "Epoch 2031 : Total Loss 5.736\n",
      "Epoch 2041 : Total Loss 5.730\n",
      "Epoch 2051 : Total Loss 5.725\n",
      "Epoch 2061 : Total Loss 5.720\n",
      "Epoch 2071 : Total Loss 5.714\n",
      "Epoch 2081 : Total Loss 5.709\n",
      "Epoch 2091 : Total Loss 5.705\n",
      "Epoch 2101 : Total Loss 5.700\n",
      "Epoch 2111 : Total Loss 5.695\n",
      "Epoch 2121 : Total Loss 5.691\n",
      "Epoch 2131 : Total Loss 5.686\n",
      "Epoch 2141 : Total Loss 5.682\n",
      "Epoch 2151 : Total Loss 5.678\n",
      "Epoch 2161 : Total Loss 5.673\n",
      "Epoch 2171 : Total Loss 5.669\n",
      "Epoch 2181 : Total Loss 5.665\n",
      "Epoch 2191 : Total Loss 5.661\n",
      "Epoch 2201 : Total Loss 5.658\n",
      "Epoch 2211 : Total Loss 5.654\n",
      "Epoch 2221 : Total Loss 5.650\n",
      "Epoch 2231 : Total Loss 5.647\n",
      "Epoch 2241 : Total Loss 5.643\n",
      "Epoch 2251 : Total Loss 5.640\n",
      "Epoch 2261 : Total Loss 5.637\n",
      "Epoch 2271 : Total Loss 5.634\n",
      "Epoch 2281 : Total Loss 5.630\n",
      "Epoch 2291 : Total Loss 5.627\n",
      "Epoch 2301 : Total Loss 5.624\n",
      "Epoch 2311 : Total Loss 5.621\n",
      "Epoch 2321 : Total Loss 5.618\n",
      "Epoch 2331 : Total Loss 5.616\n",
      "Epoch 2341 : Total Loss 5.613\n",
      "Epoch 2351 : Total Loss 5.610\n",
      "Epoch 2361 : Total Loss 5.608\n",
      "Epoch 2371 : Total Loss 5.605\n",
      "Epoch 2381 : Total Loss 5.603\n",
      "Epoch 2391 : Total Loss 5.600\n",
      "Epoch 2401 : Total Loss 5.598\n",
      "Epoch 2411 : Total Loss 5.596\n",
      "Epoch 2421 : Total Loss 5.593\n",
      "Epoch 2431 : Total Loss 5.591\n",
      "Epoch 2441 : Total Loss 5.589\n",
      "Epoch 2451 : Total Loss 5.587\n",
      "Epoch 2461 : Total Loss 5.585\n",
      "Epoch 2471 : Total Loss 5.583\n",
      "Epoch 2481 : Total Loss 5.581\n",
      "Epoch 2491 : Total Loss 5.579\n",
      "Epoch 2501 : Total Loss 5.577\n",
      "Epoch 2511 : Total Loss 5.575\n",
      "Epoch 2521 : Total Loss 5.573\n",
      "Epoch 2531 : Total Loss 5.571\n",
      "Epoch 2541 : Total Loss 5.570\n",
      "Epoch 2551 : Total Loss 5.568\n",
      "Epoch 2561 : Total Loss 5.566\n",
      "Epoch 2571 : Total Loss 5.565\n",
      "Epoch 2581 : Total Loss 5.563\n",
      "Epoch 2591 : Total Loss 5.562\n",
      "Epoch 2601 : Total Loss 5.560\n",
      "Epoch 2611 : Total Loss 5.559\n",
      "Epoch 2621 : Total Loss 5.557\n",
      "Epoch 2631 : Total Loss 5.556\n",
      "Epoch 2641 : Total Loss 5.554\n",
      "Epoch 2651 : Total Loss 5.553\n",
      "Epoch 2661 : Total Loss 5.552\n",
      "Epoch 2671 : Total Loss 5.551\n",
      "Epoch 2681 : Total Loss 5.549\n",
      "Epoch 2691 : Total Loss 5.548\n",
      "Epoch 2701 : Total Loss 5.547\n",
      "Epoch 2711 : Total Loss 5.546\n",
      "Epoch 2721 : Total Loss 5.544\n",
      "Epoch 2731 : Total Loss 5.543\n",
      "Epoch 2741 : Total Loss 5.542\n",
      "Epoch 2751 : Total Loss 5.541\n",
      "Epoch 2761 : Total Loss 5.540\n",
      "Epoch 2771 : Total Loss 5.539\n",
      "Epoch 2781 : Total Loss 5.538\n",
      "Epoch 2791 : Total Loss 5.537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2801 : Total Loss 5.536\n",
      "Epoch 2811 : Total Loss 5.535\n",
      "Epoch 2821 : Total Loss 5.534\n",
      "Epoch 2831 : Total Loss 5.533\n",
      "Epoch 2841 : Total Loss 5.532\n",
      "Epoch 2851 : Total Loss 5.531\n",
      "Epoch 2861 : Total Loss 5.531\n",
      "Epoch 2871 : Total Loss 5.530\n",
      "Epoch 2881 : Total Loss 5.529\n",
      "Epoch 2891 : Total Loss 5.528\n",
      "Epoch 2901 : Total Loss 5.527\n",
      "Epoch 2911 : Total Loss 5.526\n",
      "Epoch 2921 : Total Loss 5.526\n",
      "Epoch 2931 : Total Loss 5.525\n",
      "Epoch 2941 : Total Loss 5.524\n",
      "Epoch 2951 : Total Loss 5.523\n",
      "Epoch 2961 : Total Loss 5.523\n",
      "Epoch 2971 : Total Loss 5.522\n",
      "Epoch 2981 : Total Loss 5.521\n",
      "Epoch 2991 : Total Loss 5.521\n",
      "Epoch 3001 : Total Loss 5.520\n",
      "Epoch 3011 : Total Loss 5.519\n",
      "Epoch 3021 : Total Loss 5.519\n",
      "Epoch 3031 : Total Loss 5.518\n",
      "Epoch 3041 : Total Loss 5.517\n",
      "Epoch 3051 : Total Loss 5.517\n",
      "Epoch 3061 : Total Loss 5.516\n",
      "Epoch 3071 : Total Loss 5.516\n",
      "Epoch 3081 : Total Loss 5.515\n",
      "Epoch 3091 : Total Loss 5.515\n",
      "Epoch 3101 : Total Loss 5.514\n",
      "Epoch 3111 : Total Loss 5.513\n",
      "Epoch 3121 : Total Loss 5.513\n",
      "Epoch 3131 : Total Loss 5.512\n",
      "Epoch 3141 : Total Loss 5.512\n",
      "Epoch 3151 : Total Loss 5.511\n",
      "Epoch 3161 : Total Loss 5.511\n",
      "Epoch 3171 : Total Loss 5.510\n",
      "Epoch 3181 : Total Loss 5.510\n",
      "Epoch 3191 : Total Loss 5.509\n",
      "Epoch 3201 : Total Loss 5.509\n",
      "Epoch 3211 : Total Loss 5.508\n",
      "Epoch 3221 : Total Loss 5.508\n",
      "Epoch 3231 : Total Loss 5.508\n",
      "Epoch 3241 : Total Loss 5.507\n",
      "Epoch 3251 : Total Loss 5.507\n",
      "Epoch 3261 : Total Loss 5.506\n",
      "Epoch 3271 : Total Loss 5.506\n",
      "Epoch 3281 : Total Loss 5.505\n",
      "Epoch 3291 : Total Loss 5.505\n",
      "Epoch 3301 : Total Loss 5.505\n",
      "Epoch 3311 : Total Loss 5.504\n",
      "Epoch 3321 : Total Loss 5.504\n",
      "Epoch 3331 : Total Loss 5.503\n",
      "Epoch 3341 : Total Loss 5.503\n",
      "Epoch 3351 : Total Loss 5.503\n",
      "Epoch 3361 : Total Loss 5.502\n",
      "Epoch 3371 : Total Loss 5.502\n",
      "Epoch 3381 : Total Loss 5.502\n",
      "Epoch 3391 : Total Loss 5.501\n",
      "Epoch 3401 : Total Loss 5.501\n",
      "Epoch 3411 : Total Loss 5.501\n",
      "Epoch 3421 : Total Loss 5.500\n",
      "Epoch 3431 : Total Loss 5.500\n",
      "Epoch 3441 : Total Loss 5.500\n",
      "Epoch 3451 : Total Loss 5.499\n",
      "Epoch 3461 : Total Loss 5.499\n",
      "Epoch 3471 : Total Loss 5.499\n",
      "Epoch 3481 : Total Loss 5.498\n",
      "Epoch 3491 : Total Loss 5.498\n",
      "Epoch 3501 : Total Loss 5.498\n",
      "Epoch 3511 : Total Loss 5.497\n",
      "Epoch 3521 : Total Loss 5.497\n",
      "Epoch 3531 : Total Loss 5.497\n",
      "Epoch 3541 : Total Loss 5.496\n",
      "Epoch 3551 : Total Loss 5.496\n",
      "Epoch 3561 : Total Loss 5.496\n",
      "Epoch 3571 : Total Loss 5.496\n",
      "Epoch 3581 : Total Loss 5.495\n",
      "Epoch 3591 : Total Loss 5.495\n",
      "Epoch 3601 : Total Loss 5.495\n",
      "Epoch 3611 : Total Loss 5.494\n",
      "Epoch 3621 : Total Loss 5.494\n",
      "Epoch 3631 : Total Loss 5.494\n",
      "Epoch 3641 : Total Loss 5.494\n",
      "Epoch 3651 : Total Loss 5.493\n",
      "Epoch 3661 : Total Loss 5.493\n",
      "Epoch 3671 : Total Loss 5.493\n",
      "Epoch 3681 : Total Loss 5.493\n",
      "Epoch 3691 : Total Loss 5.492\n",
      "Epoch 3701 : Total Loss 5.492\n",
      "Epoch 3711 : Total Loss 5.492\n",
      "Epoch 3721 : Total Loss 5.492\n",
      "Epoch 3731 : Total Loss 5.491\n",
      "Epoch 3741 : Total Loss 5.491\n",
      "Epoch 3751 : Total Loss 5.491\n",
      "Epoch 3761 : Total Loss 5.491\n",
      "Epoch 3771 : Total Loss 5.491\n",
      "Epoch 3781 : Total Loss 5.490\n",
      "Epoch 3791 : Total Loss 5.490\n",
      "Epoch 3801 : Total Loss 5.490\n",
      "Epoch 3811 : Total Loss 5.490\n",
      "Epoch 3821 : Total Loss 5.489\n",
      "Epoch 3831 : Total Loss 5.489\n",
      "Epoch 3841 : Total Loss 5.489\n",
      "Epoch 3851 : Total Loss 5.489\n",
      "Epoch 3861 : Total Loss 5.489\n",
      "Epoch 3871 : Total Loss 5.488\n",
      "Epoch 3881 : Total Loss 5.488\n",
      "Epoch 3891 : Total Loss 5.488\n",
      "Epoch 3901 : Total Loss 5.488\n",
      "Epoch 3911 : Total Loss 5.488\n",
      "Epoch 3921 : Total Loss 5.487\n",
      "Epoch 3931 : Total Loss 5.487\n",
      "Epoch 3941 : Total Loss 5.487\n",
      "Epoch 3951 : Total Loss 5.487\n",
      "Epoch 3961 : Total Loss 5.487\n",
      "Epoch 3971 : Total Loss 5.486\n",
      "Epoch 3981 : Total Loss 5.486\n",
      "Epoch 3991 : Total Loss 5.486\n",
      "Epoch 4001 : Total Loss 5.486\n",
      "Epoch 4011 : Total Loss 5.486\n",
      "Epoch 4021 : Total Loss 5.485\n",
      "Epoch 4031 : Total Loss 5.485\n",
      "Epoch 4041 : Total Loss 5.485\n",
      "Epoch 4051 : Total Loss 5.485\n",
      "Epoch 4061 : Total Loss 5.485\n",
      "Epoch 4071 : Total Loss 5.485\n",
      "Epoch 4081 : Total Loss 5.484\n",
      "Epoch 4091 : Total Loss 5.484\n",
      "Epoch 4101 : Total Loss 5.484\n",
      "Epoch 4111 : Total Loss 5.484\n",
      "Epoch 4121 : Total Loss 5.484\n",
      "Epoch 4131 : Total Loss 5.483\n",
      "Epoch 4141 : Total Loss 5.483\n",
      "Epoch 4151 : Total Loss 5.483\n",
      "Epoch 4161 : Total Loss 5.483\n",
      "Epoch 4171 : Total Loss 5.483\n",
      "Epoch 4181 : Total Loss 5.483\n",
      "Epoch 4191 : Total Loss 5.482\n",
      "Epoch 4201 : Total Loss 5.482\n",
      "Epoch 4211 : Total Loss 5.482\n",
      "Epoch 4221 : Total Loss 5.482\n",
      "Epoch 4231 : Total Loss 5.482\n",
      "Epoch 4241 : Total Loss 5.482\n",
      "Epoch 4251 : Total Loss 5.482\n",
      "Epoch 4261 : Total Loss 5.481\n",
      "Epoch 4271 : Total Loss 5.481\n",
      "Epoch 4281 : Total Loss 5.481\n",
      "Epoch 4291 : Total Loss 5.481\n",
      "Epoch 4301 : Total Loss 5.481\n",
      "Epoch 4311 : Total Loss 5.481\n",
      "Epoch 4321 : Total Loss 5.480\n",
      "Epoch 4331 : Total Loss 5.480\n",
      "Epoch 4341 : Total Loss 5.480\n",
      "Epoch 4351 : Total Loss 5.480\n",
      "Epoch 4361 : Total Loss 5.480\n",
      "Epoch 4371 : Total Loss 5.480\n",
      "Epoch 4381 : Total Loss 5.479\n",
      "Epoch 4391 : Total Loss 5.479\n",
      "Epoch 4401 : Total Loss 5.479\n",
      "Epoch 4411 : Total Loss 5.479\n",
      "Epoch 4421 : Total Loss 5.479\n",
      "Epoch 4431 : Total Loss 5.479\n",
      "Epoch 4441 : Total Loss 5.479\n",
      "Epoch 4451 : Total Loss 5.478\n",
      "Epoch 4461 : Total Loss 5.478\n",
      "Epoch 4471 : Total Loss 5.478\n",
      "Epoch 4481 : Total Loss 5.478\n",
      "Epoch 4491 : Total Loss 5.478\n",
      "Epoch 4501 : Total Loss 5.478\n",
      "Epoch 4511 : Total Loss 5.478\n",
      "Epoch 4521 : Total Loss 5.477\n",
      "Epoch 4531 : Total Loss 5.477\n",
      "Epoch 4541 : Total Loss 5.477\n",
      "Epoch 4551 : Total Loss 5.477\n",
      "Epoch 4561 : Total Loss 5.477\n",
      "Epoch 4571 : Total Loss 5.477\n",
      "Epoch 4581 : Total Loss 5.477\n",
      "Epoch 4591 : Total Loss 5.476\n",
      "Epoch 4601 : Total Loss 5.476\n",
      "Epoch 4611 : Total Loss 5.476\n",
      "Epoch 4621 : Total Loss 5.476\n",
      "Epoch 4631 : Total Loss 5.476\n",
      "Epoch 4641 : Total Loss 5.476\n",
      "Epoch 4651 : Total Loss 5.476\n",
      "Epoch 4661 : Total Loss 5.475\n",
      "Epoch 4671 : Total Loss 5.475\n",
      "Epoch 4681 : Total Loss 5.475\n",
      "Epoch 4691 : Total Loss 5.475\n",
      "Epoch 4701 : Total Loss 5.475\n",
      "Epoch 4711 : Total Loss 5.475\n",
      "Epoch 4721 : Total Loss 5.475\n",
      "Epoch 4731 : Total Loss 5.474\n",
      "Epoch 4741 : Total Loss 5.474\n",
      "Epoch 4751 : Total Loss 5.474\n",
      "Epoch 4761 : Total Loss 5.474\n",
      "Epoch 4771 : Total Loss 5.474\n",
      "Epoch 4781 : Total Loss 5.474\n",
      "Epoch 4791 : Total Loss 5.474\n",
      "Epoch 4801 : Total Loss 5.473\n",
      "Epoch 4811 : Total Loss 5.473\n",
      "Epoch 4821 : Total Loss 5.473\n",
      "Epoch 4831 : Total Loss 5.473\n",
      "Epoch 4841 : Total Loss 5.473\n",
      "Epoch 4851 : Total Loss 5.473\n",
      "Epoch 4861 : Total Loss 5.473\n",
      "Epoch 4871 : Total Loss 5.473\n",
      "Epoch 4881 : Total Loss 5.472\n",
      "Epoch 4891 : Total Loss 5.472\n",
      "Epoch 4901 : Total Loss 5.472\n",
      "Epoch 4911 : Total Loss 5.472\n",
      "Epoch 4921 : Total Loss 5.472\n",
      "Epoch 4931 : Total Loss 5.472\n",
      "Epoch 4941 : Total Loss 5.472\n",
      "Epoch 4951 : Total Loss 5.471\n",
      "Epoch 4961 : Total Loss 5.471\n",
      "Epoch 4971 : Total Loss 5.471\n",
      "Epoch 4981 : Total Loss 5.471\n",
      "Epoch 4991 : Total Loss 5.471\n",
      "A_hat is:\n",
      " [[4.98447014 2.97009182 1.54598166 1.50219606 0.9902487 ]\n",
      " [2.99605858 2.12761136 3.17161116 2.87072417 0.95654322]\n",
      " [3.98158649 2.34293891 1.04127825 1.03001406 0.75980498]\n",
      " [4.52329661 2.9474781  3.05471784 2.81272953 1.16479036]\n",
      " [4.96573884 3.01061644 1.87870719 1.79363369 1.0410779 ]]\n"
     ]
    }
   ],
   "source": [
    "U_hat, V_hat = matrix_factorization(A, U, V, user_features, 5000)\n",
    "\n",
    "print('A_hat is:\\n', np.dot(U_hat, V_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefe3a4",
   "metadata": {},
   "source": [
    "After training for long enough, we see that the existing interactions are predicted quite well. To see the average error of each entry in $A_{hat}$ corresponding to NONZEROS in $A$, we can just use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c304c6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.98447014, 2.97009182, 1.54598166, 1.50219606, 0.9902487 ],\n",
       "       [2.99605858, 2.12761136, 3.17161116, 2.87072417, 0.95654322],\n",
       "       [3.98158649, 2.34293891, 1.04127825, 1.03001406, 0.75980498],\n",
       "       [4.52329661, 2.9474781 , 3.05471784, 2.81272953, 1.16479036],\n",
       "       [4.96573884, 3.01061644, 1.87870719, 1.79363369, 1.0410779 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_hat = np.dot(U_hat, V_hat.T)\n",
    "A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a91f831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 2, 1, 0],\n",
       "       [3, 0, 4, 2, 1],\n",
       "       [4, 0, 0, 1, 0],\n",
       "       [0, 0, 2, 4, 0],\n",
       "       [5, 3, 0, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de63daeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 0),\n",
       " (2, 3),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find nonzero indices\n",
    "nonzero_ind = [(i,j) for i in range(A.shape[0]) for j in range(A.shape[1]) if A[i][j] > 0]\n",
    "\n",
    "nonzero_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ff1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of all predictions is 0.295.\n",
      "The root mean squared error of all predictions is 0.544.\n"
     ]
    }
   ],
   "source": [
    "# compute average error for all entries in A_hat corresponding to nonzeros in A\n",
    "avg_error = sum([(A_hat[i][j] - A[i][j])**2 for i, j in nonzero_ind])/len(nonzero_ind)\n",
    "\n",
    "print(f'The mean squared error of all predictions is {avg_error:.3f}.')\n",
    "print(f'The root mean squared error of all predictions is {np.sqrt(avg_error):.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381c12d",
   "metadata": {},
   "source": [
    "Therefore, our matrix factorization algorithm is very good at predicting the existing entries. Let us see what the predictions were for the empty entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d85c4066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 4),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 4),\n",
       " (4, 2),\n",
       " (4, 3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find zero indices of A\n",
    "zero_ind = [(i,j) for i in range(A.shape[0]) for j in range(A.shape[1]) if A[i][j] == 0]\n",
    "\n",
    "zero_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcde43c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.990248697667072, 'At index:(0, 4)'],\n",
       " [2.1276113626791973, 'At index:(1, 1)'],\n",
       " [2.3429389126641804, 'At index:(2, 1)'],\n",
       " [1.0412782499755882, 'At index:(2, 2)'],\n",
       " [0.7598049813295886, 'At index:(2, 4)'],\n",
       " [4.523296605425644, 'At index:(3, 0)'],\n",
       " [2.947478096798725, 'At index:(3, 1)'],\n",
       " [1.1647903631780367, 'At index:(3, 4)'],\n",
       " [1.8787071873760717, 'At index:(4, 2)'],\n",
       " [1.793633687301854, 'At index:(4, 3)']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [[A_hat[i][j], f'At index:{(i,j)}'] for i,j in zero_ind]\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a54036",
   "metadata": {},
   "source": [
    "To take it one step further, we could even round these up or down in order to give a good prediction for what these interactions *would have* been rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103910d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'At index:(0, 4)'],\n",
       " [2, 'At index:(1, 1)'],\n",
       " [2, 'At index:(2, 1)'],\n",
       " [1, 'At index:(2, 2)'],\n",
       " [1, 'At index:(2, 4)'],\n",
       " [5, 'At index:(3, 0)'],\n",
       " [3, 'At index:(3, 1)'],\n",
       " [1, 'At index:(3, 4)'],\n",
       " [2, 'At index:(4, 2)'],\n",
       " [2, 'At index:(4, 3)']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_predictions = [[int(np.round(A_hat[i][j], decimals=0)), f'At index:{(i,j)}'] for i,j in zero_ind]\n",
    "\n",
    "rounded_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b186a",
   "metadata": {},
   "source": [
    "**Non-Square Matrix Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad1eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.37454012 0.95071431]\n",
      " [0.73199394 0.59865848]\n",
      " [0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615]]\n",
      "V is:\n",
      " [[0.60111501 0.70807258]\n",
      " [0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911]\n",
      " [0.18182497 0.18340451]]\n",
      "Epoch 1 : Total Loss 70.905\n",
      "Epoch 11 : Total Loss 69.765\n",
      "Epoch 21 : Total Loss 68.610\n",
      "Epoch 31 : Total Loss 67.441\n",
      "Epoch 41 : Total Loss 66.260\n",
      "Epoch 51 : Total Loss 65.068\n",
      "Epoch 61 : Total Loss 63.867\n",
      "Epoch 71 : Total Loss 62.659\n",
      "Epoch 81 : Total Loss 61.447\n",
      "Epoch 91 : Total Loss 60.231\n",
      "Epoch 101 : Total Loss 59.014\n",
      "Epoch 111 : Total Loss 57.799\n",
      "Epoch 121 : Total Loss 56.587\n",
      "Epoch 131 : Total Loss 55.380\n",
      "Epoch 141 : Total Loss 54.182\n",
      "Epoch 151 : Total Loss 52.994\n",
      "Epoch 161 : Total Loss 51.819\n",
      "Epoch 171 : Total Loss 50.657\n",
      "Epoch 181 : Total Loss 49.513\n",
      "Epoch 191 : Total Loss 48.386\n",
      "Epoch 201 : Total Loss 47.280\n",
      "Epoch 211 : Total Loss 46.195\n",
      "Epoch 221 : Total Loss 45.134\n",
      "Epoch 231 : Total Loss 44.098\n",
      "Epoch 241 : Total Loss 43.087\n",
      "Epoch 251 : Total Loss 42.103\n",
      "Epoch 261 : Total Loss 41.147\n",
      "Epoch 271 : Total Loss 40.219\n",
      "Epoch 281 : Total Loss 39.320\n",
      "Epoch 291 : Total Loss 38.450\n",
      "Epoch 301 : Total Loss 37.609\n",
      "Epoch 311 : Total Loss 36.798\n",
      "Epoch 321 : Total Loss 36.016\n",
      "Epoch 331 : Total Loss 35.263\n",
      "Epoch 341 : Total Loss 34.538\n",
      "Epoch 351 : Total Loss 33.841\n",
      "Epoch 361 : Total Loss 33.172\n",
      "Epoch 371 : Total Loss 32.529\n",
      "Epoch 381 : Total Loss 31.912\n",
      "Epoch 391 : Total Loss 31.321\n",
      "Epoch 401 : Total Loss 30.753\n",
      "Epoch 411 : Total Loss 30.209\n",
      "Epoch 421 : Total Loss 29.688\n",
      "Epoch 431 : Total Loss 29.188\n",
      "Epoch 441 : Total Loss 28.708\n",
      "Epoch 451 : Total Loss 28.248\n",
      "Epoch 461 : Total Loss 27.806\n",
      "Epoch 471 : Total Loss 27.383\n",
      "Epoch 481 : Total Loss 26.976\n",
      "Epoch 491 : Total Loss 26.585\n",
      "Epoch 501 : Total Loss 26.209\n",
      "Epoch 511 : Total Loss 25.847\n",
      "Epoch 521 : Total Loss 25.499\n",
      "Epoch 531 : Total Loss 25.164\n",
      "Epoch 541 : Total Loss 24.840\n",
      "Epoch 551 : Total Loss 24.529\n",
      "Epoch 561 : Total Loss 24.227\n",
      "Epoch 571 : Total Loss 23.936\n",
      "Epoch 581 : Total Loss 23.655\n",
      "Epoch 591 : Total Loss 23.383\n",
      "Epoch 601 : Total Loss 23.119\n",
      "Epoch 611 : Total Loss 22.864\n",
      "Epoch 621 : Total Loss 22.616\n",
      "Epoch 631 : Total Loss 22.375\n",
      "Epoch 641 : Total Loss 22.142\n",
      "Epoch 651 : Total Loss 21.915\n",
      "Epoch 661 : Total Loss 21.694\n",
      "Epoch 671 : Total Loss 21.479\n",
      "Epoch 681 : Total Loss 21.270\n",
      "Epoch 691 : Total Loss 21.066\n",
      "Epoch 701 : Total Loss 20.867\n",
      "Epoch 711 : Total Loss 20.673\n",
      "Epoch 721 : Total Loss 20.484\n",
      "Epoch 731 : Total Loss 20.299\n",
      "Epoch 741 : Total Loss 20.118\n",
      "Epoch 751 : Total Loss 19.941\n",
      "Epoch 761 : Total Loss 19.768\n",
      "Epoch 771 : Total Loss 19.599\n",
      "Epoch 781 : Total Loss 19.433\n",
      "Epoch 791 : Total Loss 19.271\n",
      "Epoch 801 : Total Loss 19.111\n",
      "Epoch 811 : Total Loss 18.955\n",
      "Epoch 821 : Total Loss 18.802\n",
      "Epoch 831 : Total Loss 18.651\n",
      "Epoch 841 : Total Loss 18.503\n",
      "Epoch 851 : Total Loss 18.358\n",
      "Epoch 861 : Total Loss 18.214\n",
      "Epoch 871 : Total Loss 18.074\n",
      "Epoch 881 : Total Loss 17.935\n",
      "Epoch 891 : Total Loss 17.798\n",
      "Epoch 901 : Total Loss 17.663\n",
      "Epoch 911 : Total Loss 17.530\n",
      "Epoch 921 : Total Loss 17.398\n",
      "Epoch 931 : Total Loss 17.268\n",
      "Epoch 941 : Total Loss 17.140\n",
      "Epoch 951 : Total Loss 17.013\n",
      "Epoch 961 : Total Loss 16.887\n",
      "Epoch 971 : Total Loss 16.763\n",
      "Epoch 981 : Total Loss 16.639\n",
      "Epoch 991 : Total Loss 16.516\n",
      "Epoch 1001 : Total Loss 16.395\n",
      "Epoch 1011 : Total Loss 16.274\n",
      "Epoch 1021 : Total Loss 16.154\n",
      "Epoch 1031 : Total Loss 16.035\n",
      "Epoch 1041 : Total Loss 15.916\n",
      "Epoch 1051 : Total Loss 15.797\n",
      "Epoch 1061 : Total Loss 15.680\n",
      "Epoch 1071 : Total Loss 15.562\n",
      "Epoch 1081 : Total Loss 15.445\n",
      "Epoch 1091 : Total Loss 15.328\n",
      "Epoch 1101 : Total Loss 15.211\n",
      "Epoch 1111 : Total Loss 15.094\n",
      "Epoch 1121 : Total Loss 14.978\n",
      "Epoch 1131 : Total Loss 14.861\n",
      "Epoch 1141 : Total Loss 14.744\n",
      "Epoch 1151 : Total Loss 14.628\n",
      "Epoch 1161 : Total Loss 14.511\n",
      "Epoch 1171 : Total Loss 14.394\n",
      "Epoch 1181 : Total Loss 14.276\n",
      "Epoch 1191 : Total Loss 14.159\n",
      "Epoch 1201 : Total Loss 14.041\n",
      "Epoch 1211 : Total Loss 13.922\n",
      "Epoch 1221 : Total Loss 13.804\n",
      "Epoch 1231 : Total Loss 13.685\n",
      "Epoch 1241 : Total Loss 13.565\n",
      "Epoch 1251 : Total Loss 13.446\n",
      "Epoch 1261 : Total Loss 13.325\n",
      "Epoch 1271 : Total Loss 13.205\n",
      "Epoch 1281 : Total Loss 13.083\n",
      "Epoch 1291 : Total Loss 12.962\n",
      "Epoch 1301 : Total Loss 12.840\n",
      "Epoch 1311 : Total Loss 12.717\n",
      "Epoch 1321 : Total Loss 12.594\n",
      "Epoch 1331 : Total Loss 12.470\n",
      "Epoch 1341 : Total Loss 12.346\n",
      "Epoch 1351 : Total Loss 12.221\n",
      "Epoch 1361 : Total Loss 12.096\n",
      "Epoch 1371 : Total Loss 11.971\n",
      "Epoch 1381 : Total Loss 11.845\n",
      "Epoch 1391 : Total Loss 11.719\n",
      "Epoch 1401 : Total Loss 11.592\n",
      "Epoch 1411 : Total Loss 11.465\n",
      "Epoch 1421 : Total Loss 11.338\n",
      "Epoch 1431 : Total Loss 11.211\n",
      "Epoch 1441 : Total Loss 11.083\n",
      "Epoch 1451 : Total Loss 10.955\n",
      "Epoch 1461 : Total Loss 10.826\n",
      "Epoch 1471 : Total Loss 10.698\n",
      "Epoch 1481 : Total Loss 10.570\n",
      "Epoch 1491 : Total Loss 10.441\n",
      "Epoch 1501 : Total Loss 10.313\n",
      "Epoch 1511 : Total Loss 10.184\n",
      "Epoch 1521 : Total Loss 10.056\n",
      "Epoch 1531 : Total Loss 9.927\n",
      "Epoch 1541 : Total Loss 9.799\n",
      "Epoch 1551 : Total Loss 9.671\n",
      "Epoch 1561 : Total Loss 9.544\n",
      "Epoch 1571 : Total Loss 9.417\n",
      "Epoch 1581 : Total Loss 9.290\n",
      "Epoch 1591 : Total Loss 9.164\n",
      "Epoch 1601 : Total Loss 9.038\n",
      "Epoch 1611 : Total Loss 8.913\n",
      "Epoch 1621 : Total Loss 8.788\n",
      "Epoch 1631 : Total Loss 8.664\n",
      "Epoch 1641 : Total Loss 8.541\n",
      "Epoch 1651 : Total Loss 8.419\n",
      "Epoch 1661 : Total Loss 8.297\n",
      "Epoch 1671 : Total Loss 8.177\n",
      "Epoch 1681 : Total Loss 8.057\n",
      "Epoch 1691 : Total Loss 7.939\n",
      "Epoch 1701 : Total Loss 7.821\n",
      "Epoch 1711 : Total Loss 7.705\n",
      "Epoch 1721 : Total Loss 7.590\n",
      "Epoch 1731 : Total Loss 7.476\n",
      "Epoch 1741 : Total Loss 7.363\n",
      "Epoch 1751 : Total Loss 7.252\n",
      "Epoch 1761 : Total Loss 7.142\n",
      "Epoch 1771 : Total Loss 7.033\n",
      "Epoch 1781 : Total Loss 6.926\n",
      "Epoch 1791 : Total Loss 6.820\n",
      "Epoch 1801 : Total Loss 6.716\n",
      "Epoch 1811 : Total Loss 6.614\n",
      "Epoch 1821 : Total Loss 6.513\n",
      "Epoch 1831 : Total Loss 6.414\n",
      "Epoch 1841 : Total Loss 6.316\n",
      "Epoch 1851 : Total Loss 6.220\n",
      "Epoch 1861 : Total Loss 6.126\n",
      "Epoch 1871 : Total Loss 6.033\n",
      "Epoch 1881 : Total Loss 5.942\n",
      "Epoch 1891 : Total Loss 5.853\n",
      "Epoch 1901 : Total Loss 5.766\n",
      "Epoch 1911 : Total Loss 5.680\n",
      "Epoch 1921 : Total Loss 5.597\n",
      "Epoch 1931 : Total Loss 5.515\n",
      "Epoch 1941 : Total Loss 5.435\n",
      "Epoch 1951 : Total Loss 5.356\n",
      "Epoch 1961 : Total Loss 5.280\n",
      "Epoch 1971 : Total Loss 5.205\n",
      "Epoch 1981 : Total Loss 5.132\n",
      "Epoch 1991 : Total Loss 5.061\n",
      "Epoch 2001 : Total Loss 4.991\n",
      "Epoch 2011 : Total Loss 4.923\n",
      "Epoch 2021 : Total Loss 4.857\n",
      "Epoch 2031 : Total Loss 4.793\n",
      "Epoch 2041 : Total Loss 4.731\n",
      "Epoch 2051 : Total Loss 4.670\n",
      "Epoch 2061 : Total Loss 4.610\n",
      "Epoch 2071 : Total Loss 4.553\n",
      "Epoch 2081 : Total Loss 4.497\n",
      "Epoch 2091 : Total Loss 4.443\n",
      "Epoch 2101 : Total Loss 4.390\n",
      "Epoch 2111 : Total Loss 4.339\n",
      "Epoch 2121 : Total Loss 4.289\n",
      "Epoch 2131 : Total Loss 4.241\n",
      "Epoch 2141 : Total Loss 4.194\n",
      "Epoch 2151 : Total Loss 4.149\n",
      "Epoch 2161 : Total Loss 4.105\n",
      "Epoch 2171 : Total Loss 4.063\n",
      "Epoch 2181 : Total Loss 4.022\n",
      "Epoch 2191 : Total Loss 3.982\n",
      "Epoch 2201 : Total Loss 3.944\n",
      "Epoch 2211 : Total Loss 3.906\n",
      "Epoch 2221 : Total Loss 3.871\n",
      "Epoch 2231 : Total Loss 3.836\n",
      "Epoch 2241 : Total Loss 3.802\n",
      "Epoch 2251 : Total Loss 3.770\n",
      "Epoch 2261 : Total Loss 3.738\n",
      "Epoch 2271 : Total Loss 3.708\n",
      "Epoch 2281 : Total Loss 3.679\n",
      "Epoch 2291 : Total Loss 3.651\n",
      "Epoch 2301 : Total Loss 3.624\n",
      "Epoch 2311 : Total Loss 3.597\n",
      "Epoch 2321 : Total Loss 3.572\n",
      "Epoch 2331 : Total Loss 3.548\n",
      "Epoch 2341 : Total Loss 3.524\n",
      "Epoch 2351 : Total Loss 3.502\n",
      "Epoch 2361 : Total Loss 3.480\n",
      "Epoch 2371 : Total Loss 3.459\n",
      "Epoch 2381 : Total Loss 3.438\n",
      "Epoch 2391 : Total Loss 3.419\n",
      "Epoch 2401 : Total Loss 3.400\n",
      "Epoch 2411 : Total Loss 3.382\n",
      "Epoch 2421 : Total Loss 3.364\n",
      "Epoch 2431 : Total Loss 3.347\n",
      "Epoch 2441 : Total Loss 3.331\n",
      "Epoch 2451 : Total Loss 3.315\n",
      "Epoch 2461 : Total Loss 3.300\n",
      "Epoch 2471 : Total Loss 3.286\n",
      "Epoch 2481 : Total Loss 3.272\n",
      "Epoch 2491 : Total Loss 3.258\n",
      "Epoch 2501 : Total Loss 3.245\n",
      "Epoch 2511 : Total Loss 3.233\n",
      "Epoch 2521 : Total Loss 3.221\n",
      "Epoch 2531 : Total Loss 3.209\n",
      "Epoch 2541 : Total Loss 3.198\n",
      "Epoch 2551 : Total Loss 3.187\n",
      "Epoch 2561 : Total Loss 3.177\n",
      "Epoch 2571 : Total Loss 3.167\n",
      "Epoch 2581 : Total Loss 3.157\n",
      "Epoch 2591 : Total Loss 3.148\n",
      "Epoch 2601 : Total Loss 3.139\n",
      "Epoch 2611 : Total Loss 3.130\n",
      "Epoch 2621 : Total Loss 3.122\n",
      "Epoch 2631 : Total Loss 3.114\n",
      "Epoch 2641 : Total Loss 3.106\n",
      "Epoch 2651 : Total Loss 3.099\n",
      "Epoch 2661 : Total Loss 3.091\n",
      "Epoch 2671 : Total Loss 3.084\n",
      "Epoch 2681 : Total Loss 3.078\n",
      "Epoch 2691 : Total Loss 3.071\n",
      "Epoch 2701 : Total Loss 3.065\n",
      "Epoch 2711 : Total Loss 3.059\n",
      "Epoch 2721 : Total Loss 3.053\n",
      "Epoch 2731 : Total Loss 3.047\n",
      "Epoch 2741 : Total Loss 3.041\n",
      "Epoch 2751 : Total Loss 3.036\n",
      "Epoch 2761 : Total Loss 3.031\n",
      "Epoch 2771 : Total Loss 3.026\n",
      "Epoch 2781 : Total Loss 3.021\n",
      "Epoch 2791 : Total Loss 3.016\n",
      "Epoch 2801 : Total Loss 3.011\n",
      "Epoch 2811 : Total Loss 3.007\n",
      "Epoch 2821 : Total Loss 3.002\n",
      "Epoch 2831 : Total Loss 2.998\n",
      "Epoch 2841 : Total Loss 2.994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2851 : Total Loss 2.990\n",
      "Epoch 2861 : Total Loss 2.986\n",
      "Epoch 2871 : Total Loss 2.982\n",
      "Epoch 2881 : Total Loss 2.979\n",
      "Epoch 2891 : Total Loss 2.975\n",
      "Epoch 2901 : Total Loss 2.972\n",
      "Epoch 2911 : Total Loss 2.968\n",
      "Epoch 2921 : Total Loss 2.965\n",
      "Epoch 2931 : Total Loss 2.961\n",
      "Epoch 2941 : Total Loss 2.958\n",
      "Epoch 2951 : Total Loss 2.955\n",
      "Epoch 2961 : Total Loss 2.952\n",
      "Epoch 2971 : Total Loss 2.949\n",
      "Epoch 2981 : Total Loss 2.946\n",
      "Epoch 2991 : Total Loss 2.943\n",
      "Epoch 3001 : Total Loss 2.940\n",
      "Epoch 3011 : Total Loss 2.938\n",
      "Epoch 3021 : Total Loss 2.935\n",
      "Epoch 3031 : Total Loss 2.932\n",
      "Epoch 3041 : Total Loss 2.930\n",
      "Epoch 3051 : Total Loss 2.927\n",
      "Epoch 3061 : Total Loss 2.925\n",
      "Epoch 3071 : Total Loss 2.922\n",
      "Epoch 3081 : Total Loss 2.920\n",
      "Epoch 3091 : Total Loss 2.917\n",
      "Epoch 3101 : Total Loss 2.915\n",
      "Epoch 3111 : Total Loss 2.912\n",
      "Epoch 3121 : Total Loss 2.910\n",
      "Epoch 3131 : Total Loss 2.908\n",
      "Epoch 3141 : Total Loss 2.906\n",
      "Epoch 3151 : Total Loss 2.903\n",
      "Epoch 3161 : Total Loss 2.901\n",
      "Epoch 3171 : Total Loss 2.899\n",
      "Epoch 3181 : Total Loss 2.897\n",
      "Epoch 3191 : Total Loss 2.895\n",
      "Epoch 3201 : Total Loss 2.893\n",
      "Epoch 3211 : Total Loss 2.891\n",
      "Epoch 3221 : Total Loss 2.888\n",
      "Epoch 3231 : Total Loss 2.886\n",
      "Epoch 3241 : Total Loss 2.884\n",
      "Epoch 3251 : Total Loss 2.882\n",
      "Epoch 3261 : Total Loss 2.880\n",
      "Epoch 3271 : Total Loss 2.878\n",
      "Epoch 3281 : Total Loss 2.877\n",
      "Epoch 3291 : Total Loss 2.875\n",
      "Epoch 3301 : Total Loss 2.873\n",
      "Epoch 3311 : Total Loss 2.871\n",
      "Epoch 3321 : Total Loss 2.869\n",
      "Epoch 3331 : Total Loss 2.867\n",
      "Epoch 3341 : Total Loss 2.865\n",
      "Epoch 3351 : Total Loss 2.863\n",
      "Epoch 3361 : Total Loss 2.861\n",
      "Epoch 3371 : Total Loss 2.859\n",
      "Epoch 3381 : Total Loss 2.858\n",
      "Epoch 3391 : Total Loss 2.856\n",
      "Epoch 3401 : Total Loss 2.854\n",
      "Epoch 3411 : Total Loss 2.852\n",
      "Epoch 3421 : Total Loss 2.850\n",
      "Epoch 3431 : Total Loss 2.849\n",
      "Epoch 3441 : Total Loss 2.847\n",
      "Epoch 3451 : Total Loss 2.845\n",
      "Epoch 3461 : Total Loss 2.843\n",
      "Epoch 3471 : Total Loss 2.841\n",
      "Epoch 3481 : Total Loss 2.840\n",
      "Epoch 3491 : Total Loss 2.838\n",
      "Epoch 3501 : Total Loss 2.836\n",
      "Epoch 3511 : Total Loss 2.834\n",
      "Epoch 3521 : Total Loss 2.833\n",
      "Epoch 3531 : Total Loss 2.831\n",
      "Epoch 3541 : Total Loss 2.829\n",
      "Epoch 3551 : Total Loss 2.828\n",
      "Epoch 3561 : Total Loss 2.826\n",
      "Epoch 3571 : Total Loss 2.824\n",
      "Epoch 3581 : Total Loss 2.822\n",
      "Epoch 3591 : Total Loss 2.821\n",
      "Epoch 3601 : Total Loss 2.819\n",
      "Epoch 3611 : Total Loss 2.817\n",
      "Epoch 3621 : Total Loss 2.815\n",
      "Epoch 3631 : Total Loss 2.814\n",
      "Epoch 3641 : Total Loss 2.812\n",
      "Epoch 3651 : Total Loss 2.810\n",
      "Epoch 3661 : Total Loss 2.809\n",
      "Epoch 3671 : Total Loss 2.807\n",
      "Epoch 3681 : Total Loss 2.805\n",
      "Epoch 3691 : Total Loss 2.804\n",
      "Epoch 3701 : Total Loss 2.802\n",
      "Epoch 3711 : Total Loss 2.800\n",
      "Epoch 3721 : Total Loss 2.798\n",
      "Epoch 3731 : Total Loss 2.797\n",
      "Epoch 3741 : Total Loss 2.795\n",
      "Epoch 3751 : Total Loss 2.793\n",
      "Epoch 3761 : Total Loss 2.792\n",
      "Epoch 3771 : Total Loss 2.790\n",
      "Epoch 3781 : Total Loss 2.788\n",
      "Epoch 3791 : Total Loss 2.787\n",
      "Epoch 3801 : Total Loss 2.785\n",
      "Epoch 3811 : Total Loss 2.783\n",
      "Epoch 3821 : Total Loss 2.782\n",
      "Epoch 3831 : Total Loss 2.780\n",
      "Epoch 3841 : Total Loss 2.778\n",
      "Epoch 3851 : Total Loss 2.776\n",
      "Epoch 3861 : Total Loss 2.775\n",
      "Epoch 3871 : Total Loss 2.773\n",
      "Epoch 3881 : Total Loss 2.771\n",
      "Epoch 3891 : Total Loss 2.770\n",
      "Epoch 3901 : Total Loss 2.768\n",
      "Epoch 3911 : Total Loss 2.766\n",
      "Epoch 3921 : Total Loss 2.765\n",
      "Epoch 3931 : Total Loss 2.763\n",
      "Epoch 3941 : Total Loss 2.761\n",
      "Epoch 3951 : Total Loss 2.760\n",
      "Epoch 3961 : Total Loss 2.758\n",
      "Epoch 3971 : Total Loss 2.756\n",
      "Epoch 3981 : Total Loss 2.754\n",
      "Epoch 3991 : Total Loss 2.753\n",
      "Epoch 4001 : Total Loss 2.751\n",
      "Epoch 4011 : Total Loss 2.749\n",
      "Epoch 4021 : Total Loss 2.748\n",
      "Epoch 4031 : Total Loss 2.746\n",
      "Epoch 4041 : Total Loss 2.744\n",
      "Epoch 4051 : Total Loss 2.743\n",
      "Epoch 4061 : Total Loss 2.741\n",
      "Epoch 4071 : Total Loss 2.739\n",
      "Epoch 4081 : Total Loss 2.737\n",
      "Epoch 4091 : Total Loss 2.736\n",
      "Epoch 4101 : Total Loss 2.734\n",
      "Epoch 4111 : Total Loss 2.732\n",
      "Epoch 4121 : Total Loss 2.731\n",
      "Epoch 4131 : Total Loss 2.729\n",
      "Epoch 4141 : Total Loss 2.727\n",
      "Epoch 4151 : Total Loss 2.725\n",
      "Epoch 4161 : Total Loss 2.724\n",
      "Epoch 4171 : Total Loss 2.722\n",
      "Epoch 4181 : Total Loss 2.720\n",
      "Epoch 4191 : Total Loss 2.719\n",
      "Epoch 4201 : Total Loss 2.717\n",
      "Epoch 4211 : Total Loss 2.715\n",
      "Epoch 4221 : Total Loss 2.713\n",
      "Epoch 4231 : Total Loss 2.712\n",
      "Epoch 4241 : Total Loss 2.710\n",
      "Epoch 4251 : Total Loss 2.708\n",
      "Epoch 4261 : Total Loss 2.706\n",
      "Epoch 4271 : Total Loss 2.705\n",
      "Epoch 4281 : Total Loss 2.703\n",
      "Epoch 4291 : Total Loss 2.701\n",
      "Epoch 4301 : Total Loss 2.699\n",
      "Epoch 4311 : Total Loss 2.698\n",
      "Epoch 4321 : Total Loss 2.696\n",
      "Epoch 4331 : Total Loss 2.694\n",
      "Epoch 4341 : Total Loss 2.692\n",
      "Epoch 4351 : Total Loss 2.691\n",
      "Epoch 4361 : Total Loss 2.689\n",
      "Epoch 4371 : Total Loss 2.687\n",
      "Epoch 4381 : Total Loss 2.685\n",
      "Epoch 4391 : Total Loss 2.684\n",
      "Epoch 4401 : Total Loss 2.682\n",
      "Epoch 4411 : Total Loss 2.680\n",
      "Epoch 4421 : Total Loss 2.678\n",
      "Epoch 4431 : Total Loss 2.677\n",
      "Epoch 4441 : Total Loss 2.675\n",
      "Epoch 4451 : Total Loss 2.673\n",
      "Epoch 4461 : Total Loss 2.671\n",
      "Epoch 4471 : Total Loss 2.669\n",
      "Epoch 4481 : Total Loss 2.668\n",
      "Epoch 4491 : Total Loss 2.666\n",
      "Epoch 4501 : Total Loss 2.664\n",
      "Epoch 4511 : Total Loss 2.662\n",
      "Epoch 4521 : Total Loss 2.660\n",
      "Epoch 4531 : Total Loss 2.659\n",
      "Epoch 4541 : Total Loss 2.657\n",
      "Epoch 4551 : Total Loss 2.655\n",
      "Epoch 4561 : Total Loss 2.653\n",
      "Epoch 4571 : Total Loss 2.651\n",
      "Epoch 4581 : Total Loss 2.650\n",
      "Epoch 4591 : Total Loss 2.648\n",
      "Epoch 4601 : Total Loss 2.646\n",
      "Epoch 4611 : Total Loss 2.644\n",
      "Epoch 4621 : Total Loss 2.642\n",
      "Epoch 4631 : Total Loss 2.641\n",
      "Epoch 4641 : Total Loss 2.639\n",
      "Epoch 4651 : Total Loss 2.637\n",
      "Epoch 4661 : Total Loss 2.635\n",
      "Epoch 4671 : Total Loss 2.633\n",
      "Epoch 4681 : Total Loss 2.631\n",
      "Epoch 4691 : Total Loss 2.630\n",
      "Epoch 4701 : Total Loss 2.628\n",
      "Epoch 4711 : Total Loss 2.626\n",
      "Epoch 4721 : Total Loss 2.624\n",
      "Epoch 4731 : Total Loss 2.622\n",
      "Epoch 4741 : Total Loss 2.620\n",
      "Epoch 4751 : Total Loss 2.619\n",
      "Epoch 4761 : Total Loss 2.617\n",
      "Epoch 4771 : Total Loss 2.615\n",
      "Epoch 4781 : Total Loss 2.613\n",
      "Epoch 4791 : Total Loss 2.611\n",
      "Epoch 4801 : Total Loss 2.609\n",
      "Epoch 4811 : Total Loss 2.607\n",
      "Epoch 4821 : Total Loss 2.606\n",
      "Epoch 4831 : Total Loss 2.604\n",
      "Epoch 4841 : Total Loss 2.602\n",
      "Epoch 4851 : Total Loss 2.600\n",
      "Epoch 4861 : Total Loss 2.598\n",
      "Epoch 4871 : Total Loss 2.596\n",
      "Epoch 4881 : Total Loss 2.594\n",
      "Epoch 4891 : Total Loss 2.592\n",
      "Epoch 4901 : Total Loss 2.591\n",
      "Epoch 4911 : Total Loss 2.589\n",
      "Epoch 4921 : Total Loss 2.587\n",
      "Epoch 4931 : Total Loss 2.585\n",
      "Epoch 4941 : Total Loss 2.583\n",
      "Epoch 4951 : Total Loss 2.581\n",
      "Epoch 4961 : Total Loss 2.579\n",
      "Epoch 4971 : Total Loss 2.577\n",
      "Epoch 4981 : Total Loss 2.575\n",
      "Epoch 4991 : Total Loss 2.573\n",
      "U_hat is:\n",
      " [[ 2.05048103  1.25238602]\n",
      " [ 0.93699003  1.17998147]\n",
      " [ 1.3271836   1.04952428]\n",
      " [-0.74675306  2.26241272]]\n",
      "V_hat is:\n",
      " [[ 1.83237981  1.0451009 ]\n",
      " [ 0.22721574  1.51001292]\n",
      " [ 2.05903658  1.05193632]\n",
      " [-0.5466826   2.02986403]]\n",
      "A is:\n",
      " [[5 3 0 1]\n",
      " [3 2 0 0]\n",
      " [0 1 4 2]\n",
      " [1 0 0 5]]\n",
      "A_hat is:\n",
      " [[5.06612981 2.35702065 5.53944581 1.42121104]\n",
      " [2.95012131 1.99468616 3.17056212 1.8829658 ]\n",
      " [3.52876319 1.88635223 3.83675229 1.4048434 ]\n",
      " [0.99611434 3.24659839 0.84232225 5.00062711]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [5,3,0,1],\n",
    "    [3,2,0,0],\n",
    "    [0,1,4,2],\n",
    "    [1,0,0,5]\n",
    "])\n",
    "\n",
    "m, n = A.shape[0], A.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "U_1 = np.random.rand(m, user_features)\n",
    "V_1 = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U_1)\n",
    "print('V is:\\n', V_1)\n",
    "\n",
    "# perform factorization\n",
    "\n",
    "U_hat, V_hat = matrix_factorization(A, U_1, V_1, user_features, 5000)\n",
    "\n",
    "A_hat = np.dot(U_hat, V_hat.T)\n",
    "\n",
    "print('U_hat is:\\n', U_hat)\n",
    "print('V_hat is:\\n', V_hat)\n",
    "\n",
    "## check if the product results in something close to A\n",
    "print('A is:\\n', A)\n",
    "print('A_hat is:\\n', A_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52240388",
   "metadata": {},
   "source": [
    "## TensorFlow Implementation of Matrix Factorization (Pending)\n",
    "\n",
    "We can take advantage of TensorFlow's automatic differentation to perform this for us. \n",
    "\n",
    "One major step that was a weakness for us in the previous implementation was that **we needed to solve for the gradient of each entry of the error matrix by hand**. While this is easily the most computationally cost-effective way to do this, we would like to compute a number for the gradient as we go instead of obtaining the closed-form expression and then plugging our values in.\n",
    "\n",
    "(**Not Done Yet**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32721420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dee05a",
   "metadata": {},
   "source": [
    "We will now be implementing this from scratch on tensorflow. One reason why optimizing to run this in tensorflow would be more effective is that we can take advantage of **graph execution**. If we can somehow get the error computation in a graph, it would be extremely quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f501fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.9600173 , 0.69951205],\n",
       "        [0.99986729, 0.2200673 ],\n",
       "        [0.36105635, 0.73984099],\n",
       "        [0.99645573, 0.31634698],\n",
       "        [0.13654458, 0.38398001]]),\n",
       " array([[0.32051928, 0.36641475],\n",
       "        [0.70965156, 0.90014243],\n",
       "        [0.53411544, 0.24729376],\n",
       "        [0.67180656, 0.56172911],\n",
       "        [0.54255988, 0.8934476 ]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A = np.array([\n",
    "#     [5,3,2,1,0],\n",
    "#     [3,0,4,2,1],\n",
    "#     [4,0,0,1,0],\n",
    "#     [0,0,2,4,0],\n",
    "#     [5,3,0,0,1]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# m = A.shape[0]\n",
    "# n = A.shape[1]\n",
    "# user_features = 2\n",
    "\n",
    "# np.random.seed(24)\n",
    "\n",
    "# # generate matrices with entries of random numbers 0 - 1\n",
    "# U = np.random.rand(m, user_features)\n",
    "# V = np.random.rand(n, user_features)\n",
    "\n",
    "# U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d1674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = tf.convert_to_tensor(A, dtype = tf.float32)\n",
    "\n",
    "# U = tf.convert_to_tensor(U, dtype=tf.float32)\n",
    "\n",
    "# V = tf.convert_to_tensor(V, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b2e3cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       " array([[5., 3., 2., 1., 0.],\n",
       "        [3., 0., 4., 2., 1.],\n",
       "        [4., 0., 0., 1., 0.],\n",
       "        [0., 0., 2., 4., 0.],\n",
       "        [5., 3., 0., 0., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       " array([[0.9600173 , 0.69951206],\n",
       "        [0.9998673 , 0.22006729],\n",
       "        [0.36105636, 0.739841  ],\n",
       "        [0.9964557 , 0.31634697],\n",
       "        [0.13654459, 0.38398   ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       " array([[0.3205193 , 0.36641476],\n",
       "        [0.7096516 , 0.90014243],\n",
       "        [0.53411543, 0.24729377],\n",
       "        [0.6718066 , 0.56172913],\n",
       "        [0.54255986, 0.8934476 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A, U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1128512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.9998673 , 0.22006729], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# U[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44e2c7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.90764934>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V_t = tf.transpose(V)\n",
    "\n",
    "# tf.einsum('i,i->',U[1,:],V_t[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V = tf.transpose(V)\n",
    "\n",
    "# m = tf.einsum('ij,jk->ik',U, V)\n",
    "\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0495e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "## TensorFlow Implementation Pending (likely not possible directly)\n",
    "\n",
    "# A = np.array([\n",
    "#     [5,3,2,1,0],\n",
    "#     [3,0,4,2,1],\n",
    "#     [4,0,0,1,0],\n",
    "#     [0,0,2,4,0],\n",
    "#     [5,3,0,0,1]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# steps = 200\n",
    "# m = A.shape[0]\n",
    "# n = A.shape[1]\n",
    "# user_features = 2\n",
    "\n",
    "# np.random.seed(24)\n",
    "\n",
    "# # generate matrices with entries of random numbers 0 - 1\n",
    "# U = np.random.rand(m, user_features)\n",
    "# V = np.random.rand(n, user_features)\n",
    "\n",
    "# # convert A, U, V into tf tensors for input\n",
    "# A = tf.convert_to_tensor(A, dtype = tf.float32)\n",
    "\n",
    "# U = tf.convert_to_tensor(U, dtype=tf.float32)\n",
    "\n",
    "# V = tf.convert_to_tensor(V, dtype=tf.float32)\n",
    "\n",
    "# # transpose V\n",
    "# V = tf.transpose(V)\n",
    "# # set optimizer for gradient descent\n",
    "# optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "# # number of latent features\n",
    "# K = 2\n",
    "\n",
    "# @tf.function\n",
    "# def component_wise_mse(A, comp1, comp2):\n",
    "#     eij = A[i,j] - comp1*comp2\n",
    "#     return tf.pow(eij, 2)\n",
    "\n",
    "\n",
    "# for epoch in range(steps):\n",
    "#     for i in range(A.shape[0]):\n",
    "#         for j in range(A.shape[1]):\n",
    "#             # we are only looking to minimize the loss on the already-filled entries\n",
    "#             if A[i][j] > 0:\n",
    "#                 for k in range(K):\n",
    "#                     # error                \n",
    "#                     v_kj = tf.Variable(V[k, j], trainable=True)\n",
    "#                     u_ik = tf.Variable(U[i, k], trainable=True)\n",
    "#                     trainable = [u_ik, v_kj]\n",
    "#                     with tf.GradientTape() as tp:\n",
    "#                         # define error (loss) in terms of trainable variables\n",
    "#                         loss_fn = component_wise_mse(A, u_ik, v_kj)\n",
    "\n",
    "#                         dloss = tp.gradient(loss_fn, trainable)\n",
    "#     #                     # gradient descent step\n",
    "#     #                     optimizer.apply_gradients(zip(dloss, loss_fn))\n",
    "#     print(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommenders",
   "language": "python",
   "name": "recommenders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
