{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2e48f8",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "We will implement matrix factorization here, both manually and maybe through TensorFlow.\n",
    "\n",
    "## Theory Behind Matrix Factorization: Problem Statement\n",
    "\n",
    "We will give a brief rundown, nothing too complicated.\n",
    "\n",
    "The main **mathematical** statement that we will tackle with matrix factorization is as follows:\n",
    "\n",
    "We have some matrix $A$ (this could be a tabular dataset or anything else). Now the *caveat* is that $A$ has **missing entries** that do not know about. We want to find two matrices $U$ and $V$, so that their product:\n",
    "\n",
    "$$U \\cdot V^{T} \\approx A$$\n",
    "\n",
    "So really, in terms of an optimization problem, we want to minimize each entry of the matrix $E$:\n",
    "\n",
    "$$E = A - U\\cdot V^{T}$$\n",
    "\n",
    "It really helps to write this in index notation to represent each component so that we are not dealing with matrices directly. Let $E_{ij} := e_{ij}$, then, writing out the above expression in matrix form, we see that:\n",
    "\n",
    "$$e_{ij} = a_{ij} - \\sum_{k}u_{ik}v_{jk}^{T} = a_{ij} - \\sum_{k}u_{ik}v_{kj}$$\n",
    "\n",
    "We are now only working with real numbers! We can now adopt the traditional MSE loss for each entry.\n",
    "\n",
    "$$e_{ij}^{2} = \\left(a_{ij} - \\sum_{k}u_{ik}v_{kj}\\right)^{2}$$\n",
    "\n",
    "or to make it look simpler, we can write $\\hat{a}_{ij} = \\sum_{k}u_{ik}v_{kj}$, and we now have:\n",
    "\n",
    "$$e_{ij}^{2} = (a_{ij} - \\hat{a}_{ij})^{2}$$\n",
    "\n",
    "We are now minimizing this expression for all $(i,j) \\in rows(A), cols(A))$. **For all intents and purposes**, this is all we really need to know to continue.\n",
    "\n",
    "## Theory of Matrix Factorization: Machine Learning\n",
    "\n",
    "However, we can continue our derivation of our equations, as this will help us in our implementation. We take the gradient (derivative) of our loss above, with respect to the nonzero components of both $U$ and $V$ (**Notice that we are not training weights and biases, but just the entries of the matrices $U$ and $V$!!!**):\n",
    "\n",
    "$$\\frac{\\partial e_{ij}^{2}}{\\partial u_{ik}} = \\frac{\\partial e_{ij}^{2}}{\\partial e_{ij}}\\frac{\\partial e_{ij}}{\\partial u_{ik}} = -2e_{ij}v_{kj} $$\n",
    "\n",
    "Likewise, for the other component:\n",
    "\n",
    "$$\\frac{\\partial e_{ij}^{2}}{\\partial v_{ik}} = -2e_{ij}u_{ik}$$\n",
    "\n",
    "We now do the **optimization step** (i.e. the *Gradient Descent Step*):\n",
    "\n",
    "$$u_{ik} \\longmapsto u_{ik} - \\lambda \\frac{\\partial e_{ij}^{2}}{\\partial u_{ik}} = u_{ik} + 2\\lambda e_{ij}v_{kj}$$\n",
    "\n",
    "$$v_{kj} \\longmapsto v_{kj} - \\lambda \\frac{\\partial e_{ij}^{2}}{\\partial v_{kj}} = v_{kj} + 2\\lambda e_{ij}u_{ik}$$\n",
    "\n",
    "## Business Use of Matrix Factorization\n",
    "\n",
    "The business problem we are trying to solve is the following:\n",
    "\n",
    "We are given a table of **customers** and **products** they interacted with, however, some customers do not leave ratings on the product that they interacted with. In this case, how do we find these missing ratings?\n",
    "\n",
    "Furthermore, let us say that we track certain features for the customers and products. For example:\n",
    "\n",
    "Let $U$ denote the customers, and let $V$ denote items bought at a food mart. Let us say that $U$, $V$ contains **2 features**, sweet and savory, and let $A$ be the rating of sweet or savory items (from 1-5) by a customer that purchased them. Sweet and savory are what we refer to as **latent features**, and they relate customers to products as they are keys that merge customers and products. \n",
    "\n",
    "Now assume $A$ is the following customer-product interaction table...\n",
    "\n",
    "$$A = \\begin{pmatrix} \n",
    "4 & 0 & 3 & 2 & 2 & 1\\\\[3pt]\n",
    "0 & 5 & 3 & 4 & 5 & 1\\\\[3pt]\n",
    "5 & 5 & 0 & 0 & 3 & 5\\\\[3pt]\n",
    "3 & 3 & 4 & 3 & 2 & 0\\\\[3pt]\n",
    "2 & 4 & 2 & 4 & 5 & 1\\\\[3pt]\n",
    "0 & 0 & 5 & 3 & 3 & 4\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then we want to find $U$ and $V$ such that:\n",
    "\n",
    "- $U$ encodes the latent features (sweet or savory) of the customers, and $V$ does the same for the products.\n",
    "- $U \\cdot V^{T} \\approx A$.\n",
    "\n",
    "Thus, we are looking for $U$, $V$ such that:\n",
    "\n",
    "$$A \\approx U\\cdot V^{T}$$\n",
    "\n",
    "Now refer to the previous section for the mathematical details of our machine learning problem.\n",
    "\n",
    "\n",
    "## Manual Implementation of Matrix Factorization\n",
    "\n",
    "By \"manual implementation\", we mean *implementation in python with only numpy*. This is possible because we are using the following loss:\n",
    "\n",
    "$$\\mathrm{Loss} = \\mathrm{MSE} + \\mathrm{Regularization}$$\n",
    "\n",
    "This loss goes component-by-component:\n",
    "\n",
    "$$e_{ij}^{2} = (a_{ij} - \\hat{a}_{ij})^{2}$$\n",
    "\n",
    "The total loss is\n",
    "\n",
    "$$\\hat{L} = \\sum_{i,j} e_{ij}^{2}$$\n",
    "\n",
    "We also add a regularization term:\n",
    "\n",
    "$$L = \\hat{L} + \\sum_{k=1}^{N}\\beta\\left(u_{ik}^{2} + v_{jk}^{2}\\right)$$\n",
    "\n",
    "For $N$ latent features. This is to prevent overfitting.\n",
    "\n",
    "So we have a closed-form expression for the gradient of the loss in this case. If we wanted to implement more complicated loss functions, we will likely need to use an automatic differentiation package.\n",
    "\n",
    "### Matrix Factorization 1: A Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ab2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import timeit as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f4aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARGUMENTS\n",
    "# A - ground truth matrix\n",
    "# U - product 1\n",
    "# V - product 2\n",
    "# number of features\n",
    "# epochs - number of epochs to train\n",
    "# lambda - learning rate\n",
    "# beta - regularization parameter\n",
    "\n",
    "def matrix_factorization(A, U, V, k, steps, lmbda=0.0002, beta = 0.02):\n",
    "    V = V.T\n",
    "\n",
    "    for epoch in range(steps):\n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                # we are only looking to minimize the loss on the already-filled entries\n",
    "                if A[i][j] > 0:\n",
    "                    # error\n",
    "                    eij = A[i][j] - np.dot(U[i, :], V[:, j])\n",
    "                    \n",
    "                    for K in range(k):\n",
    "                        # gradient descent step\n",
    "                        U[i][K] = U[i][K] + lmbda * (2*eij * V[K][j] - beta*U[i][K])\n",
    "                        V[K][j] = V[K][j] + lmbda * (2*eij * U[i][K] - beta*V[K][j])\n",
    "        \n",
    "        # e is our loss (i.e. our e_{ij}^{2})\n",
    "        e = 0\n",
    "        \n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                if A[i][j] > 0:\n",
    "                    # update the loss\n",
    "                    e = e + (A[i][j] - np.dot(U[i,:], V[:,j]))**2\n",
    "                    \n",
    "                    for K in range(k):\n",
    "                        # put regularization term on loss so it doesn't overfit on training data\n",
    "                        e = e + (beta/2) * (U[i][K]**2 + V[K][j]**2)\n",
    "                     \n",
    "        # give print output of training (print only every 10 epochs)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} : Total Loss {e:.3f}')\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return U, V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0113a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.9600173  0.69951205]\n",
      " [0.99986729 0.2200673 ]\n",
      " [0.36105635 0.73984099]\n",
      " [0.99645573 0.31634698]\n",
      " [0.13654458 0.38398001]]\n",
      "V is:\n",
      " [[0.32051928 0.36641475]\n",
      " [0.70965156 0.90014243]\n",
      " [0.53411544 0.24729376]\n",
      " [0.67180656 0.56172911]\n",
      " [0.54255988 0.8934476 ]]\n",
      "Epoch 1 : Total Loss 99.275\n",
      "Epoch 11 : Total Loss 96.767\n",
      "Epoch 21 : Total Loss 94.202\n",
      "Epoch 31 : Total Loss 91.584\n",
      "Epoch 41 : Total Loss 88.918\n",
      "Epoch 51 : Total Loss 86.207\n",
      "Epoch 61 : Total Loss 83.459\n",
      "Epoch 71 : Total Loss 80.680\n",
      "Epoch 81 : Total Loss 77.878\n",
      "Epoch 91 : Total Loss 75.059\n",
      "Epoch 101 : Total Loss 72.233\n",
      "Epoch 111 : Total Loss 69.408\n",
      "Epoch 121 : Total Loss 66.595\n",
      "Epoch 131 : Total Loss 63.801\n",
      "Epoch 141 : Total Loss 61.036\n",
      "Epoch 151 : Total Loss 58.311\n",
      "Epoch 161 : Total Loss 55.633\n",
      "Epoch 171 : Total Loss 53.012\n",
      "Epoch 181 : Total Loss 50.455\n",
      "Epoch 191 : Total Loss 47.970\n",
      "U_hat is:\n",
      " [[1.2427563  0.99404657]\n",
      " [1.32958304 0.4449768 ]\n",
      " [0.53087359 0.91554011]\n",
      " [1.23935835 0.47920855]\n",
      " [0.51268872 0.8134178 ]]\n",
      "V_hat is:\n",
      " [[1.02502384 1.0589032 ]\n",
      " [0.87169684 1.08305621]\n",
      " [0.97105056 0.41299391]\n",
      " [0.95914034 0.64674232]\n",
      " [0.55186549 0.90495035]]\n",
      "A_hat is:\n",
      " [[2.32645393 2.15991506 1.61731439 1.83486969 1.58539711]\n",
      " [1.83404167 1.64092823 1.47486507 1.56304206 1.13643291]\n",
      " [1.51362644 1.45434224 0.89361759 1.10130081 1.12148916]\n",
      " [1.77780732 1.59935457 1.40138984 1.49864305 1.11761906]\n",
      " [1.38684887 1.32788634 0.83378327 1.01781215 1.01903794]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [5,3,2,1,0],\n",
    "    [3,0,4,2,1],\n",
    "    [4,0,0,1,0],\n",
    "    [0,0,2,4,0],\n",
    "    [5,3,0,0,1]\n",
    "])\n",
    "\n",
    "m = A.shape[0]\n",
    "n = A.shape[1]\n",
    "user_features = 2\n",
    "\n",
    "np.random.seed(24)\n",
    "\n",
    "# generate matrices with entries of random numbers 0 - 1\n",
    "U = np.random.rand(m, user_features)\n",
    "V = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U)\n",
    "print('V is:\\n', V)\n",
    "\n",
    "# perform factorization\n",
    "\n",
    "U_hat, V_hat = matrix_factorization(A, U, V, user_features, 200)\n",
    "\n",
    "print('U_hat is:\\n', U_hat)\n",
    "print('V_hat is:\\n', V_hat)\n",
    "\n",
    "## check if the product results in something close to A\n",
    "\n",
    "print('A_hat is:\\n', np.dot(U_hat, V_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9e1ff",
   "metadata": {},
   "source": [
    "This does not look good after 200 epochs! Let's try more epochs, and if that doesn't work, let's increase the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a54ec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 45.564\n",
      "Epoch 11 : Total Loss 43.244\n",
      "Epoch 21 : Total Loss 41.013\n",
      "Epoch 31 : Total Loss 38.877\n",
      "Epoch 41 : Total Loss 36.838\n",
      "Epoch 51 : Total Loss 34.898\n",
      "Epoch 61 : Total Loss 33.060\n",
      "Epoch 71 : Total Loss 31.323\n",
      "Epoch 81 : Total Loss 29.687\n",
      "Epoch 91 : Total Loss 28.150\n",
      "Epoch 101 : Total Loss 26.711\n",
      "Epoch 111 : Total Loss 25.366\n",
      "Epoch 121 : Total Loss 24.114\n",
      "Epoch 131 : Total Loss 22.950\n",
      "Epoch 141 : Total Loss 21.870\n",
      "Epoch 151 : Total Loss 20.871\n",
      "Epoch 161 : Total Loss 19.947\n",
      "Epoch 171 : Total Loss 19.095\n",
      "Epoch 181 : Total Loss 18.310\n",
      "Epoch 191 : Total Loss 17.587\n",
      "Epoch 201 : Total Loss 16.923\n",
      "Epoch 211 : Total Loss 16.312\n",
      "Epoch 221 : Total Loss 15.751\n",
      "Epoch 231 : Total Loss 15.236\n",
      "Epoch 241 : Total Loss 14.763\n",
      "Epoch 251 : Total Loss 14.328\n",
      "Epoch 261 : Total Loss 13.929\n",
      "Epoch 271 : Total Loss 13.563\n",
      "Epoch 281 : Total Loss 13.225\n",
      "Epoch 291 : Total Loss 12.914\n",
      "Epoch 301 : Total Loss 12.628\n",
      "Epoch 311 : Total Loss 12.364\n",
      "Epoch 321 : Total Loss 12.120\n",
      "Epoch 331 : Total Loss 11.894\n",
      "Epoch 341 : Total Loss 11.684\n",
      "Epoch 351 : Total Loss 11.490\n",
      "Epoch 361 : Total Loss 11.309\n",
      "Epoch 371 : Total Loss 11.140\n",
      "Epoch 381 : Total Loss 10.983\n",
      "Epoch 391 : Total Loss 10.835\n",
      "Epoch 401 : Total Loss 10.697\n",
      "Epoch 411 : Total Loss 10.568\n",
      "Epoch 421 : Total Loss 10.445\n",
      "Epoch 431 : Total Loss 10.330\n",
      "Epoch 441 : Total Loss 10.221\n",
      "Epoch 451 : Total Loss 10.118\n",
      "Epoch 461 : Total Loss 10.020\n",
      "Epoch 471 : Total Loss 9.927\n",
      "Epoch 481 : Total Loss 9.838\n",
      "Epoch 491 : Total Loss 9.753\n",
      "Epoch 501 : Total Loss 9.672\n",
      "Epoch 511 : Total Loss 9.594\n",
      "Epoch 521 : Total Loss 9.519\n",
      "Epoch 531 : Total Loss 9.448\n",
      "Epoch 541 : Total Loss 9.378\n",
      "Epoch 551 : Total Loss 9.312\n",
      "Epoch 561 : Total Loss 9.247\n",
      "Epoch 571 : Total Loss 9.185\n",
      "Epoch 581 : Total Loss 9.125\n",
      "Epoch 591 : Total Loss 9.066\n",
      "Epoch 601 : Total Loss 9.009\n",
      "Epoch 611 : Total Loss 8.954\n",
      "Epoch 621 : Total Loss 8.900\n",
      "Epoch 631 : Total Loss 8.847\n",
      "Epoch 641 : Total Loss 8.796\n",
      "Epoch 651 : Total Loss 8.746\n",
      "Epoch 661 : Total Loss 8.697\n",
      "Epoch 671 : Total Loss 8.649\n",
      "Epoch 681 : Total Loss 8.602\n",
      "Epoch 691 : Total Loss 8.556\n",
      "Epoch 701 : Total Loss 8.511\n",
      "Epoch 711 : Total Loss 8.467\n",
      "Epoch 721 : Total Loss 8.423\n",
      "Epoch 731 : Total Loss 8.380\n",
      "Epoch 741 : Total Loss 8.338\n",
      "Epoch 751 : Total Loss 8.297\n",
      "Epoch 761 : Total Loss 8.256\n",
      "Epoch 771 : Total Loss 8.216\n",
      "Epoch 781 : Total Loss 8.176\n",
      "Epoch 791 : Total Loss 8.137\n",
      "Epoch 801 : Total Loss 8.099\n",
      "Epoch 811 : Total Loss 8.061\n",
      "Epoch 821 : Total Loss 8.024\n",
      "Epoch 831 : Total Loss 7.987\n",
      "Epoch 841 : Total Loss 7.950\n",
      "Epoch 851 : Total Loss 7.914\n",
      "Epoch 861 : Total Loss 7.878\n",
      "Epoch 871 : Total Loss 7.843\n",
      "Epoch 881 : Total Loss 7.808\n",
      "Epoch 891 : Total Loss 7.774\n",
      "Epoch 901 : Total Loss 7.740\n",
      "Epoch 911 : Total Loss 7.706\n",
      "Epoch 921 : Total Loss 7.673\n",
      "Epoch 931 : Total Loss 7.640\n",
      "Epoch 941 : Total Loss 7.608\n",
      "Epoch 951 : Total Loss 7.575\n",
      "Epoch 961 : Total Loss 7.544\n",
      "Epoch 971 : Total Loss 7.512\n",
      "Epoch 981 : Total Loss 7.481\n",
      "Epoch 991 : Total Loss 7.450\n",
      "Epoch 1001 : Total Loss 7.419\n",
      "Epoch 1011 : Total Loss 7.389\n",
      "Epoch 1021 : Total Loss 7.359\n",
      "Epoch 1031 : Total Loss 7.330\n",
      "Epoch 1041 : Total Loss 7.300\n",
      "Epoch 1051 : Total Loss 7.271\n",
      "Epoch 1061 : Total Loss 7.243\n",
      "Epoch 1071 : Total Loss 7.214\n",
      "Epoch 1081 : Total Loss 7.186\n",
      "Epoch 1091 : Total Loss 7.158\n",
      "Epoch 1101 : Total Loss 7.131\n",
      "Epoch 1111 : Total Loss 7.104\n",
      "Epoch 1121 : Total Loss 7.077\n",
      "Epoch 1131 : Total Loss 7.050\n",
      "Epoch 1141 : Total Loss 7.024\n",
      "Epoch 1151 : Total Loss 6.998\n",
      "Epoch 1161 : Total Loss 6.972\n",
      "Epoch 1171 : Total Loss 6.947\n",
      "Epoch 1181 : Total Loss 6.922\n",
      "Epoch 1191 : Total Loss 6.897\n",
      "Epoch 1201 : Total Loss 6.873\n",
      "Epoch 1211 : Total Loss 6.849\n",
      "Epoch 1221 : Total Loss 6.825\n",
      "Epoch 1231 : Total Loss 6.801\n",
      "Epoch 1241 : Total Loss 6.778\n",
      "Epoch 1251 : Total Loss 6.755\n",
      "Epoch 1261 : Total Loss 6.732\n",
      "Epoch 1271 : Total Loss 6.709\n",
      "Epoch 1281 : Total Loss 6.687\n",
      "Epoch 1291 : Total Loss 6.665\n",
      "Epoch 1301 : Total Loss 6.644\n",
      "Epoch 1311 : Total Loss 6.622\n",
      "Epoch 1321 : Total Loss 6.601\n",
      "Epoch 1331 : Total Loss 6.581\n",
      "Epoch 1341 : Total Loss 6.560\n",
      "Epoch 1351 : Total Loss 6.540\n",
      "Epoch 1361 : Total Loss 6.520\n",
      "Epoch 1371 : Total Loss 6.501\n",
      "Epoch 1381 : Total Loss 6.481\n",
      "Epoch 1391 : Total Loss 6.462\n",
      "Epoch 1401 : Total Loss 6.443\n",
      "Epoch 1411 : Total Loss 6.425\n",
      "Epoch 1421 : Total Loss 6.407\n",
      "Epoch 1431 : Total Loss 6.389\n",
      "Epoch 1441 : Total Loss 6.371\n",
      "Epoch 1451 : Total Loss 6.354\n",
      "Epoch 1461 : Total Loss 6.337\n",
      "Epoch 1471 : Total Loss 6.320\n",
      "Epoch 1481 : Total Loss 6.303\n",
      "Epoch 1491 : Total Loss 6.287\n",
      "Epoch 1501 : Total Loss 6.271\n",
      "Epoch 1511 : Total Loss 6.255\n",
      "Epoch 1521 : Total Loss 6.240\n",
      "Epoch 1531 : Total Loss 6.224\n",
      "Epoch 1541 : Total Loss 6.209\n",
      "Epoch 1551 : Total Loss 6.195\n",
      "Epoch 1561 : Total Loss 6.180\n",
      "Epoch 1571 : Total Loss 6.166\n",
      "Epoch 1581 : Total Loss 6.152\n",
      "Epoch 1591 : Total Loss 6.138\n",
      "Epoch 1601 : Total Loss 6.124\n",
      "Epoch 1611 : Total Loss 6.111\n",
      "Epoch 1621 : Total Loss 6.098\n",
      "Epoch 1631 : Total Loss 6.085\n",
      "Epoch 1641 : Total Loss 6.073\n",
      "Epoch 1651 : Total Loss 6.060\n",
      "Epoch 1661 : Total Loss 6.048\n",
      "Epoch 1671 : Total Loss 6.036\n",
      "Epoch 1681 : Total Loss 6.025\n",
      "Epoch 1691 : Total Loss 6.013\n",
      "Epoch 1701 : Total Loss 6.002\n",
      "Epoch 1711 : Total Loss 5.991\n",
      "Epoch 1721 : Total Loss 5.980\n",
      "Epoch 1731 : Total Loss 5.970\n",
      "Epoch 1741 : Total Loss 5.959\n",
      "Epoch 1751 : Total Loss 5.949\n",
      "Epoch 1761 : Total Loss 5.939\n",
      "Epoch 1771 : Total Loss 5.929\n",
      "Epoch 1781 : Total Loss 5.920\n",
      "Epoch 1791 : Total Loss 5.910\n",
      "Epoch 1801 : Total Loss 5.901\n",
      "Epoch 1811 : Total Loss 5.892\n",
      "Epoch 1821 : Total Loss 5.883\n",
      "Epoch 1831 : Total Loss 5.875\n",
      "Epoch 1841 : Total Loss 5.866\n",
      "Epoch 1851 : Total Loss 5.858\n",
      "Epoch 1861 : Total Loss 5.850\n",
      "Epoch 1871 : Total Loss 5.842\n",
      "Epoch 1881 : Total Loss 5.834\n",
      "Epoch 1891 : Total Loss 5.827\n",
      "Epoch 1901 : Total Loss 5.819\n",
      "Epoch 1911 : Total Loss 5.812\n",
      "Epoch 1921 : Total Loss 5.805\n",
      "Epoch 1931 : Total Loss 5.798\n",
      "Epoch 1941 : Total Loss 5.791\n",
      "Epoch 1951 : Total Loss 5.784\n",
      "Epoch 1961 : Total Loss 5.778\n",
      "Epoch 1971 : Total Loss 5.771\n",
      "Epoch 1981 : Total Loss 5.765\n",
      "Epoch 1991 : Total Loss 5.759\n",
      "Epoch 2001 : Total Loss 5.753\n",
      "Epoch 2011 : Total Loss 5.747\n",
      "Epoch 2021 : Total Loss 5.741\n",
      "Epoch 2031 : Total Loss 5.736\n",
      "Epoch 2041 : Total Loss 5.730\n",
      "Epoch 2051 : Total Loss 5.725\n",
      "Epoch 2061 : Total Loss 5.720\n",
      "Epoch 2071 : Total Loss 5.714\n",
      "Epoch 2081 : Total Loss 5.709\n",
      "Epoch 2091 : Total Loss 5.705\n",
      "Epoch 2101 : Total Loss 5.700\n",
      "Epoch 2111 : Total Loss 5.695\n",
      "Epoch 2121 : Total Loss 5.691\n",
      "Epoch 2131 : Total Loss 5.686\n",
      "Epoch 2141 : Total Loss 5.682\n",
      "Epoch 2151 : Total Loss 5.678\n",
      "Epoch 2161 : Total Loss 5.673\n",
      "Epoch 2171 : Total Loss 5.669\n",
      "Epoch 2181 : Total Loss 5.665\n",
      "Epoch 2191 : Total Loss 5.661\n",
      "Epoch 2201 : Total Loss 5.658\n",
      "Epoch 2211 : Total Loss 5.654\n",
      "Epoch 2221 : Total Loss 5.650\n",
      "Epoch 2231 : Total Loss 5.647\n",
      "Epoch 2241 : Total Loss 5.643\n",
      "Epoch 2251 : Total Loss 5.640\n",
      "Epoch 2261 : Total Loss 5.637\n",
      "Epoch 2271 : Total Loss 5.634\n",
      "Epoch 2281 : Total Loss 5.630\n",
      "Epoch 2291 : Total Loss 5.627\n",
      "Epoch 2301 : Total Loss 5.624\n",
      "Epoch 2311 : Total Loss 5.621\n",
      "Epoch 2321 : Total Loss 5.618\n",
      "Epoch 2331 : Total Loss 5.616\n",
      "Epoch 2341 : Total Loss 5.613\n",
      "Epoch 2351 : Total Loss 5.610\n",
      "Epoch 2361 : Total Loss 5.608\n",
      "Epoch 2371 : Total Loss 5.605\n",
      "Epoch 2381 : Total Loss 5.603\n",
      "Epoch 2391 : Total Loss 5.600\n",
      "Epoch 2401 : Total Loss 5.598\n",
      "Epoch 2411 : Total Loss 5.596\n",
      "Epoch 2421 : Total Loss 5.593\n",
      "Epoch 2431 : Total Loss 5.591\n",
      "Epoch 2441 : Total Loss 5.589\n",
      "Epoch 2451 : Total Loss 5.587\n",
      "Epoch 2461 : Total Loss 5.585\n",
      "Epoch 2471 : Total Loss 5.583\n",
      "Epoch 2481 : Total Loss 5.581\n",
      "Epoch 2491 : Total Loss 5.579\n",
      "Epoch 2501 : Total Loss 5.577\n",
      "Epoch 2511 : Total Loss 5.575\n",
      "Epoch 2521 : Total Loss 5.573\n",
      "Epoch 2531 : Total Loss 5.571\n",
      "Epoch 2541 : Total Loss 5.570\n",
      "Epoch 2551 : Total Loss 5.568\n",
      "Epoch 2561 : Total Loss 5.566\n",
      "Epoch 2571 : Total Loss 5.565\n",
      "Epoch 2581 : Total Loss 5.563\n",
      "Epoch 2591 : Total Loss 5.562\n",
      "Epoch 2601 : Total Loss 5.560\n",
      "Epoch 2611 : Total Loss 5.559\n",
      "Epoch 2621 : Total Loss 5.557\n",
      "Epoch 2631 : Total Loss 5.556\n",
      "Epoch 2641 : Total Loss 5.554\n",
      "Epoch 2651 : Total Loss 5.553\n",
      "Epoch 2661 : Total Loss 5.552\n",
      "Epoch 2671 : Total Loss 5.551\n",
      "Epoch 2681 : Total Loss 5.549\n",
      "Epoch 2691 : Total Loss 5.548\n",
      "Epoch 2701 : Total Loss 5.547\n",
      "Epoch 2711 : Total Loss 5.546\n",
      "Epoch 2721 : Total Loss 5.544\n",
      "Epoch 2731 : Total Loss 5.543\n",
      "Epoch 2741 : Total Loss 5.542\n",
      "Epoch 2751 : Total Loss 5.541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2761 : Total Loss 5.540\n",
      "Epoch 2771 : Total Loss 5.539\n",
      "Epoch 2781 : Total Loss 5.538\n",
      "Epoch 2791 : Total Loss 5.537\n",
      "Epoch 2801 : Total Loss 5.536\n",
      "Epoch 2811 : Total Loss 5.535\n",
      "Epoch 2821 : Total Loss 5.534\n",
      "Epoch 2831 : Total Loss 5.533\n",
      "Epoch 2841 : Total Loss 5.532\n",
      "Epoch 2851 : Total Loss 5.531\n",
      "Epoch 2861 : Total Loss 5.531\n",
      "Epoch 2871 : Total Loss 5.530\n",
      "Epoch 2881 : Total Loss 5.529\n",
      "Epoch 2891 : Total Loss 5.528\n",
      "Epoch 2901 : Total Loss 5.527\n",
      "Epoch 2911 : Total Loss 5.526\n",
      "Epoch 2921 : Total Loss 5.526\n",
      "Epoch 2931 : Total Loss 5.525\n",
      "Epoch 2941 : Total Loss 5.524\n",
      "Epoch 2951 : Total Loss 5.523\n",
      "Epoch 2961 : Total Loss 5.523\n",
      "Epoch 2971 : Total Loss 5.522\n",
      "Epoch 2981 : Total Loss 5.521\n",
      "Epoch 2991 : Total Loss 5.521\n",
      "Epoch 3001 : Total Loss 5.520\n",
      "Epoch 3011 : Total Loss 5.519\n",
      "Epoch 3021 : Total Loss 5.519\n",
      "Epoch 3031 : Total Loss 5.518\n",
      "Epoch 3041 : Total Loss 5.517\n",
      "Epoch 3051 : Total Loss 5.517\n",
      "Epoch 3061 : Total Loss 5.516\n",
      "Epoch 3071 : Total Loss 5.516\n",
      "Epoch 3081 : Total Loss 5.515\n",
      "Epoch 3091 : Total Loss 5.515\n",
      "Epoch 3101 : Total Loss 5.514\n",
      "Epoch 3111 : Total Loss 5.513\n",
      "Epoch 3121 : Total Loss 5.513\n",
      "Epoch 3131 : Total Loss 5.512\n",
      "Epoch 3141 : Total Loss 5.512\n",
      "Epoch 3151 : Total Loss 5.511\n",
      "Epoch 3161 : Total Loss 5.511\n",
      "Epoch 3171 : Total Loss 5.510\n",
      "Epoch 3181 : Total Loss 5.510\n",
      "Epoch 3191 : Total Loss 5.509\n",
      "Epoch 3201 : Total Loss 5.509\n",
      "Epoch 3211 : Total Loss 5.508\n",
      "Epoch 3221 : Total Loss 5.508\n",
      "Epoch 3231 : Total Loss 5.508\n",
      "Epoch 3241 : Total Loss 5.507\n",
      "Epoch 3251 : Total Loss 5.507\n",
      "Epoch 3261 : Total Loss 5.506\n",
      "Epoch 3271 : Total Loss 5.506\n",
      "Epoch 3281 : Total Loss 5.505\n",
      "Epoch 3291 : Total Loss 5.505\n",
      "Epoch 3301 : Total Loss 5.505\n",
      "Epoch 3311 : Total Loss 5.504\n",
      "Epoch 3321 : Total Loss 5.504\n",
      "Epoch 3331 : Total Loss 5.503\n",
      "Epoch 3341 : Total Loss 5.503\n",
      "Epoch 3351 : Total Loss 5.503\n",
      "Epoch 3361 : Total Loss 5.502\n",
      "Epoch 3371 : Total Loss 5.502\n",
      "Epoch 3381 : Total Loss 5.502\n",
      "Epoch 3391 : Total Loss 5.501\n",
      "Epoch 3401 : Total Loss 5.501\n",
      "Epoch 3411 : Total Loss 5.501\n",
      "Epoch 3421 : Total Loss 5.500\n",
      "Epoch 3431 : Total Loss 5.500\n",
      "Epoch 3441 : Total Loss 5.500\n",
      "Epoch 3451 : Total Loss 5.499\n",
      "Epoch 3461 : Total Loss 5.499\n",
      "Epoch 3471 : Total Loss 5.499\n",
      "Epoch 3481 : Total Loss 5.498\n",
      "Epoch 3491 : Total Loss 5.498\n",
      "Epoch 3501 : Total Loss 5.498\n",
      "Epoch 3511 : Total Loss 5.497\n",
      "Epoch 3521 : Total Loss 5.497\n",
      "Epoch 3531 : Total Loss 5.497\n",
      "Epoch 3541 : Total Loss 5.496\n",
      "Epoch 3551 : Total Loss 5.496\n",
      "Epoch 3561 : Total Loss 5.496\n",
      "Epoch 3571 : Total Loss 5.496\n",
      "Epoch 3581 : Total Loss 5.495\n",
      "Epoch 3591 : Total Loss 5.495\n",
      "Epoch 3601 : Total Loss 5.495\n",
      "Epoch 3611 : Total Loss 5.494\n",
      "Epoch 3621 : Total Loss 5.494\n",
      "Epoch 3631 : Total Loss 5.494\n",
      "Epoch 3641 : Total Loss 5.494\n",
      "Epoch 3651 : Total Loss 5.493\n",
      "Epoch 3661 : Total Loss 5.493\n",
      "Epoch 3671 : Total Loss 5.493\n",
      "Epoch 3681 : Total Loss 5.493\n",
      "Epoch 3691 : Total Loss 5.492\n",
      "Epoch 3701 : Total Loss 5.492\n",
      "Epoch 3711 : Total Loss 5.492\n",
      "Epoch 3721 : Total Loss 5.492\n",
      "Epoch 3731 : Total Loss 5.491\n",
      "Epoch 3741 : Total Loss 5.491\n",
      "Epoch 3751 : Total Loss 5.491\n",
      "Epoch 3761 : Total Loss 5.491\n",
      "Epoch 3771 : Total Loss 5.491\n",
      "Epoch 3781 : Total Loss 5.490\n",
      "Epoch 3791 : Total Loss 5.490\n",
      "Epoch 3801 : Total Loss 5.490\n",
      "Epoch 3811 : Total Loss 5.490\n",
      "Epoch 3821 : Total Loss 5.489\n",
      "Epoch 3831 : Total Loss 5.489\n",
      "Epoch 3841 : Total Loss 5.489\n",
      "Epoch 3851 : Total Loss 5.489\n",
      "Epoch 3861 : Total Loss 5.489\n",
      "Epoch 3871 : Total Loss 5.488\n",
      "Epoch 3881 : Total Loss 5.488\n",
      "Epoch 3891 : Total Loss 5.488\n",
      "Epoch 3901 : Total Loss 5.488\n",
      "Epoch 3911 : Total Loss 5.488\n",
      "Epoch 3921 : Total Loss 5.487\n",
      "Epoch 3931 : Total Loss 5.487\n",
      "Epoch 3941 : Total Loss 5.487\n",
      "Epoch 3951 : Total Loss 5.487\n",
      "Epoch 3961 : Total Loss 5.487\n",
      "Epoch 3971 : Total Loss 5.486\n",
      "Epoch 3981 : Total Loss 5.486\n",
      "Epoch 3991 : Total Loss 5.486\n",
      "Epoch 4001 : Total Loss 5.486\n",
      "Epoch 4011 : Total Loss 5.486\n",
      "Epoch 4021 : Total Loss 5.485\n",
      "Epoch 4031 : Total Loss 5.485\n",
      "Epoch 4041 : Total Loss 5.485\n",
      "Epoch 4051 : Total Loss 5.485\n",
      "Epoch 4061 : Total Loss 5.485\n",
      "Epoch 4071 : Total Loss 5.485\n",
      "Epoch 4081 : Total Loss 5.484\n",
      "Epoch 4091 : Total Loss 5.484\n",
      "Epoch 4101 : Total Loss 5.484\n",
      "Epoch 4111 : Total Loss 5.484\n",
      "Epoch 4121 : Total Loss 5.484\n",
      "Epoch 4131 : Total Loss 5.483\n",
      "Epoch 4141 : Total Loss 5.483\n",
      "Epoch 4151 : Total Loss 5.483\n",
      "Epoch 4161 : Total Loss 5.483\n",
      "Epoch 4171 : Total Loss 5.483\n",
      "Epoch 4181 : Total Loss 5.483\n",
      "Epoch 4191 : Total Loss 5.482\n",
      "Epoch 4201 : Total Loss 5.482\n",
      "Epoch 4211 : Total Loss 5.482\n",
      "Epoch 4221 : Total Loss 5.482\n",
      "Epoch 4231 : Total Loss 5.482\n",
      "Epoch 4241 : Total Loss 5.482\n",
      "Epoch 4251 : Total Loss 5.482\n",
      "Epoch 4261 : Total Loss 5.481\n",
      "Epoch 4271 : Total Loss 5.481\n",
      "Epoch 4281 : Total Loss 5.481\n",
      "Epoch 4291 : Total Loss 5.481\n",
      "Epoch 4301 : Total Loss 5.481\n",
      "Epoch 4311 : Total Loss 5.481\n",
      "Epoch 4321 : Total Loss 5.480\n",
      "Epoch 4331 : Total Loss 5.480\n",
      "Epoch 4341 : Total Loss 5.480\n",
      "Epoch 4351 : Total Loss 5.480\n",
      "Epoch 4361 : Total Loss 5.480\n",
      "Epoch 4371 : Total Loss 5.480\n",
      "Epoch 4381 : Total Loss 5.479\n",
      "Epoch 4391 : Total Loss 5.479\n",
      "Epoch 4401 : Total Loss 5.479\n",
      "Epoch 4411 : Total Loss 5.479\n",
      "Epoch 4421 : Total Loss 5.479\n",
      "Epoch 4431 : Total Loss 5.479\n",
      "Epoch 4441 : Total Loss 5.479\n",
      "Epoch 4451 : Total Loss 5.478\n",
      "Epoch 4461 : Total Loss 5.478\n",
      "Epoch 4471 : Total Loss 5.478\n",
      "Epoch 4481 : Total Loss 5.478\n",
      "Epoch 4491 : Total Loss 5.478\n",
      "Epoch 4501 : Total Loss 5.478\n",
      "Epoch 4511 : Total Loss 5.478\n",
      "Epoch 4521 : Total Loss 5.477\n",
      "Epoch 4531 : Total Loss 5.477\n",
      "Epoch 4541 : Total Loss 5.477\n",
      "Epoch 4551 : Total Loss 5.477\n",
      "Epoch 4561 : Total Loss 5.477\n",
      "Epoch 4571 : Total Loss 5.477\n",
      "Epoch 4581 : Total Loss 5.477\n",
      "Epoch 4591 : Total Loss 5.476\n",
      "Epoch 4601 : Total Loss 5.476\n",
      "Epoch 4611 : Total Loss 5.476\n",
      "Epoch 4621 : Total Loss 5.476\n",
      "Epoch 4631 : Total Loss 5.476\n",
      "Epoch 4641 : Total Loss 5.476\n",
      "Epoch 4651 : Total Loss 5.476\n",
      "Epoch 4661 : Total Loss 5.475\n",
      "Epoch 4671 : Total Loss 5.475\n",
      "Epoch 4681 : Total Loss 5.475\n",
      "Epoch 4691 : Total Loss 5.475\n",
      "Epoch 4701 : Total Loss 5.475\n",
      "Epoch 4711 : Total Loss 5.475\n",
      "Epoch 4721 : Total Loss 5.475\n",
      "Epoch 4731 : Total Loss 5.474\n",
      "Epoch 4741 : Total Loss 5.474\n",
      "Epoch 4751 : Total Loss 5.474\n",
      "Epoch 4761 : Total Loss 5.474\n",
      "Epoch 4771 : Total Loss 5.474\n",
      "Epoch 4781 : Total Loss 5.474\n",
      "Epoch 4791 : Total Loss 5.474\n",
      "Epoch 4801 : Total Loss 5.473\n",
      "Epoch 4811 : Total Loss 5.473\n",
      "Epoch 4821 : Total Loss 5.473\n",
      "Epoch 4831 : Total Loss 5.473\n",
      "Epoch 4841 : Total Loss 5.473\n",
      "Epoch 4851 : Total Loss 5.473\n",
      "Epoch 4861 : Total Loss 5.473\n",
      "Epoch 4871 : Total Loss 5.473\n",
      "Epoch 4881 : Total Loss 5.472\n",
      "Epoch 4891 : Total Loss 5.472\n",
      "Epoch 4901 : Total Loss 5.472\n",
      "Epoch 4911 : Total Loss 5.472\n",
      "Epoch 4921 : Total Loss 5.472\n",
      "Epoch 4931 : Total Loss 5.472\n",
      "Epoch 4941 : Total Loss 5.472\n",
      "Epoch 4951 : Total Loss 5.471\n",
      "Epoch 4961 : Total Loss 5.471\n",
      "Epoch 4971 : Total Loss 5.471\n",
      "Epoch 4981 : Total Loss 5.471\n",
      "Epoch 4991 : Total Loss 5.471\n",
      "A_hat is:\n",
      " [[4.98447014 2.97009182 1.54598166 1.50219606 0.9902487 ]\n",
      " [2.99605858 2.12761136 3.17161116 2.87072417 0.95654322]\n",
      " [3.98158649 2.34293891 1.04127825 1.03001406 0.75980498]\n",
      " [4.52329661 2.9474781  3.05471784 2.81272953 1.16479036]\n",
      " [4.96573884 3.01061644 1.87870719 1.79363369 1.0410779 ]]\n"
     ]
    }
   ],
   "source": [
    "U_hat, V_hat = matrix_factorization(A, U, V, user_features, 5000)\n",
    "\n",
    "print('A_hat is:\\n', np.dot(U_hat, V_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefe3a4",
   "metadata": {},
   "source": [
    "After training for long enough, we see that the existing interactions are predicted quite well. To see the average error of each entry in $A_{hat}$ corresponding to NONZEROS in $A$, we can just use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c304c6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.98447014, 2.97009182, 1.54598166, 1.50219606, 0.9902487 ],\n",
       "       [2.99605858, 2.12761136, 3.17161116, 2.87072417, 0.95654322],\n",
       "       [3.98158649, 2.34293891, 1.04127825, 1.03001406, 0.75980498],\n",
       "       [4.52329661, 2.9474781 , 3.05471784, 2.81272953, 1.16479036],\n",
       "       [4.96573884, 3.01061644, 1.87870719, 1.79363369, 1.0410779 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_hat = np.dot(U_hat, V_hat.T)\n",
    "A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a91f831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 2, 1, 0],\n",
       "       [3, 0, 4, 2, 1],\n",
       "       [4, 0, 0, 1, 0],\n",
       "       [0, 0, 2, 4, 0],\n",
       "       [5, 3, 0, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64eecf",
   "metadata": {},
   "source": [
    "### Write a Method to Evaluate our Output\n",
    "\n",
    "Since this is a regression task, we will be evaluating the performance of our model through (root) mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29d81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have function to evaluate total loss in our prediction\n",
    "\n",
    "def evaluation_metrics(A, A_hat, return_nonzero_indices = False, return_rmse = True):\n",
    "    # find all nonzero indices of ground truth matrix\n",
    "    nonzero_ind = [(i,j) for i in range(A.shape[0]) for j in range(A.shape[1]) if A[i,j] > 0]\n",
    "    \n",
    "    # compute the losses in each ground truth matrix\n",
    "    avg_error = sum([(A_hat[i][j] - A[i][j])**2 for i, j in nonzero_ind])/len(nonzero_ind)\n",
    "    rmse = np.sqrt(avg_error)\n",
    "    \n",
    "    # set a flag for if we want to return the nonzero indices or not\n",
    "    li_results = []\n",
    "    if return_rmse:\n",
    "        li_results.append(avg_error)\n",
    "        li_results.append(rmse)\n",
    "    else:\n",
    "        li_results.append(avg_error)\n",
    "    \n",
    "    if return_nonzero_indices:\n",
    "        li_results.append(nonzero_indices)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return li_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8f88f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of all predictions is 0.295.\n",
      "The root mean squared error of all predictions is 0.544.\n"
     ]
    }
   ],
   "source": [
    "mse, rmse = evaluation_metrics(A, A_hat)\n",
    "\n",
    "print(f'The mean squared error of all predictions is {mse:.3f}.')\n",
    "print(f'The root mean squared error of all predictions is {rmse:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381c12d",
   "metadata": {},
   "source": [
    "Therefore, our matrix factorization algorithm is very good at predicting the existing entries. Let us see what the predictions were for the empty entries.\n",
    "\n",
    "### Write Method to Predict the Zero Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e9733bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a function predict the missing entries of our ground truth matrix\n",
    "\n",
    "def mat_predict(A, A_hat, conv_to_int = True, zip_indices = False):\n",
    "    # find zero indices of A (our missing entries that we are predicting)\n",
    "    zero_ind = [(i,j) for i in range(A.shape[0]) for j in range(A.shape[1]) if A[i][j] == 0]\n",
    "    \n",
    "    # find predictions\n",
    "    og_pred = [A_hat[i,j] for i,j in zero_ind]\n",
    "    \n",
    "    # WARNING: FOR VERY LARGE AND SPARSE MATRICES, I DO NOT RECOMMEND DOING THIS. JUST LOOK AT THE OUTPUT\n",
    "    if zip_indices:\n",
    "        for prediction, indices in zip(pred, zero_ind):\n",
    "            print(f'The interaction at row, column {indices} is {prediction}.')\n",
    "            \n",
    "    # convert output to integers\n",
    "    if conv_to_int:\n",
    "        # if the rounded A_hat has entries greater than the max of A, then set it to floor, otherwise, round it to integer\n",
    "        int_prediction = np.where(A_hat > np.max(A), np.floor(A_hat), np.round(A_hat, decimals=0))\n",
    "        \n",
    "        # return integer predictions at indices where 0's originally were in the ground truth A\n",
    "        return [int_prediction[i,j] for i,j in zero_ind]\n",
    "    else:\n",
    "        # return the original floating point predictions from the algorithm output\n",
    "        \n",
    "        return og_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7faa3856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 2.0, 1.0, 1.0, 5.0, 3.0, 1.0, 2.0, 2.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_predict(A, A_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eadcde4",
   "metadata": {},
   "source": [
    "### Write Method to Pipeline the Training and Evaluation of our Algorithm\n",
    "\n",
    "We will incorporate all of the evaluation outputs of the algorithm in the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa32a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the matrix factorization, display the evaluation metrics, show time elapsed, return predictions\n",
    "# this is a simple pipeline function for the evaluation of our algorithm\n",
    "\n",
    "def evaluation_pipeline(A, U, V, epochs, latent_features, factorize_fn, flag_integer_ratings = False, print_divs=10):\n",
    "        \n",
    "    m, n = A.shape[0], A.shape[1]\n",
    "\n",
    "    user_features = latent_features\n",
    "\n",
    "    # compute number of entries in input matrix\n",
    "    num_entries = A.shape[0] * A.shape[1]\n",
    "    \n",
    "    # compute density/sparsity of matrix\n",
    "    density = np.count_nonzero(A)/num_entries\n",
    "    \n",
    "    sparsity = 1 - density    \n",
    "\n",
    "    # perform factorization, record time\n",
    "    start_time = t.default_timer()\n",
    "    \n",
    "    # perform matrix factorization\n",
    "    U_hat, V_hat = factorize_fn(A, U, V, user_features, epochs, print_divs)\n",
    "\n",
    "    end_time = t.default_timer()\n",
    "    \n",
    "    # factorization result\n",
    "    A_hat = np.dot(U_hat, V_hat.T)\n",
    "    \n",
    "    # get run time\n",
    "    time_elapsed = end_time - start_time\n",
    "    \n",
    "    # get evaluation metrics\n",
    "    mse, rmse = evaluation_metrics(A, A_hat)\n",
    "    \n",
    "    # get predictions\n",
    "    if flag_integer_ratings:\n",
    "        ratings = mat_predict(A, A_hat)\n",
    "    else:\n",
    "        ratings = mat_predict(A, A_hat, conv_to_int=False)\n",
    "    \n",
    "    return {'Ratings': ratings, 'Metrics': (mse, rmse), 'Runtime': time_elapsed, 'Resulting Output': A_hat, 'Other Info': {'Sparsity': sparsity, 'Density': density, 'Input Dimension': (A.shape[0], A.shape[1])}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb405f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log everything out of evaluation_pipeline function so we don't have to write it\n",
    "\n",
    "def call_eval_pipeline(dict_output, flag_output = True):\n",
    "    \n",
    "    predictions, metrics, runtime, A_hat, mat_facts = dict_output['Ratings'], dict_output['Metrics'], dict_output['Runtime'], dict_output['Resulting Output'], dict_output['Other Info']\n",
    "    \n",
    "    mse, rmse = metrics\n",
    "    \n",
    "    # facts about our original matrix (interaction table)\n",
    "    sparsity, input_dim = mat_facts['Sparsity'], mat_facts['Input Dimension']\n",
    "    if flag_output:\n",
    "        print(f'Here is the resulting prediction matrix:\\n{A_hat}\\n')\n",
    "        print(f'Here are the predicted ratings:\\n{predictions}\\n')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(f'The Mean Squared Error is {mse}.\\nThe Root Mean Squared Error is {rmse}.\\n')\n",
    "    print(f'The Runtime of this algorithm of this {input_dim[0]} x {input_dim[1]}, {sparsity*100:.3f}% sparse matrix, is {runtime} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b186a",
   "metadata": {},
   "source": [
    "**Non-Square Matrix Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dad1eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.37454012 0.95071431]\n",
      " [0.73199394 0.59865848]\n",
      " [0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615]\n",
      " [0.60111501 0.70807258]]\n",
      "V is:\n",
      " [[0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911]\n",
      " [0.18182497 0.18340451]\n",
      " [0.30424224 0.52475643]]\n",
      "Epoch 1 : Total Loss 76.548\n",
      "Epoch 11 : Total Loss 75.321\n",
      "Epoch 21 : Total Loss 74.076\n",
      "Epoch 31 : Total Loss 72.817\n",
      "Epoch 41 : Total Loss 71.544\n",
      "Epoch 51 : Total Loss 70.260\n",
      "Epoch 61 : Total Loss 68.968\n",
      "Epoch 71 : Total Loss 67.669\n",
      "Epoch 81 : Total Loss 66.366\n",
      "Epoch 91 : Total Loss 65.062\n",
      "Epoch 101 : Total Loss 63.759\n",
      "Epoch 111 : Total Loss 62.460\n",
      "Epoch 121 : Total Loss 61.169\n",
      "Epoch 131 : Total Loss 59.887\n",
      "Epoch 141 : Total Loss 58.617\n",
      "Epoch 151 : Total Loss 57.363\n",
      "Epoch 161 : Total Loss 56.126\n",
      "Epoch 171 : Total Loss 54.909\n",
      "Epoch 181 : Total Loss 53.715\n",
      "Epoch 191 : Total Loss 52.546\n",
      "Epoch 201 : Total Loss 51.404\n",
      "Epoch 211 : Total Loss 50.290\n",
      "Epoch 221 : Total Loss 49.206\n",
      "Epoch 231 : Total Loss 48.154\n",
      "Epoch 241 : Total Loss 47.135\n",
      "Epoch 251 : Total Loss 46.149\n",
      "Epoch 261 : Total Loss 45.198\n",
      "Epoch 271 : Total Loss 44.282\n",
      "Epoch 281 : Total Loss 43.401\n",
      "Epoch 291 : Total Loss 42.555\n",
      "Epoch 301 : Total Loss 41.744\n",
      "Epoch 311 : Total Loss 40.968\n",
      "Epoch 321 : Total Loss 40.227\n",
      "Epoch 331 : Total Loss 39.519\n",
      "Epoch 341 : Total Loss 38.843\n",
      "Epoch 351 : Total Loss 38.200\n",
      "Epoch 361 : Total Loss 37.588\n",
      "Epoch 371 : Total Loss 37.005\n",
      "Epoch 381 : Total Loss 36.451\n",
      "Epoch 391 : Total Loss 35.925\n",
      "Epoch 401 : Total Loss 35.424\n",
      "Epoch 411 : Total Loss 34.949\n",
      "Epoch 421 : Total Loss 34.497\n",
      "Epoch 431 : Total Loss 34.067\n",
      "Epoch 441 : Total Loss 33.659\n",
      "Epoch 451 : Total Loss 33.270\n",
      "Epoch 461 : Total Loss 32.900\n",
      "Epoch 471 : Total Loss 32.547\n",
      "Epoch 481 : Total Loss 32.211\n",
      "Epoch 491 : Total Loss 31.890\n",
      "Epoch 501 : Total Loss 31.583\n",
      "Epoch 511 : Total Loss 31.290\n",
      "Epoch 521 : Total Loss 31.008\n",
      "Epoch 531 : Total Loss 30.739\n",
      "Epoch 541 : Total Loss 30.479\n",
      "Epoch 551 : Total Loss 30.230\n",
      "Epoch 561 : Total Loss 29.990\n",
      "Epoch 571 : Total Loss 29.758\n",
      "Epoch 581 : Total Loss 29.534\n",
      "Epoch 591 : Total Loss 29.317\n",
      "Epoch 601 : Total Loss 29.107\n",
      "Epoch 611 : Total Loss 28.904\n",
      "Epoch 621 : Total Loss 28.706\n",
      "Epoch 631 : Total Loss 28.513\n",
      "Epoch 641 : Total Loss 28.326\n",
      "Epoch 651 : Total Loss 28.143\n",
      "Epoch 661 : Total Loss 27.965\n",
      "Epoch 671 : Total Loss 27.791\n",
      "Epoch 681 : Total Loss 27.620\n",
      "Epoch 691 : Total Loss 27.454\n",
      "Epoch 701 : Total Loss 27.290\n",
      "Epoch 711 : Total Loss 27.130\n",
      "Epoch 721 : Total Loss 26.973\n",
      "Epoch 731 : Total Loss 26.819\n",
      "Epoch 741 : Total Loss 26.667\n",
      "Epoch 751 : Total Loss 26.518\n",
      "Epoch 761 : Total Loss 26.372\n",
      "Epoch 771 : Total Loss 26.228\n",
      "Epoch 781 : Total Loss 26.086\n",
      "Epoch 791 : Total Loss 25.947\n",
      "Epoch 801 : Total Loss 25.809\n",
      "Epoch 811 : Total Loss 25.674\n",
      "Epoch 821 : Total Loss 25.541\n",
      "Epoch 831 : Total Loss 25.409\n",
      "Epoch 841 : Total Loss 25.280\n",
      "Epoch 851 : Total Loss 25.152\n",
      "Epoch 861 : Total Loss 25.027\n",
      "Epoch 871 : Total Loss 24.903\n",
      "Epoch 881 : Total Loss 24.780\n",
      "Epoch 891 : Total Loss 24.660\n",
      "Epoch 901 : Total Loss 24.541\n",
      "Epoch 911 : Total Loss 24.423\n",
      "Epoch 921 : Total Loss 24.308\n",
      "Epoch 931 : Total Loss 24.194\n",
      "Epoch 941 : Total Loss 24.081\n",
      "Epoch 951 : Total Loss 23.970\n",
      "Epoch 961 : Total Loss 23.861\n",
      "Epoch 971 : Total Loss 23.753\n",
      "Epoch 981 : Total Loss 23.646\n",
      "Epoch 991 : Total Loss 23.541\n",
      "Epoch 1001 : Total Loss 23.438\n",
      "Epoch 1011 : Total Loss 23.335\n",
      "Epoch 1021 : Total Loss 23.235\n",
      "Epoch 1031 : Total Loss 23.135\n",
      "Epoch 1041 : Total Loss 23.037\n",
      "Epoch 1051 : Total Loss 22.941\n",
      "Epoch 1061 : Total Loss 22.845\n",
      "Epoch 1071 : Total Loss 22.751\n",
      "Epoch 1081 : Total Loss 22.658\n",
      "Epoch 1091 : Total Loss 22.567\n",
      "Epoch 1101 : Total Loss 22.477\n",
      "Epoch 1111 : Total Loss 22.388\n",
      "Epoch 1121 : Total Loss 22.300\n",
      "Epoch 1131 : Total Loss 22.213\n",
      "Epoch 1141 : Total Loss 22.127\n",
      "Epoch 1151 : Total Loss 22.043\n",
      "Epoch 1161 : Total Loss 21.960\n",
      "Epoch 1171 : Total Loss 21.877\n",
      "Epoch 1181 : Total Loss 21.796\n",
      "Epoch 1191 : Total Loss 21.716\n",
      "Epoch 1201 : Total Loss 21.637\n",
      "Epoch 1211 : Total Loss 21.559\n",
      "Epoch 1221 : Total Loss 21.481\n",
      "Epoch 1231 : Total Loss 21.405\n",
      "Epoch 1241 : Total Loss 21.330\n",
      "Epoch 1251 : Total Loss 21.255\n",
      "Epoch 1261 : Total Loss 21.181\n",
      "Epoch 1271 : Total Loss 21.108\n",
      "Epoch 1281 : Total Loss 21.036\n",
      "Epoch 1291 : Total Loss 20.965\n",
      "Epoch 1301 : Total Loss 20.894\n",
      "Epoch 1311 : Total Loss 20.824\n",
      "Epoch 1321 : Total Loss 20.755\n",
      "Epoch 1331 : Total Loss 20.686\n",
      "Epoch 1341 : Total Loss 20.618\n",
      "Epoch 1351 : Total Loss 20.551\n",
      "Epoch 1361 : Total Loss 20.484\n",
      "Epoch 1371 : Total Loss 20.417\n",
      "Epoch 1381 : Total Loss 20.351\n",
      "Epoch 1391 : Total Loss 20.286\n",
      "Epoch 1401 : Total Loss 20.221\n",
      "Epoch 1411 : Total Loss 20.156\n",
      "Epoch 1421 : Total Loss 20.092\n",
      "Epoch 1431 : Total Loss 20.027\n",
      "Epoch 1441 : Total Loss 19.964\n",
      "Epoch 1451 : Total Loss 19.900\n",
      "Epoch 1461 : Total Loss 19.837\n",
      "Epoch 1471 : Total Loss 19.774\n",
      "Epoch 1481 : Total Loss 19.711\n",
      "Epoch 1491 : Total Loss 19.648\n",
      "Epoch 1501 : Total Loss 19.585\n",
      "Epoch 1511 : Total Loss 19.522\n",
      "Epoch 1521 : Total Loss 19.459\n",
      "Epoch 1531 : Total Loss 19.397\n",
      "Epoch 1541 : Total Loss 19.334\n",
      "Epoch 1551 : Total Loss 19.271\n",
      "Epoch 1561 : Total Loss 19.208\n",
      "Epoch 1571 : Total Loss 19.144\n",
      "Epoch 1581 : Total Loss 19.081\n",
      "Epoch 1591 : Total Loss 19.017\n",
      "Epoch 1601 : Total Loss 18.953\n",
      "Epoch 1611 : Total Loss 18.889\n",
      "Epoch 1621 : Total Loss 18.824\n",
      "Epoch 1631 : Total Loss 18.759\n",
      "Epoch 1641 : Total Loss 18.693\n",
      "Epoch 1651 : Total Loss 18.627\n",
      "Epoch 1661 : Total Loss 18.561\n",
      "Epoch 1671 : Total Loss 18.494\n",
      "Epoch 1681 : Total Loss 18.426\n",
      "Epoch 1691 : Total Loss 18.358\n",
      "Epoch 1701 : Total Loss 18.289\n",
      "Epoch 1711 : Total Loss 18.219\n",
      "Epoch 1721 : Total Loss 18.149\n",
      "Epoch 1731 : Total Loss 18.078\n",
      "Epoch 1741 : Total Loss 18.006\n",
      "Epoch 1751 : Total Loss 17.933\n",
      "Epoch 1761 : Total Loss 17.860\n",
      "Epoch 1771 : Total Loss 17.785\n",
      "Epoch 1781 : Total Loss 17.710\n",
      "Epoch 1791 : Total Loss 17.633\n",
      "Epoch 1801 : Total Loss 17.555\n",
      "Epoch 1811 : Total Loss 17.477\n",
      "Epoch 1821 : Total Loss 17.397\n",
      "Epoch 1831 : Total Loss 17.316\n",
      "Epoch 1841 : Total Loss 17.234\n",
      "Epoch 1851 : Total Loss 17.151\n",
      "Epoch 1861 : Total Loss 17.066\n",
      "Epoch 1871 : Total Loss 16.980\n",
      "Epoch 1881 : Total Loss 16.893\n",
      "Epoch 1891 : Total Loss 16.804\n",
      "Epoch 1901 : Total Loss 16.714\n",
      "Epoch 1911 : Total Loss 16.623\n",
      "Epoch 1921 : Total Loss 16.530\n",
      "Epoch 1931 : Total Loss 16.435\n",
      "Epoch 1941 : Total Loss 16.340\n",
      "Epoch 1951 : Total Loss 16.242\n",
      "Epoch 1961 : Total Loss 16.143\n",
      "Epoch 1971 : Total Loss 16.042\n",
      "Epoch 1981 : Total Loss 15.940\n",
      "Epoch 1991 : Total Loss 15.836\n",
      "Epoch 2001 : Total Loss 15.730\n",
      "Epoch 2011 : Total Loss 15.623\n",
      "Epoch 2021 : Total Loss 15.513\n",
      "Epoch 2031 : Total Loss 15.403\n",
      "Epoch 2041 : Total Loss 15.290\n",
      "Epoch 2051 : Total Loss 15.175\n",
      "Epoch 2061 : Total Loss 15.059\n",
      "Epoch 2071 : Total Loss 14.941\n",
      "Epoch 2081 : Total Loss 14.821\n",
      "Epoch 2091 : Total Loss 14.699\n",
      "Epoch 2101 : Total Loss 14.575\n",
      "Epoch 2111 : Total Loss 14.450\n",
      "Epoch 2121 : Total Loss 14.323\n",
      "Epoch 2131 : Total Loss 14.194\n",
      "Epoch 2141 : Total Loss 14.063\n",
      "Epoch 2151 : Total Loss 13.930\n",
      "Epoch 2161 : Total Loss 13.795\n",
      "Epoch 2171 : Total Loss 13.659\n",
      "Epoch 2181 : Total Loss 13.521\n",
      "Epoch 2191 : Total Loss 13.381\n",
      "Epoch 2201 : Total Loss 13.239\n",
      "Epoch 2211 : Total Loss 13.096\n",
      "Epoch 2221 : Total Loss 12.951\n",
      "Epoch 2231 : Total Loss 12.804\n",
      "Epoch 2241 : Total Loss 12.656\n",
      "Epoch 2251 : Total Loss 12.506\n",
      "Epoch 2261 : Total Loss 12.355\n",
      "Epoch 2271 : Total Loss 12.202\n",
      "Epoch 2281 : Total Loss 12.048\n",
      "Epoch 2291 : Total Loss 11.892\n",
      "Epoch 2301 : Total Loss 11.736\n",
      "Epoch 2311 : Total Loss 11.578\n",
      "Epoch 2321 : Total Loss 11.419\n",
      "Epoch 2331 : Total Loss 11.259\n",
      "Epoch 2341 : Total Loss 11.097\n",
      "Epoch 2351 : Total Loss 10.935\n",
      "Epoch 2361 : Total Loss 10.772\n",
      "Epoch 2371 : Total Loss 10.609\n",
      "Epoch 2381 : Total Loss 10.445\n",
      "Epoch 2391 : Total Loss 10.280\n",
      "Epoch 2401 : Total Loss 10.114\n",
      "Epoch 2411 : Total Loss 9.949\n",
      "Epoch 2421 : Total Loss 9.783\n",
      "Epoch 2431 : Total Loss 9.616\n",
      "Epoch 2441 : Total Loss 9.450\n",
      "Epoch 2451 : Total Loss 9.284\n",
      "Epoch 2461 : Total Loss 9.118\n",
      "Epoch 2471 : Total Loss 8.952\n",
      "Epoch 2481 : Total Loss 8.787\n",
      "Epoch 2491 : Total Loss 8.622\n",
      "Epoch 2501 : Total Loss 8.457\n",
      "Epoch 2511 : Total Loss 8.294\n",
      "Epoch 2521 : Total Loss 8.131\n",
      "Epoch 2531 : Total Loss 7.969\n",
      "Epoch 2541 : Total Loss 7.807\n",
      "Epoch 2551 : Total Loss 7.648\n",
      "Epoch 2561 : Total Loss 7.489\n",
      "Epoch 2571 : Total Loss 7.331\n",
      "Epoch 2581 : Total Loss 7.175\n",
      "Epoch 2591 : Total Loss 7.021\n",
      "Epoch 2601 : Total Loss 6.868\n",
      "Epoch 2611 : Total Loss 6.717\n",
      "Epoch 2621 : Total Loss 6.568\n",
      "Epoch 2631 : Total Loss 6.420\n",
      "Epoch 2641 : Total Loss 6.275\n",
      "Epoch 2651 : Total Loss 6.131\n",
      "Epoch 2661 : Total Loss 5.990\n",
      "Epoch 2671 : Total Loss 5.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2681 : Total Loss 5.714\n",
      "Epoch 2691 : Total Loss 5.579\n",
      "Epoch 2701 : Total Loss 5.447\n",
      "Epoch 2711 : Total Loss 5.317\n",
      "Epoch 2721 : Total Loss 5.190\n",
      "Epoch 2731 : Total Loss 5.065\n",
      "Epoch 2741 : Total Loss 4.943\n",
      "Epoch 2751 : Total Loss 4.823\n",
      "Epoch 2761 : Total Loss 4.706\n",
      "Epoch 2771 : Total Loss 4.592\n",
      "Epoch 2781 : Total Loss 4.480\n",
      "Epoch 2791 : Total Loss 4.371\n",
      "Epoch 2801 : Total Loss 4.265\n",
      "Epoch 2811 : Total Loss 4.161\n",
      "Epoch 2821 : Total Loss 4.060\n",
      "Epoch 2831 : Total Loss 3.962\n",
      "Epoch 2841 : Total Loss 3.866\n",
      "Epoch 2851 : Total Loss 3.773\n",
      "Epoch 2861 : Total Loss 3.683\n",
      "Epoch 2871 : Total Loss 3.595\n",
      "Epoch 2881 : Total Loss 3.510\n",
      "Epoch 2891 : Total Loss 3.428\n",
      "Epoch 2901 : Total Loss 3.348\n",
      "Epoch 2911 : Total Loss 3.270\n",
      "Epoch 2921 : Total Loss 3.196\n",
      "Epoch 2931 : Total Loss 3.123\n",
      "Epoch 2941 : Total Loss 3.053\n",
      "Epoch 2951 : Total Loss 2.985\n",
      "Epoch 2961 : Total Loss 2.920\n",
      "Epoch 2971 : Total Loss 2.857\n",
      "Epoch 2981 : Total Loss 2.796\n",
      "Epoch 2991 : Total Loss 2.738\n",
      "Epoch 3001 : Total Loss 2.681\n",
      "Epoch 3011 : Total Loss 2.627\n",
      "Epoch 3021 : Total Loss 2.574\n",
      "Epoch 3031 : Total Loss 2.524\n",
      "Epoch 3041 : Total Loss 2.475\n",
      "Epoch 3051 : Total Loss 2.429\n",
      "Epoch 3061 : Total Loss 2.384\n",
      "Epoch 3071 : Total Loss 2.341\n",
      "Epoch 3081 : Total Loss 2.299\n",
      "Epoch 3091 : Total Loss 2.260\n",
      "Epoch 3101 : Total Loss 2.221\n",
      "Epoch 3111 : Total Loss 2.185\n",
      "Epoch 3121 : Total Loss 2.150\n",
      "Epoch 3131 : Total Loss 2.116\n",
      "Epoch 3141 : Total Loss 2.084\n",
      "Epoch 3151 : Total Loss 2.053\n",
      "Epoch 3161 : Total Loss 2.023\n",
      "Epoch 3171 : Total Loss 1.995\n",
      "Epoch 3181 : Total Loss 1.968\n",
      "Epoch 3191 : Total Loss 1.942\n",
      "Epoch 3201 : Total Loss 1.917\n",
      "Epoch 3211 : Total Loss 1.893\n",
      "Epoch 3221 : Total Loss 1.870\n",
      "Epoch 3231 : Total Loss 1.849\n",
      "Epoch 3241 : Total Loss 1.828\n",
      "Epoch 3251 : Total Loss 1.808\n",
      "Epoch 3261 : Total Loss 1.789\n",
      "Epoch 3271 : Total Loss 1.770\n",
      "Epoch 3281 : Total Loss 1.753\n",
      "Epoch 3291 : Total Loss 1.736\n",
      "Epoch 3301 : Total Loss 1.720\n",
      "Epoch 3311 : Total Loss 1.705\n",
      "Epoch 3321 : Total Loss 1.690\n",
      "Epoch 3331 : Total Loss 1.676\n",
      "Epoch 3341 : Total Loss 1.663\n",
      "Epoch 3351 : Total Loss 1.650\n",
      "Epoch 3361 : Total Loss 1.637\n",
      "Epoch 3371 : Total Loss 1.626\n",
      "Epoch 3381 : Total Loss 1.614\n",
      "Epoch 3391 : Total Loss 1.603\n",
      "Epoch 3401 : Total Loss 1.593\n",
      "Epoch 3411 : Total Loss 1.583\n",
      "Epoch 3421 : Total Loss 1.574\n",
      "Epoch 3431 : Total Loss 1.565\n",
      "Epoch 3441 : Total Loss 1.556\n",
      "Epoch 3451 : Total Loss 1.547\n",
      "Epoch 3461 : Total Loss 1.539\n",
      "Epoch 3471 : Total Loss 1.532\n",
      "Epoch 3481 : Total Loss 1.524\n",
      "Epoch 3491 : Total Loss 1.517\n",
      "Epoch 3501 : Total Loss 1.510\n",
      "Epoch 3511 : Total Loss 1.503\n",
      "Epoch 3521 : Total Loss 1.497\n",
      "Epoch 3531 : Total Loss 1.491\n",
      "Epoch 3541 : Total Loss 1.485\n",
      "Epoch 3551 : Total Loss 1.479\n",
      "Epoch 3561 : Total Loss 1.474\n",
      "Epoch 3571 : Total Loss 1.468\n",
      "Epoch 3581 : Total Loss 1.463\n",
      "Epoch 3591 : Total Loss 1.458\n",
      "Epoch 3601 : Total Loss 1.453\n",
      "Epoch 3611 : Total Loss 1.449\n",
      "Epoch 3621 : Total Loss 1.444\n",
      "Epoch 3631 : Total Loss 1.440\n",
      "Epoch 3641 : Total Loss 1.436\n",
      "Epoch 3651 : Total Loss 1.431\n",
      "Epoch 3661 : Total Loss 1.427\n",
      "Epoch 3671 : Total Loss 1.424\n",
      "Epoch 3681 : Total Loss 1.420\n",
      "Epoch 3691 : Total Loss 1.416\n",
      "Epoch 3701 : Total Loss 1.412\n",
      "Epoch 3711 : Total Loss 1.409\n",
      "Epoch 3721 : Total Loss 1.406\n",
      "Epoch 3731 : Total Loss 1.402\n",
      "Epoch 3741 : Total Loss 1.399\n",
      "Epoch 3751 : Total Loss 1.396\n",
      "Epoch 3761 : Total Loss 1.393\n",
      "Epoch 3771 : Total Loss 1.390\n",
      "Epoch 3781 : Total Loss 1.387\n",
      "Epoch 3791 : Total Loss 1.384\n",
      "Epoch 3801 : Total Loss 1.381\n",
      "Epoch 3811 : Total Loss 1.378\n",
      "Epoch 3821 : Total Loss 1.376\n",
      "Epoch 3831 : Total Loss 1.373\n",
      "Epoch 3841 : Total Loss 1.370\n",
      "Epoch 3851 : Total Loss 1.368\n",
      "Epoch 3861 : Total Loss 1.365\n",
      "Epoch 3871 : Total Loss 1.363\n",
      "Epoch 3881 : Total Loss 1.360\n",
      "Epoch 3891 : Total Loss 1.358\n",
      "Epoch 3901 : Total Loss 1.355\n",
      "Epoch 3911 : Total Loss 1.353\n",
      "Epoch 3921 : Total Loss 1.351\n",
      "Epoch 3931 : Total Loss 1.348\n",
      "Epoch 3941 : Total Loss 1.346\n",
      "Epoch 3951 : Total Loss 1.344\n",
      "Epoch 3961 : Total Loss 1.342\n",
      "Epoch 3971 : Total Loss 1.340\n",
      "Epoch 3981 : Total Loss 1.338\n",
      "Epoch 3991 : Total Loss 1.335\n",
      "Epoch 4001 : Total Loss 1.333\n",
      "Epoch 4011 : Total Loss 1.331\n",
      "Epoch 4021 : Total Loss 1.329\n",
      "Epoch 4031 : Total Loss 1.327\n",
      "Epoch 4041 : Total Loss 1.325\n",
      "Epoch 4051 : Total Loss 1.323\n",
      "Epoch 4061 : Total Loss 1.321\n",
      "Epoch 4071 : Total Loss 1.319\n",
      "Epoch 4081 : Total Loss 1.317\n",
      "Epoch 4091 : Total Loss 1.316\n",
      "Epoch 4101 : Total Loss 1.314\n",
      "Epoch 4111 : Total Loss 1.312\n",
      "Epoch 4121 : Total Loss 1.310\n",
      "Epoch 4131 : Total Loss 1.308\n",
      "Epoch 4141 : Total Loss 1.306\n",
      "Epoch 4151 : Total Loss 1.305\n",
      "Epoch 4161 : Total Loss 1.303\n",
      "Epoch 4171 : Total Loss 1.301\n",
      "Epoch 4181 : Total Loss 1.299\n",
      "Epoch 4191 : Total Loss 1.298\n",
      "Epoch 4201 : Total Loss 1.296\n",
      "Epoch 4211 : Total Loss 1.294\n",
      "Epoch 4221 : Total Loss 1.292\n",
      "Epoch 4231 : Total Loss 1.291\n",
      "Epoch 4241 : Total Loss 1.289\n",
      "Epoch 4251 : Total Loss 1.287\n",
      "Epoch 4261 : Total Loss 1.286\n",
      "Epoch 4271 : Total Loss 1.284\n",
      "Epoch 4281 : Total Loss 1.282\n",
      "Epoch 4291 : Total Loss 1.281\n",
      "Epoch 4301 : Total Loss 1.279\n",
      "Epoch 4311 : Total Loss 1.278\n",
      "Epoch 4321 : Total Loss 1.276\n",
      "Epoch 4331 : Total Loss 1.274\n",
      "Epoch 4341 : Total Loss 1.273\n",
      "Epoch 4351 : Total Loss 1.271\n",
      "Epoch 4361 : Total Loss 1.270\n",
      "Epoch 4371 : Total Loss 1.268\n",
      "Epoch 4381 : Total Loss 1.267\n",
      "Epoch 4391 : Total Loss 1.265\n",
      "Epoch 4401 : Total Loss 1.264\n",
      "Epoch 4411 : Total Loss 1.262\n",
      "Epoch 4421 : Total Loss 1.261\n",
      "Epoch 4431 : Total Loss 1.259\n",
      "Epoch 4441 : Total Loss 1.258\n",
      "Epoch 4451 : Total Loss 1.256\n",
      "Epoch 4461 : Total Loss 1.255\n",
      "Epoch 4471 : Total Loss 1.253\n",
      "Epoch 4481 : Total Loss 1.252\n",
      "Epoch 4491 : Total Loss 1.250\n",
      "Epoch 4501 : Total Loss 1.249\n",
      "Epoch 4511 : Total Loss 1.248\n",
      "Epoch 4521 : Total Loss 1.246\n",
      "Epoch 4531 : Total Loss 1.245\n",
      "Epoch 4541 : Total Loss 1.243\n",
      "Epoch 4551 : Total Loss 1.242\n",
      "Epoch 4561 : Total Loss 1.241\n",
      "Epoch 4571 : Total Loss 1.239\n",
      "Epoch 4581 : Total Loss 1.238\n",
      "Epoch 4591 : Total Loss 1.237\n",
      "Epoch 4601 : Total Loss 1.235\n",
      "Epoch 4611 : Total Loss 1.234\n",
      "Epoch 4621 : Total Loss 1.233\n",
      "Epoch 4631 : Total Loss 1.231\n",
      "Epoch 4641 : Total Loss 1.230\n",
      "Epoch 4651 : Total Loss 1.229\n",
      "Epoch 4661 : Total Loss 1.227\n",
      "Epoch 4671 : Total Loss 1.226\n",
      "Epoch 4681 : Total Loss 1.225\n",
      "Epoch 4691 : Total Loss 1.223\n",
      "Epoch 4701 : Total Loss 1.222\n",
      "Epoch 4711 : Total Loss 1.221\n",
      "Epoch 4721 : Total Loss 1.220\n",
      "Epoch 4731 : Total Loss 1.218\n",
      "Epoch 4741 : Total Loss 1.217\n",
      "Epoch 4751 : Total Loss 1.216\n",
      "Epoch 4761 : Total Loss 1.215\n",
      "Epoch 4771 : Total Loss 1.213\n",
      "Epoch 4781 : Total Loss 1.212\n",
      "Epoch 4791 : Total Loss 1.211\n",
      "Epoch 4801 : Total Loss 1.210\n",
      "Epoch 4811 : Total Loss 1.209\n",
      "Epoch 4821 : Total Loss 1.207\n",
      "Epoch 4831 : Total Loss 1.206\n",
      "Epoch 4841 : Total Loss 1.205\n",
      "Epoch 4851 : Total Loss 1.204\n",
      "Epoch 4861 : Total Loss 1.203\n",
      "Epoch 4871 : Total Loss 1.202\n",
      "Epoch 4881 : Total Loss 1.200\n",
      "Epoch 4891 : Total Loss 1.199\n",
      "Epoch 4901 : Total Loss 1.198\n",
      "Epoch 4911 : Total Loss 1.197\n",
      "Epoch 4921 : Total Loss 1.196\n",
      "Epoch 4931 : Total Loss 1.195\n",
      "Epoch 4941 : Total Loss 1.194\n",
      "Epoch 4951 : Total Loss 1.193\n",
      "Epoch 4961 : Total Loss 1.191\n",
      "Epoch 4971 : Total Loss 1.190\n",
      "Epoch 4981 : Total Loss 1.189\n",
      "Epoch 4991 : Total Loss 1.188\n",
      "Here is the resulting prediction matrix:\n",
      "[[4.96418223 2.90834894 6.19016767 0.99748207]\n",
      " [3.08715043 1.75046476 4.36609466 1.94134099]\n",
      " [2.58073845 1.44436075 3.8181746  2.05328838]\n",
      " [0.98448624 0.36665428 3.09265512 4.96770121]\n",
      " [3.19921179 1.77146363 4.90217708 2.97752458]]\n",
      "\n",
      "Here are the predicted ratings:\n",
      "[6.190167665816127, 4.366094663071692, 1.9413409876705892, 2.5807384453452413, 0.36665427524652583, 3.0926551205739212, 3.199211788907813, 1.7714636276896636, 4.902177081911962]\n",
      "\n",
      "The Mean Squared Error is 0.02860889466415564.\n",
      "The Root Mean Squared Error is 0.1691416408344073.\n",
      "\n",
      "The Runtime of this algorithm of this 5 x 4, 45.000% sparse matrix, is 1.722647099988535 seconds.\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [5,3,0,1],\n",
    "    [3,2,0,0],\n",
    "    [0,1,4,2],\n",
    "    [1,0,0,5],\n",
    "    [0,0,0,3]\n",
    "])\n",
    "\n",
    "m, n = A.shape[0], A.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "U_1 = np.random.rand(m, user_features)\n",
    "V_1 = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U_1)\n",
    "print('V is:\\n', V_1)\n",
    "\n",
    "# perform factorization\n",
    "\n",
    "dict_small_nonsq = evaluation_pipeline(A, U_1, V_1, 5000, 2, matrix_factorization)\n",
    "\n",
    "call_eval_pipeline(dict_small_nonsq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a0fb2",
   "metadata": {},
   "source": [
    "**Try a Large Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b3a5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "# A = np.random.randint(0, 6, size=(100,100))\n",
    "\n",
    "# m, n = A.shape[0], A.shape[1]\n",
    "\n",
    "# user_features = 2\n",
    "\n",
    "# U_2 = np.random.rand(m, user_features)\n",
    "# V_2 = np.random.rand(n, user_features)\n",
    "\n",
    "# # check entries of matrices\n",
    "\n",
    "# print('U is:\\n', U_2)\n",
    "# print('V is:\\n', V_2)\n",
    "\n",
    "# # perform factorization\n",
    "\n",
    "# U_hat, V_hat = matrix_factorization(A, U_2, V_2, user_features, 5000)\n",
    "\n",
    "# A_hat = np.dot(U_hat, V_hat.T)\n",
    "\n",
    "# print('U_hat is:\\n', U_hat)\n",
    "# print('V_hat is:\\n', V_hat)\n",
    "\n",
    "# ## check if the product results in something close to A\n",
    "# print('A is:\\n', A)\n",
    "# print('A_hat is:\\n', A_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14026e",
   "metadata": {},
   "source": [
    "**Note: This takes a really long time to train and evaluate. It took over 15 minutes to train and evaluate a 100 by 100 matrix with 2 latent features. Most datasets that we will put in in a business situation will be WELL over that amount so this current implementation is not feasible for large datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a781c",
   "metadata": {},
   "source": [
    "## Matrix Factorization Implementation for Sparse Matrices\n",
    "\n",
    "We will investigate how to perform matrix factorization for sparse matrices, as most matrices we will work with for the purpose of matrix factorization are large and sparse (if it were large but not sparse, we would have most of our information anyways, so this technique wouldn't be super useful).\n",
    "\n",
    "### Sparse Matrices\n",
    "\n",
    "The definition of a sparse matrix is very imprecise, but for our purposes, we define a sparse matrix as any matrix that stores a 0 (or any other value that represents the absence of meaningful data) in at most $\\max(m,n)$ entries in an $m\\times n$ matrix.\n",
    "\n",
    "We can use scipy's csr_matrix class to get the nonzero elements of a sparse matrix. Let us use a toy example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbfc2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7136888b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 5, 0, 0, 1, 0],\n",
       "       [0, 3, 0, 0, 0, 0, 1],\n",
       "       [4, 0, 0, 2, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0,1,0,0,0,0,0],\n",
    "    [0,0,5,0,0,1,0],\n",
    "    [0,3,0,0,0,0,1],\n",
    "    [4,0,0,2,0,0,0]    \n",
    "          ])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c39f0e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t5\n",
      "  (0, 5)\t1\n",
      "  (0, 1)\t3\n",
      "  (0, 6)\t1\n",
      "  (0, 0)\t4\n",
      "  (0, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "# print items in csr_matrix\n",
    "\n",
    "for item in csr_matrix(x):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a380e",
   "metadata": {},
   "source": [
    "So a csr_matrix object only stores nonzero elements (the meaningful data entries) in memory!\n",
    "\n",
    "This is important for our matrix factorization algorithm implementation here, because recall that **our loss computation only involves NONZERO elements** anyways! So we do not need any information about the matrix with the 0 entries!\n",
    "\n",
    "We can pick out the nonzero indices of the matrix with **csr_matrix's nonzero method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8189af01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 2, 2, 3, 3], dtype=int32),\n",
       " array([1, 2, 5, 1, 6, 0, 3], dtype=int32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix(x).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc764a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (1, 5), (2, 1), (2, 6), (3, 0), (3, 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index the nonzero elements through extracting the indices\n",
    "li_ind = []\n",
    "\n",
    "for x_ind, y_ind in zip(csr_matrix(x).nonzero()[0], csr_matrix(x).nonzero()[1]):\n",
    "    li_ind.append((x_ind, y_ind))\n",
    "\n",
    "li_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b3e2909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# access all elements of our original array\n",
    "for i, j in zip(csr_matrix(x).nonzero()[0], csr_matrix(x).nonzero()[1]):\n",
    "    print(x[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a93b2d",
   "metadata": {},
   "source": [
    "### Matrix Factorization 2: Matrix Factorization for Sparse Matrices\n",
    "\n",
    "We will be implementing the matrix factorization algorithm for sparse matrices here. We will see that this is a significant optimization over the first implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7c7d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is different from the previous one because we do not need to traverse each array separately\n",
    "# we isolate and extract all of the nonzero entries from the matrix in linear time (at worst O(n) for ground truth mxn)\n",
    "# and then we only loop through epochs and the number of latent features\n",
    "\n",
    "# ARGUMENTS\n",
    "# A - ground truth matrix (sparse numpy array)\n",
    "# U - product 1\n",
    "# V - product 2\n",
    "# number of features\n",
    "# epochs - number of epochs to train\n",
    "# lambda - learning rate\n",
    "# beta - regularization parameter\n",
    "\n",
    "def matrix_factorization_sparse(A, U, V, k, steps, lmbda=0.0002, beta = 0.02):\n",
    "    # transpose V to make matrix multiplication work\n",
    "    V = V.T\n",
    "    \n",
    "    # convert A, a sparse numpy array, into a sparse scipy matrix object\n",
    "    sparse_A = csr_matrix(A)\n",
    "    \n",
    "    for epoch in range(steps):\n",
    "        # initialize total loss\n",
    "        e = 0\n",
    "        for i, j in zip(sparse_A.nonzero()[0], sparse_A.nonzero()[1]):\n",
    "            # component-wise error\n",
    "            eij = A[i,j] - np.dot(U[i,:], V[:,j])\n",
    "            # total error\n",
    "            e += (eij**2)\n",
    "            \n",
    "            for K in range(k):\n",
    "                # gradient descent step for each entry, with regularization term\n",
    "                U[i,K] = U[i,K] + lmbda * (2 * eij * V[K,j] - beta*U[i,K])\n",
    "                V[K,j] = V[K,j] + lmbda * (2 * eij * U[i,K] - beta*V[K,j])\n",
    "                \n",
    "                # sum up total loss with regularization terms\n",
    "                e += 0.5 * beta * (U[i][K]**2 + V[K][j]**2)\n",
    "\n",
    "        # give print output of training (print only every 10 epochs)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} : Total Loss {e:.3f}')\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return U, V.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf98739",
   "metadata": {},
   "source": [
    "### Test on a Small Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "716eab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 99.448\n",
      "Epoch 11 : Total Loss 96.944\n",
      "Epoch 21 : Total Loss 94.384\n",
      "Epoch 31 : Total Loss 91.770\n",
      "Epoch 41 : Total Loss 89.107\n",
      "Epoch 51 : Total Loss 86.400\n",
      "Epoch 61 : Total Loss 83.655\n",
      "Epoch 71 : Total Loss 80.879\n",
      "Epoch 81 : Total Loss 78.078\n",
      "Epoch 91 : Total Loss 75.261\n",
      "Epoch 101 : Total Loss 72.436\n",
      "Epoch 111 : Total Loss 69.612\n",
      "Epoch 121 : Total Loss 66.798\n",
      "Epoch 131 : Total Loss 64.003\n",
      "Epoch 141 : Total Loss 61.238\n",
      "Epoch 151 : Total Loss 58.510\n",
      "Epoch 161 : Total Loss 55.830\n",
      "Epoch 171 : Total Loss 53.205\n",
      "Epoch 181 : Total Loss 50.644\n",
      "Epoch 191 : Total Loss 48.155\n",
      "Epoch 201 : Total Loss 45.745\n",
      "Epoch 211 : Total Loss 43.419\n",
      "Epoch 221 : Total Loss 41.183\n",
      "Epoch 231 : Total Loss 39.040\n",
      "Epoch 241 : Total Loss 36.995\n",
      "Epoch 251 : Total Loss 35.049\n",
      "Epoch 261 : Total Loss 33.205\n",
      "Epoch 271 : Total Loss 31.461\n",
      "Epoch 281 : Total Loss 29.818\n",
      "Epoch 291 : Total Loss 28.275\n",
      "Epoch 301 : Total Loss 26.829\n",
      "Epoch 311 : Total Loss 25.479\n",
      "Epoch 321 : Total Loss 24.220\n",
      "Epoch 331 : Total Loss 23.050\n",
      "Epoch 341 : Total Loss 21.965\n",
      "Epoch 351 : Total Loss 20.960\n",
      "Epoch 361 : Total Loss 20.032\n",
      "Epoch 371 : Total Loss 19.175\n",
      "Epoch 381 : Total Loss 18.385\n",
      "Epoch 391 : Total Loss 17.658\n",
      "Epoch 401 : Total Loss 16.989\n",
      "Epoch 411 : Total Loss 16.375\n",
      "Epoch 421 : Total Loss 15.810\n",
      "Epoch 431 : Total Loss 15.292\n",
      "Epoch 441 : Total Loss 14.816\n",
      "Epoch 451 : Total Loss 14.379\n",
      "Epoch 461 : Total Loss 13.977\n",
      "Epoch 471 : Total Loss 13.608\n",
      "Epoch 481 : Total Loss 13.269\n",
      "Epoch 491 : Total Loss 12.956\n",
      "Epoch 501 : Total Loss 12.668\n",
      "Epoch 511 : Total Loss 12.402\n",
      "Epoch 521 : Total Loss 12.156\n",
      "Epoch 531 : Total Loss 11.929\n",
      "Epoch 541 : Total Loss 11.718\n",
      "Epoch 551 : Total Loss 11.523\n",
      "Epoch 561 : Total Loss 11.341\n",
      "Epoch 571 : Total Loss 11.171\n",
      "Epoch 581 : Total Loss 11.013\n",
      "Epoch 591 : Total Loss 10.865\n",
      "Epoch 601 : Total Loss 10.726\n",
      "Epoch 611 : Total Loss 10.595\n",
      "Epoch 621 : Total Loss 10.472\n",
      "Epoch 631 : Total Loss 10.356\n",
      "Epoch 641 : Total Loss 10.247\n",
      "Epoch 651 : Total Loss 10.143\n",
      "Epoch 661 : Total Loss 10.045\n",
      "Epoch 671 : Total Loss 9.951\n",
      "Epoch 681 : Total Loss 9.862\n",
      "Epoch 691 : Total Loss 9.777\n",
      "Epoch 701 : Total Loss 9.695\n",
      "Epoch 711 : Total Loss 9.617\n",
      "Epoch 721 : Total Loss 9.542\n",
      "Epoch 731 : Total Loss 9.470\n",
      "Epoch 741 : Total Loss 9.401\n",
      "Epoch 751 : Total Loss 9.334\n",
      "Epoch 761 : Total Loss 9.269\n",
      "Epoch 771 : Total Loss 9.206\n",
      "Epoch 781 : Total Loss 9.146\n",
      "Epoch 791 : Total Loss 9.087\n",
      "Epoch 801 : Total Loss 9.030\n",
      "Epoch 811 : Total Loss 8.974\n",
      "Epoch 821 : Total Loss 8.920\n",
      "Epoch 831 : Total Loss 8.868\n",
      "Epoch 841 : Total Loss 8.816\n",
      "Epoch 851 : Total Loss 8.766\n",
      "Epoch 861 : Total Loss 8.717\n",
      "Epoch 871 : Total Loss 8.669\n",
      "Epoch 881 : Total Loss 8.622\n",
      "Epoch 891 : Total Loss 8.575\n",
      "Epoch 901 : Total Loss 8.530\n",
      "Epoch 911 : Total Loss 8.486\n",
      "Epoch 921 : Total Loss 8.442\n",
      "Epoch 931 : Total Loss 8.399\n",
      "Epoch 941 : Total Loss 8.357\n",
      "Epoch 951 : Total Loss 8.316\n",
      "Epoch 961 : Total Loss 8.275\n",
      "Epoch 971 : Total Loss 8.234\n",
      "Epoch 981 : Total Loss 8.195\n",
      "Epoch 991 : Total Loss 8.156\n",
      "Epoch 1001 : Total Loss 8.117\n",
      "Epoch 1011 : Total Loss 8.079\n",
      "Epoch 1021 : Total Loss 8.042\n",
      "Epoch 1031 : Total Loss 8.005\n",
      "Epoch 1041 : Total Loss 7.968\n",
      "Epoch 1051 : Total Loss 7.932\n",
      "Epoch 1061 : Total Loss 7.896\n",
      "Epoch 1071 : Total Loss 7.861\n",
      "Epoch 1081 : Total Loss 7.826\n",
      "Epoch 1091 : Total Loss 7.791\n",
      "Epoch 1101 : Total Loss 7.757\n",
      "Epoch 1111 : Total Loss 7.724\n",
      "Epoch 1121 : Total Loss 7.690\n",
      "Epoch 1131 : Total Loss 7.657\n",
      "Epoch 1141 : Total Loss 7.625\n",
      "Epoch 1151 : Total Loss 7.592\n",
      "Epoch 1161 : Total Loss 7.560\n",
      "Epoch 1171 : Total Loss 7.529\n",
      "Epoch 1181 : Total Loss 7.497\n",
      "Epoch 1191 : Total Loss 7.467\n",
      "Epoch 1201 : Total Loss 7.436\n",
      "Epoch 1211 : Total Loss 7.406\n",
      "Epoch 1221 : Total Loss 7.376\n",
      "Epoch 1231 : Total Loss 7.346\n",
      "Epoch 1241 : Total Loss 7.317\n",
      "Epoch 1251 : Total Loss 7.288\n",
      "Epoch 1261 : Total Loss 7.259\n",
      "Epoch 1271 : Total Loss 7.230\n",
      "Epoch 1281 : Total Loss 7.202\n",
      "Epoch 1291 : Total Loss 7.174\n",
      "Epoch 1301 : Total Loss 7.147\n",
      "Epoch 1311 : Total Loss 7.120\n",
      "Epoch 1321 : Total Loss 7.093\n",
      "Epoch 1331 : Total Loss 7.066\n",
      "Epoch 1341 : Total Loss 7.040\n",
      "Epoch 1351 : Total Loss 7.014\n",
      "Epoch 1361 : Total Loss 6.988\n",
      "Epoch 1371 : Total Loss 6.962\n",
      "Epoch 1381 : Total Loss 6.937\n",
      "Epoch 1391 : Total Loss 6.912\n",
      "Epoch 1401 : Total Loss 6.888\n",
      "Epoch 1411 : Total Loss 6.864\n",
      "Epoch 1421 : Total Loss 6.840\n",
      "Epoch 1431 : Total Loss 6.816\n",
      "Epoch 1441 : Total Loss 6.793\n",
      "Epoch 1451 : Total Loss 6.769\n",
      "Epoch 1461 : Total Loss 6.747\n",
      "Epoch 1471 : Total Loss 6.724\n",
      "Epoch 1481 : Total Loss 6.702\n",
      "Epoch 1491 : Total Loss 6.680\n",
      "Epoch 1501 : Total Loss 6.658\n",
      "Epoch 1511 : Total Loss 6.637\n",
      "Epoch 1521 : Total Loss 6.616\n",
      "Epoch 1531 : Total Loss 6.595\n",
      "Epoch 1541 : Total Loss 6.574\n",
      "Epoch 1551 : Total Loss 6.554\n",
      "Epoch 1561 : Total Loss 6.534\n",
      "Epoch 1571 : Total Loss 6.515\n",
      "Epoch 1581 : Total Loss 6.495\n",
      "Epoch 1591 : Total Loss 6.476\n",
      "Epoch 1601 : Total Loss 6.457\n",
      "Epoch 1611 : Total Loss 6.439\n",
      "Epoch 1621 : Total Loss 6.421\n",
      "Epoch 1631 : Total Loss 6.403\n",
      "Epoch 1641 : Total Loss 6.385\n",
      "Epoch 1651 : Total Loss 6.367\n",
      "Epoch 1661 : Total Loss 6.350\n",
      "Epoch 1671 : Total Loss 6.333\n",
      "Epoch 1681 : Total Loss 6.317\n",
      "Epoch 1691 : Total Loss 6.300\n",
      "Epoch 1701 : Total Loss 6.284\n",
      "Epoch 1711 : Total Loss 6.268\n",
      "Epoch 1721 : Total Loss 6.253\n",
      "Epoch 1731 : Total Loss 6.238\n",
      "Epoch 1741 : Total Loss 6.223\n",
      "Epoch 1751 : Total Loss 6.208\n",
      "Epoch 1761 : Total Loss 6.193\n",
      "Epoch 1771 : Total Loss 6.179\n",
      "Epoch 1781 : Total Loss 6.165\n",
      "Epoch 1791 : Total Loss 6.151\n",
      "Epoch 1801 : Total Loss 6.137\n",
      "Epoch 1811 : Total Loss 6.124\n",
      "Epoch 1821 : Total Loss 6.111\n",
      "Epoch 1831 : Total Loss 6.098\n",
      "Epoch 1841 : Total Loss 6.086\n",
      "Epoch 1851 : Total Loss 6.073\n",
      "Epoch 1861 : Total Loss 6.061\n",
      "Epoch 1871 : Total Loss 6.049\n",
      "Epoch 1881 : Total Loss 6.037\n",
      "Epoch 1891 : Total Loss 6.026\n",
      "Epoch 1901 : Total Loss 6.015\n",
      "Epoch 1911 : Total Loss 6.004\n",
      "Epoch 1921 : Total Loss 5.993\n",
      "Epoch 1931 : Total Loss 5.982\n",
      "Epoch 1941 : Total Loss 5.972\n",
      "Epoch 1951 : Total Loss 5.962\n",
      "Epoch 1961 : Total Loss 5.952\n",
      "Epoch 1971 : Total Loss 5.942\n",
      "Epoch 1981 : Total Loss 5.932\n",
      "Epoch 1991 : Total Loss 5.923\n",
      "Epoch 2001 : Total Loss 5.914\n",
      "Epoch 2011 : Total Loss 5.905\n",
      "Epoch 2021 : Total Loss 5.896\n",
      "Epoch 2031 : Total Loss 5.887\n",
      "Epoch 2041 : Total Loss 5.878\n",
      "Epoch 2051 : Total Loss 5.870\n",
      "Epoch 2061 : Total Loss 5.862\n",
      "Epoch 2071 : Total Loss 5.854\n",
      "Epoch 2081 : Total Loss 5.846\n",
      "Epoch 2091 : Total Loss 5.839\n",
      "Epoch 2101 : Total Loss 5.831\n",
      "Epoch 2111 : Total Loss 5.824\n",
      "Epoch 2121 : Total Loss 5.817\n",
      "Epoch 2131 : Total Loss 5.810\n",
      "Epoch 2141 : Total Loss 5.803\n",
      "Epoch 2151 : Total Loss 5.796\n",
      "Epoch 2161 : Total Loss 5.790\n",
      "Epoch 2171 : Total Loss 5.783\n",
      "Epoch 2181 : Total Loss 5.777\n",
      "Epoch 2191 : Total Loss 5.771\n",
      "Epoch 2201 : Total Loss 5.765\n",
      "Epoch 2211 : Total Loss 5.759\n",
      "Epoch 2221 : Total Loss 5.753\n",
      "Epoch 2231 : Total Loss 5.747\n",
      "Epoch 2241 : Total Loss 5.742\n",
      "Epoch 2251 : Total Loss 5.737\n",
      "Epoch 2261 : Total Loss 5.731\n",
      "Epoch 2271 : Total Loss 5.726\n",
      "Epoch 2281 : Total Loss 5.721\n",
      "Epoch 2291 : Total Loss 5.716\n",
      "Epoch 2301 : Total Loss 5.712\n",
      "Epoch 2311 : Total Loss 5.707\n",
      "Epoch 2321 : Total Loss 5.702\n",
      "Epoch 2331 : Total Loss 5.698\n",
      "Epoch 2341 : Total Loss 5.693\n",
      "Epoch 2351 : Total Loss 5.689\n",
      "Epoch 2361 : Total Loss 5.685\n",
      "Epoch 2371 : Total Loss 5.681\n",
      "Epoch 2381 : Total Loss 5.677\n",
      "Epoch 2391 : Total Loss 5.673\n",
      "Epoch 2401 : Total Loss 5.669\n",
      "Epoch 2411 : Total Loss 5.666\n",
      "Epoch 2421 : Total Loss 5.662\n",
      "Epoch 2431 : Total Loss 5.658\n",
      "Epoch 2441 : Total Loss 5.655\n",
      "Epoch 2451 : Total Loss 5.652\n",
      "Epoch 2461 : Total Loss 5.648\n",
      "Epoch 2471 : Total Loss 5.645\n",
      "Epoch 2481 : Total Loss 5.642\n",
      "Epoch 2491 : Total Loss 5.639\n",
      "Epoch 2501 : Total Loss 5.636\n",
      "Epoch 2511 : Total Loss 5.633\n",
      "Epoch 2521 : Total Loss 5.630\n",
      "Epoch 2531 : Total Loss 5.627\n",
      "Epoch 2541 : Total Loss 5.624\n",
      "Epoch 2551 : Total Loss 5.622\n",
      "Epoch 2561 : Total Loss 5.619\n",
      "Epoch 2571 : Total Loss 5.617\n",
      "Epoch 2581 : Total Loss 5.614\n",
      "Epoch 2591 : Total Loss 5.612\n",
      "Epoch 2601 : Total Loss 5.609\n",
      "Epoch 2611 : Total Loss 5.607\n",
      "Epoch 2621 : Total Loss 5.605\n",
      "Epoch 2631 : Total Loss 5.602\n",
      "Epoch 2641 : Total Loss 5.600\n",
      "Epoch 2651 : Total Loss 5.598\n",
      "Epoch 2661 : Total Loss 5.596\n",
      "Epoch 2671 : Total Loss 5.594\n",
      "Epoch 2681 : Total Loss 5.592\n",
      "Epoch 2691 : Total Loss 5.590\n",
      "Epoch 2701 : Total Loss 5.588\n",
      "Epoch 2711 : Total Loss 5.586\n",
      "Epoch 2721 : Total Loss 5.585\n",
      "Epoch 2731 : Total Loss 5.583\n",
      "Epoch 2741 : Total Loss 5.581\n",
      "Epoch 2751 : Total Loss 5.579\n",
      "Epoch 2761 : Total Loss 5.578\n",
      "Epoch 2771 : Total Loss 5.576\n",
      "Epoch 2781 : Total Loss 5.575\n",
      "Epoch 2791 : Total Loss 5.573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2801 : Total Loss 5.571\n",
      "Epoch 2811 : Total Loss 5.570\n",
      "Epoch 2821 : Total Loss 5.569\n",
      "Epoch 2831 : Total Loss 5.567\n",
      "Epoch 2841 : Total Loss 5.566\n",
      "Epoch 2851 : Total Loss 5.564\n",
      "Epoch 2861 : Total Loss 5.563\n",
      "Epoch 2871 : Total Loss 5.562\n",
      "Epoch 2881 : Total Loss 5.561\n",
      "Epoch 2891 : Total Loss 5.559\n",
      "Epoch 2901 : Total Loss 5.558\n",
      "Epoch 2911 : Total Loss 5.557\n",
      "Epoch 2921 : Total Loss 5.556\n",
      "Epoch 2931 : Total Loss 5.555\n",
      "Epoch 2941 : Total Loss 5.553\n",
      "Epoch 2951 : Total Loss 5.552\n",
      "Epoch 2961 : Total Loss 5.551\n",
      "Epoch 2971 : Total Loss 5.550\n",
      "Epoch 2981 : Total Loss 5.549\n",
      "Epoch 2991 : Total Loss 5.548\n",
      "Epoch 3001 : Total Loss 5.547\n",
      "Epoch 3011 : Total Loss 5.546\n",
      "Epoch 3021 : Total Loss 5.545\n",
      "Epoch 3031 : Total Loss 5.544\n",
      "Epoch 3041 : Total Loss 5.544\n",
      "Epoch 3051 : Total Loss 5.543\n",
      "Epoch 3061 : Total Loss 5.542\n",
      "Epoch 3071 : Total Loss 5.541\n",
      "Epoch 3081 : Total Loss 5.540\n",
      "Epoch 3091 : Total Loss 5.539\n",
      "Epoch 3101 : Total Loss 5.538\n",
      "Epoch 3111 : Total Loss 5.538\n",
      "Epoch 3121 : Total Loss 5.537\n",
      "Epoch 3131 : Total Loss 5.536\n",
      "Epoch 3141 : Total Loss 5.535\n",
      "Epoch 3151 : Total Loss 5.535\n",
      "Epoch 3161 : Total Loss 5.534\n",
      "Epoch 3171 : Total Loss 5.533\n",
      "Epoch 3181 : Total Loss 5.533\n",
      "Epoch 3191 : Total Loss 5.532\n",
      "Epoch 3201 : Total Loss 5.531\n",
      "Epoch 3211 : Total Loss 5.531\n",
      "Epoch 3221 : Total Loss 5.530\n",
      "Epoch 3231 : Total Loss 5.529\n",
      "Epoch 3241 : Total Loss 5.529\n",
      "Epoch 3251 : Total Loss 5.528\n",
      "Epoch 3261 : Total Loss 5.527\n",
      "Epoch 3271 : Total Loss 5.527\n",
      "Epoch 3281 : Total Loss 5.526\n",
      "Epoch 3291 : Total Loss 5.526\n",
      "Epoch 3301 : Total Loss 5.525\n",
      "Epoch 3311 : Total Loss 5.525\n",
      "Epoch 3321 : Total Loss 5.524\n",
      "Epoch 3331 : Total Loss 5.524\n",
      "Epoch 3341 : Total Loss 5.523\n",
      "Epoch 3351 : Total Loss 5.523\n",
      "Epoch 3361 : Total Loss 5.522\n",
      "Epoch 3371 : Total Loss 5.522\n",
      "Epoch 3381 : Total Loss 5.521\n",
      "Epoch 3391 : Total Loss 5.521\n",
      "Epoch 3401 : Total Loss 5.520\n",
      "Epoch 3411 : Total Loss 5.520\n",
      "Epoch 3421 : Total Loss 5.519\n",
      "Epoch 3431 : Total Loss 5.519\n",
      "Epoch 3441 : Total Loss 5.518\n",
      "Epoch 3451 : Total Loss 5.518\n",
      "Epoch 3461 : Total Loss 5.517\n",
      "Epoch 3471 : Total Loss 5.517\n",
      "Epoch 3481 : Total Loss 5.517\n",
      "Epoch 3491 : Total Loss 5.516\n",
      "Epoch 3501 : Total Loss 5.516\n",
      "Epoch 3511 : Total Loss 5.515\n",
      "Epoch 3521 : Total Loss 5.515\n",
      "Epoch 3531 : Total Loss 5.515\n",
      "Epoch 3541 : Total Loss 5.514\n",
      "Epoch 3551 : Total Loss 5.514\n",
      "Epoch 3561 : Total Loss 5.513\n",
      "Epoch 3571 : Total Loss 5.513\n",
      "Epoch 3581 : Total Loss 5.513\n",
      "Epoch 3591 : Total Loss 5.512\n",
      "Epoch 3601 : Total Loss 5.512\n",
      "Epoch 3611 : Total Loss 5.512\n",
      "Epoch 3621 : Total Loss 5.511\n",
      "Epoch 3631 : Total Loss 5.511\n",
      "Epoch 3641 : Total Loss 5.511\n",
      "Epoch 3651 : Total Loss 5.510\n",
      "Epoch 3661 : Total Loss 5.510\n",
      "Epoch 3671 : Total Loss 5.510\n",
      "Epoch 3681 : Total Loss 5.509\n",
      "Epoch 3691 : Total Loss 5.509\n",
      "Epoch 3701 : Total Loss 5.509\n",
      "Epoch 3711 : Total Loss 5.508\n",
      "Epoch 3721 : Total Loss 5.508\n",
      "Epoch 3731 : Total Loss 5.508\n",
      "Epoch 3741 : Total Loss 5.508\n",
      "Epoch 3751 : Total Loss 5.507\n",
      "Epoch 3761 : Total Loss 5.507\n",
      "Epoch 3771 : Total Loss 5.507\n",
      "Epoch 3781 : Total Loss 5.506\n",
      "Epoch 3791 : Total Loss 5.506\n",
      "Epoch 3801 : Total Loss 5.506\n",
      "Epoch 3811 : Total Loss 5.506\n",
      "Epoch 3821 : Total Loss 5.505\n",
      "Epoch 3831 : Total Loss 5.505\n",
      "Epoch 3841 : Total Loss 5.505\n",
      "Epoch 3851 : Total Loss 5.505\n",
      "Epoch 3861 : Total Loss 5.504\n",
      "Epoch 3871 : Total Loss 5.504\n",
      "Epoch 3881 : Total Loss 5.504\n",
      "Epoch 3891 : Total Loss 5.504\n",
      "Epoch 3901 : Total Loss 5.503\n",
      "Epoch 3911 : Total Loss 5.503\n",
      "Epoch 3921 : Total Loss 5.503\n",
      "Epoch 3931 : Total Loss 5.503\n",
      "Epoch 3941 : Total Loss 5.502\n",
      "Epoch 3951 : Total Loss 5.502\n",
      "Epoch 3961 : Total Loss 5.502\n",
      "Epoch 3971 : Total Loss 5.502\n",
      "Epoch 3981 : Total Loss 5.501\n",
      "Epoch 3991 : Total Loss 5.501\n",
      "Epoch 4001 : Total Loss 5.501\n",
      "Epoch 4011 : Total Loss 5.501\n",
      "Epoch 4021 : Total Loss 5.501\n",
      "Epoch 4031 : Total Loss 5.500\n",
      "Epoch 4041 : Total Loss 5.500\n",
      "Epoch 4051 : Total Loss 5.500\n",
      "Epoch 4061 : Total Loss 5.500\n",
      "Epoch 4071 : Total Loss 5.499\n",
      "Epoch 4081 : Total Loss 5.499\n",
      "Epoch 4091 : Total Loss 5.499\n",
      "Epoch 4101 : Total Loss 5.499\n",
      "Epoch 4111 : Total Loss 5.499\n",
      "Epoch 4121 : Total Loss 5.498\n",
      "Epoch 4131 : Total Loss 5.498\n",
      "Epoch 4141 : Total Loss 5.498\n",
      "Epoch 4151 : Total Loss 5.498\n",
      "Epoch 4161 : Total Loss 5.498\n",
      "Epoch 4171 : Total Loss 5.497\n",
      "Epoch 4181 : Total Loss 5.497\n",
      "Epoch 4191 : Total Loss 5.497\n",
      "Epoch 4201 : Total Loss 5.497\n",
      "Epoch 4211 : Total Loss 5.497\n",
      "Epoch 4221 : Total Loss 5.497\n",
      "Epoch 4231 : Total Loss 5.496\n",
      "Epoch 4241 : Total Loss 5.496\n",
      "Epoch 4251 : Total Loss 5.496\n",
      "Epoch 4261 : Total Loss 5.496\n",
      "Epoch 4271 : Total Loss 5.496\n",
      "Epoch 4281 : Total Loss 5.495\n",
      "Epoch 4291 : Total Loss 5.495\n",
      "Epoch 4301 : Total Loss 5.495\n",
      "Epoch 4311 : Total Loss 5.495\n",
      "Epoch 4321 : Total Loss 5.495\n",
      "Epoch 4331 : Total Loss 5.495\n",
      "Epoch 4341 : Total Loss 5.494\n",
      "Epoch 4351 : Total Loss 5.494\n",
      "Epoch 4361 : Total Loss 5.494\n",
      "Epoch 4371 : Total Loss 5.494\n",
      "Epoch 4381 : Total Loss 5.494\n",
      "Epoch 4391 : Total Loss 5.494\n",
      "Epoch 4401 : Total Loss 5.493\n",
      "Epoch 4411 : Total Loss 5.493\n",
      "Epoch 4421 : Total Loss 5.493\n",
      "Epoch 4431 : Total Loss 5.493\n",
      "Epoch 4441 : Total Loss 5.493\n",
      "Epoch 4451 : Total Loss 5.493\n",
      "Epoch 4461 : Total Loss 5.492\n",
      "Epoch 4471 : Total Loss 5.492\n",
      "Epoch 4481 : Total Loss 5.492\n",
      "Epoch 4491 : Total Loss 5.492\n",
      "Epoch 4501 : Total Loss 5.492\n",
      "Epoch 4511 : Total Loss 5.492\n",
      "Epoch 4521 : Total Loss 5.492\n",
      "Epoch 4531 : Total Loss 5.491\n",
      "Epoch 4541 : Total Loss 5.491\n",
      "Epoch 4551 : Total Loss 5.491\n",
      "Epoch 4561 : Total Loss 5.491\n",
      "Epoch 4571 : Total Loss 5.491\n",
      "Epoch 4581 : Total Loss 5.491\n",
      "Epoch 4591 : Total Loss 5.490\n",
      "Epoch 4601 : Total Loss 5.490\n",
      "Epoch 4611 : Total Loss 5.490\n",
      "Epoch 4621 : Total Loss 5.490\n",
      "Epoch 4631 : Total Loss 5.490\n",
      "Epoch 4641 : Total Loss 5.490\n",
      "Epoch 4651 : Total Loss 5.490\n",
      "Epoch 4661 : Total Loss 5.489\n",
      "Epoch 4671 : Total Loss 5.489\n",
      "Epoch 4681 : Total Loss 5.489\n",
      "Epoch 4691 : Total Loss 5.489\n",
      "Epoch 4701 : Total Loss 5.489\n",
      "Epoch 4711 : Total Loss 5.489\n",
      "Epoch 4721 : Total Loss 5.489\n",
      "Epoch 4731 : Total Loss 5.488\n",
      "Epoch 4741 : Total Loss 5.488\n",
      "Epoch 4751 : Total Loss 5.488\n",
      "Epoch 4761 : Total Loss 5.488\n",
      "Epoch 4771 : Total Loss 5.488\n",
      "Epoch 4781 : Total Loss 5.488\n",
      "Epoch 4791 : Total Loss 5.488\n",
      "Epoch 4801 : Total Loss 5.487\n",
      "Epoch 4811 : Total Loss 5.487\n",
      "Epoch 4821 : Total Loss 5.487\n",
      "Epoch 4831 : Total Loss 5.487\n",
      "Epoch 4841 : Total Loss 5.487\n",
      "Epoch 4851 : Total Loss 5.487\n",
      "Epoch 4861 : Total Loss 5.487\n",
      "Epoch 4871 : Total Loss 5.486\n",
      "Epoch 4881 : Total Loss 5.486\n",
      "Epoch 4891 : Total Loss 5.486\n",
      "Epoch 4901 : Total Loss 5.486\n",
      "Epoch 4911 : Total Loss 5.486\n",
      "Epoch 4921 : Total Loss 5.486\n",
      "Epoch 4931 : Total Loss 5.486\n",
      "Epoch 4941 : Total Loss 5.485\n",
      "Epoch 4951 : Total Loss 5.485\n",
      "Epoch 4961 : Total Loss 5.485\n",
      "Epoch 4971 : Total Loss 5.485\n",
      "Epoch 4981 : Total Loss 5.485\n",
      "Epoch 4991 : Total Loss 5.485\n",
      "Here is the resulting prediction matrix:\n",
      "[[4.98491191 2.97046866 1.54967559 1.49742589 0.99629819]\n",
      " [2.99874535 2.13063156 3.16736876 2.87226948 0.95215147]\n",
      " [3.98144094 2.34362793 1.04991045 1.03033401 0.76609569]\n",
      " [4.51111332 2.94242653 3.05606241 2.81370307 1.16263457]\n",
      " [4.96419389 3.01072185 1.88529089 1.79290779 1.04615055]]\n",
      "\n",
      "Here are the predicted ratings:\n",
      "[0.996298193329284, 2.130631562564697, 2.343627930009872, 1.0499104477032921, 0.7660956918962316, 4.511113315354235, 2.942426533997399, 1.1626345748924993, 1.8852908885836186, 1.7929077888868246]\n",
      "\n",
      "The Mean Squared Error is 0.29567359617808037.\n",
      "The Root Mean Squared Error is 0.5437587665298651.\n",
      "\n",
      "The Runtime of this algorithm of this 5 x 5, 40.000% sparse matrix, is 5.514454500051215 seconds.\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [5,3,2,1,0],\n",
    "    [3,0,4,2,1],\n",
    "    [4,0,0,1,0],\n",
    "    [0,0,2,4,0],\n",
    "    [5,3,0,0,1]\n",
    "])\n",
    "\n",
    "m = A.shape[0]\n",
    "n = A.shape[1]\n",
    "user_features = 2\n",
    "\n",
    "np.random.seed(24)\n",
    "\n",
    "# generate matrices with entries of random numbers 0 - 1\n",
    "U = np.random.rand(m, user_features)\n",
    "V = np.random.rand(n, user_features)\n",
    "\n",
    "dict_small_sparse1 = evaluation_pipeline(A, U, V, 5000, 2, matrix_factorization_sparse)\n",
    "\n",
    "call_eval_pipeline(dict_small_sparse1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc081cc3",
   "metadata": {},
   "source": [
    "### Test Runtime\n",
    "\n",
    "We will test the runtime of the code we implemented for the sparse data.\n",
    "\n",
    "We will test both methods for the same amount of epochs and latent features, learning rate, etc. on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee22777",
   "metadata": {},
   "source": [
    "**Regular Matrix Factorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b281371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 5.473\n",
      "Epoch 11 : Total Loss 5.473\n",
      "Epoch 21 : Total Loss 5.473\n",
      "Epoch 31 : Total Loss 5.473\n",
      "Epoch 41 : Total Loss 5.473\n",
      "Epoch 51 : Total Loss 5.473\n",
      "Epoch 61 : Total Loss 5.473\n",
      "Epoch 71 : Total Loss 5.473\n",
      "Epoch 81 : Total Loss 5.472\n",
      "Epoch 91 : Total Loss 5.472\n",
      "Epoch 101 : Total Loss 5.472\n",
      "Epoch 111 : Total Loss 5.472\n",
      "Epoch 121 : Total Loss 5.472\n",
      "Epoch 131 : Total Loss 5.472\n",
      "Epoch 141 : Total Loss 5.472\n",
      "Epoch 151 : Total Loss 5.471\n",
      "Epoch 161 : Total Loss 5.471\n",
      "Epoch 171 : Total Loss 5.471\n",
      "Epoch 181 : Total Loss 5.471\n",
      "Epoch 191 : Total Loss 5.471\n",
      "Epoch 201 : Total Loss 5.471\n",
      "Epoch 211 : Total Loss 5.471\n",
      "Epoch 221 : Total Loss 5.470\n",
      "Epoch 231 : Total Loss 5.470\n",
      "Epoch 241 : Total Loss 5.470\n",
      "Epoch 251 : Total Loss 5.470\n",
      "Epoch 261 : Total Loss 5.470\n",
      "Epoch 271 : Total Loss 5.470\n",
      "Epoch 281 : Total Loss 5.470\n",
      "Epoch 291 : Total Loss 5.469\n",
      "Epoch 301 : Total Loss 5.469\n",
      "Epoch 311 : Total Loss 5.469\n",
      "Epoch 321 : Total Loss 5.469\n",
      "Epoch 331 : Total Loss 5.469\n",
      "Epoch 341 : Total Loss 5.469\n",
      "Epoch 351 : Total Loss 5.468\n",
      "Epoch 361 : Total Loss 5.468\n",
      "Epoch 371 : Total Loss 5.468\n",
      "Epoch 381 : Total Loss 5.468\n",
      "Epoch 391 : Total Loss 5.468\n",
      "Epoch 401 : Total Loss 5.468\n",
      "Epoch 411 : Total Loss 5.468\n",
      "Epoch 421 : Total Loss 5.467\n",
      "Epoch 431 : Total Loss 5.467\n",
      "Epoch 441 : Total Loss 5.467\n",
      "Epoch 451 : Total Loss 5.467\n",
      "Epoch 461 : Total Loss 5.467\n",
      "Epoch 471 : Total Loss 5.467\n",
      "Epoch 481 : Total Loss 5.466\n",
      "Epoch 491 : Total Loss 5.466\n",
      "Epoch 501 : Total Loss 5.466\n",
      "Epoch 511 : Total Loss 5.466\n",
      "Epoch 521 : Total Loss 5.466\n",
      "Epoch 531 : Total Loss 5.466\n",
      "Epoch 541 : Total Loss 5.466\n",
      "Epoch 551 : Total Loss 5.465\n",
      "Epoch 561 : Total Loss 5.465\n",
      "Epoch 571 : Total Loss 5.465\n",
      "Epoch 581 : Total Loss 5.465\n",
      "Epoch 591 : Total Loss 5.465\n",
      "Epoch 601 : Total Loss 5.465\n",
      "Epoch 611 : Total Loss 5.464\n",
      "Epoch 621 : Total Loss 5.464\n",
      "Epoch 631 : Total Loss 5.464\n",
      "Epoch 641 : Total Loss 5.464\n",
      "Epoch 651 : Total Loss 5.464\n",
      "Epoch 661 : Total Loss 5.464\n",
      "Epoch 671 : Total Loss 5.463\n",
      "Epoch 681 : Total Loss 5.463\n",
      "Epoch 691 : Total Loss 5.463\n",
      "Epoch 701 : Total Loss 5.463\n",
      "Epoch 711 : Total Loss 5.463\n",
      "Epoch 721 : Total Loss 5.463\n",
      "Epoch 731 : Total Loss 5.462\n",
      "Epoch 741 : Total Loss 5.462\n",
      "Epoch 751 : Total Loss 5.462\n",
      "Epoch 761 : Total Loss 5.462\n",
      "Epoch 771 : Total Loss 5.462\n",
      "Epoch 781 : Total Loss 5.461\n",
      "Epoch 791 : Total Loss 5.461\n",
      "Epoch 801 : Total Loss 5.461\n",
      "Epoch 811 : Total Loss 5.461\n",
      "Epoch 821 : Total Loss 5.461\n",
      "Epoch 831 : Total Loss 5.461\n",
      "Epoch 841 : Total Loss 5.460\n",
      "Epoch 851 : Total Loss 5.460\n",
      "Epoch 861 : Total Loss 5.460\n",
      "Epoch 871 : Total Loss 5.460\n",
      "Epoch 881 : Total Loss 5.460\n",
      "Epoch 891 : Total Loss 5.459\n",
      "Epoch 901 : Total Loss 5.459\n",
      "Epoch 911 : Total Loss 5.459\n",
      "Epoch 921 : Total Loss 5.459\n",
      "Epoch 931 : Total Loss 5.459\n",
      "Epoch 941 : Total Loss 5.458\n",
      "Epoch 951 : Total Loss 5.458\n",
      "Epoch 961 : Total Loss 5.458\n",
      "Epoch 971 : Total Loss 5.458\n",
      "Epoch 981 : Total Loss 5.458\n",
      "Epoch 991 : Total Loss 5.457\n",
      "Epoch 1001 : Total Loss 5.457\n",
      "Epoch 1011 : Total Loss 5.457\n",
      "Epoch 1021 : Total Loss 5.457\n",
      "Epoch 1031 : Total Loss 5.457\n",
      "Epoch 1041 : Total Loss 5.456\n",
      "Epoch 1051 : Total Loss 5.456\n",
      "Epoch 1061 : Total Loss 5.456\n",
      "Epoch 1071 : Total Loss 5.456\n",
      "Epoch 1081 : Total Loss 5.455\n",
      "Epoch 1091 : Total Loss 5.455\n",
      "Epoch 1101 : Total Loss 5.455\n",
      "Epoch 1111 : Total Loss 5.455\n",
      "Epoch 1121 : Total Loss 5.455\n",
      "Epoch 1131 : Total Loss 5.454\n",
      "Epoch 1141 : Total Loss 5.454\n",
      "Epoch 1151 : Total Loss 5.454\n",
      "Epoch 1161 : Total Loss 5.454\n",
      "Epoch 1171 : Total Loss 5.453\n",
      "Epoch 1181 : Total Loss 5.453\n",
      "Epoch 1191 : Total Loss 5.453\n",
      "Epoch 1201 : Total Loss 5.453\n",
      "Epoch 1211 : Total Loss 5.452\n",
      "Epoch 1221 : Total Loss 5.452\n",
      "Epoch 1231 : Total Loss 5.452\n",
      "Epoch 1241 : Total Loss 5.452\n",
      "Epoch 1251 : Total Loss 5.451\n",
      "Epoch 1261 : Total Loss 5.451\n",
      "Epoch 1271 : Total Loss 5.451\n",
      "Epoch 1281 : Total Loss 5.451\n",
      "Epoch 1291 : Total Loss 5.450\n",
      "Epoch 1301 : Total Loss 5.450\n",
      "Epoch 1311 : Total Loss 5.450\n",
      "Epoch 1321 : Total Loss 5.450\n",
      "Epoch 1331 : Total Loss 5.449\n",
      "Epoch 1341 : Total Loss 5.449\n",
      "Epoch 1351 : Total Loss 5.449\n",
      "Epoch 1361 : Total Loss 5.449\n",
      "Epoch 1371 : Total Loss 5.448\n",
      "Epoch 1381 : Total Loss 5.448\n",
      "Epoch 1391 : Total Loss 5.448\n",
      "Epoch 1401 : Total Loss 5.448\n",
      "Epoch 1411 : Total Loss 5.447\n",
      "Epoch 1421 : Total Loss 5.447\n",
      "Epoch 1431 : Total Loss 5.447\n",
      "Epoch 1441 : Total Loss 5.446\n",
      "Epoch 1451 : Total Loss 5.446\n",
      "Epoch 1461 : Total Loss 5.446\n",
      "Epoch 1471 : Total Loss 5.446\n",
      "Epoch 1481 : Total Loss 5.445\n",
      "Epoch 1491 : Total Loss 5.445\n",
      "Epoch 1501 : Total Loss 5.445\n",
      "Epoch 1511 : Total Loss 5.444\n",
      "Epoch 1521 : Total Loss 5.444\n",
      "Epoch 1531 : Total Loss 5.444\n",
      "Epoch 1541 : Total Loss 5.443\n",
      "Epoch 1551 : Total Loss 5.443\n",
      "Epoch 1561 : Total Loss 5.443\n",
      "Epoch 1571 : Total Loss 5.442\n",
      "Epoch 1581 : Total Loss 5.442\n",
      "Epoch 1591 : Total Loss 5.442\n",
      "Epoch 1601 : Total Loss 5.442\n",
      "Epoch 1611 : Total Loss 5.441\n",
      "Epoch 1621 : Total Loss 5.441\n",
      "Epoch 1631 : Total Loss 5.441\n",
      "Epoch 1641 : Total Loss 5.440\n",
      "Epoch 1651 : Total Loss 5.440\n",
      "Epoch 1661 : Total Loss 5.440\n",
      "Epoch 1671 : Total Loss 5.439\n",
      "Epoch 1681 : Total Loss 5.439\n",
      "Epoch 1691 : Total Loss 5.438\n",
      "Epoch 1701 : Total Loss 5.438\n",
      "Epoch 1711 : Total Loss 5.438\n",
      "Epoch 1721 : Total Loss 5.437\n",
      "Epoch 1731 : Total Loss 5.437\n",
      "Epoch 1741 : Total Loss 5.437\n",
      "Epoch 1751 : Total Loss 5.436\n",
      "Epoch 1761 : Total Loss 5.436\n",
      "Epoch 1771 : Total Loss 5.436\n",
      "Epoch 1781 : Total Loss 5.435\n",
      "Epoch 1791 : Total Loss 5.435\n",
      "Epoch 1801 : Total Loss 5.434\n",
      "Epoch 1811 : Total Loss 5.434\n",
      "Epoch 1821 : Total Loss 5.434\n",
      "Epoch 1831 : Total Loss 5.433\n",
      "Epoch 1841 : Total Loss 5.433\n",
      "Epoch 1851 : Total Loss 5.433\n",
      "Epoch 1861 : Total Loss 5.432\n",
      "Epoch 1871 : Total Loss 5.432\n",
      "Epoch 1881 : Total Loss 5.431\n",
      "Epoch 1891 : Total Loss 5.431\n",
      "Epoch 1901 : Total Loss 5.431\n",
      "Epoch 1911 : Total Loss 5.430\n",
      "Epoch 1921 : Total Loss 5.430\n",
      "Epoch 1931 : Total Loss 5.429\n",
      "Epoch 1941 : Total Loss 5.429\n",
      "Epoch 1951 : Total Loss 5.428\n",
      "Epoch 1961 : Total Loss 5.428\n",
      "Epoch 1971 : Total Loss 5.428\n",
      "Epoch 1981 : Total Loss 5.427\n",
      "Epoch 1991 : Total Loss 5.427\n",
      "Epoch 2001 : Total Loss 5.426\n",
      "Epoch 2011 : Total Loss 5.426\n",
      "Epoch 2021 : Total Loss 5.425\n",
      "Epoch 2031 : Total Loss 5.425\n",
      "Epoch 2041 : Total Loss 5.424\n",
      "Epoch 2051 : Total Loss 5.424\n",
      "Epoch 2061 : Total Loss 5.423\n",
      "Epoch 2071 : Total Loss 5.423\n",
      "Epoch 2081 : Total Loss 5.423\n",
      "Epoch 2091 : Total Loss 5.422\n",
      "Epoch 2101 : Total Loss 5.422\n",
      "Epoch 2111 : Total Loss 5.421\n",
      "Epoch 2121 : Total Loss 5.421\n",
      "Epoch 2131 : Total Loss 5.420\n",
      "Epoch 2141 : Total Loss 5.420\n",
      "Epoch 2151 : Total Loss 5.419\n",
      "Epoch 2161 : Total Loss 5.419\n",
      "Epoch 2171 : Total Loss 5.418\n",
      "Epoch 2181 : Total Loss 5.418\n",
      "Epoch 2191 : Total Loss 5.417\n",
      "Epoch 2201 : Total Loss 5.416\n",
      "Epoch 2211 : Total Loss 5.416\n",
      "Epoch 2221 : Total Loss 5.415\n",
      "Epoch 2231 : Total Loss 5.415\n",
      "Epoch 2241 : Total Loss 5.414\n",
      "Epoch 2251 : Total Loss 5.414\n",
      "Epoch 2261 : Total Loss 5.413\n",
      "Epoch 2271 : Total Loss 5.413\n",
      "Epoch 2281 : Total Loss 5.412\n",
      "Epoch 2291 : Total Loss 5.412\n",
      "Epoch 2301 : Total Loss 5.411\n",
      "Epoch 2311 : Total Loss 5.410\n",
      "Epoch 2321 : Total Loss 5.410\n",
      "Epoch 2331 : Total Loss 5.409\n",
      "Epoch 2341 : Total Loss 5.409\n",
      "Epoch 2351 : Total Loss 5.408\n",
      "Epoch 2361 : Total Loss 5.407\n",
      "Epoch 2371 : Total Loss 5.407\n",
      "Epoch 2381 : Total Loss 5.406\n",
      "Epoch 2391 : Total Loss 5.406\n",
      "Epoch 2401 : Total Loss 5.405\n",
      "Epoch 2411 : Total Loss 5.404\n",
      "Epoch 2421 : Total Loss 5.404\n",
      "Epoch 2431 : Total Loss 5.403\n",
      "Epoch 2441 : Total Loss 5.403\n",
      "Epoch 2451 : Total Loss 5.402\n",
      "Epoch 2461 : Total Loss 5.401\n",
      "Epoch 2471 : Total Loss 5.401\n",
      "Epoch 2481 : Total Loss 5.400\n",
      "Epoch 2491 : Total Loss 5.399\n",
      "Epoch 2501 : Total Loss 5.399\n",
      "Epoch 2511 : Total Loss 5.398\n",
      "Epoch 2521 : Total Loss 5.397\n",
      "Epoch 2531 : Total Loss 5.397\n",
      "Epoch 2541 : Total Loss 5.396\n",
      "Epoch 2551 : Total Loss 5.395\n",
      "Epoch 2561 : Total Loss 5.394\n",
      "Epoch 2571 : Total Loss 5.394\n",
      "Epoch 2581 : Total Loss 5.393\n",
      "Epoch 2591 : Total Loss 5.392\n",
      "Epoch 2601 : Total Loss 5.392\n",
      "Epoch 2611 : Total Loss 5.391\n",
      "Epoch 2621 : Total Loss 5.390\n",
      "Epoch 2631 : Total Loss 5.389\n",
      "Epoch 2641 : Total Loss 5.389\n",
      "Epoch 2651 : Total Loss 5.388\n",
      "Epoch 2661 : Total Loss 5.387\n",
      "Epoch 2671 : Total Loss 5.386\n",
      "Epoch 2681 : Total Loss 5.385\n",
      "Epoch 2691 : Total Loss 5.385\n",
      "Epoch 2701 : Total Loss 5.384\n",
      "Epoch 2711 : Total Loss 5.383\n",
      "Epoch 2721 : Total Loss 5.382\n",
      "Epoch 2731 : Total Loss 5.381\n",
      "Epoch 2741 : Total Loss 5.381\n",
      "Epoch 2751 : Total Loss 5.380\n",
      "Epoch 2761 : Total Loss 5.379\n",
      "Epoch 2771 : Total Loss 5.378\n",
      "Epoch 2781 : Total Loss 5.377\n",
      "Epoch 2791 : Total Loss 5.376\n",
      "Epoch 2801 : Total Loss 5.376\n",
      "Epoch 2811 : Total Loss 5.375\n",
      "Epoch 2821 : Total Loss 5.374\n",
      "Epoch 2831 : Total Loss 5.373\n",
      "Epoch 2841 : Total Loss 5.372\n",
      "Epoch 2851 : Total Loss 5.371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2861 : Total Loss 5.370\n",
      "Epoch 2871 : Total Loss 5.369\n",
      "Epoch 2881 : Total Loss 5.369\n",
      "Epoch 2891 : Total Loss 5.368\n",
      "Epoch 2901 : Total Loss 5.367\n",
      "Epoch 2911 : Total Loss 5.366\n",
      "Epoch 2921 : Total Loss 5.365\n",
      "Epoch 2931 : Total Loss 5.364\n",
      "Epoch 2941 : Total Loss 5.363\n",
      "Epoch 2951 : Total Loss 5.362\n",
      "Epoch 2961 : Total Loss 5.361\n",
      "Epoch 2971 : Total Loss 5.360\n",
      "Epoch 2981 : Total Loss 5.359\n",
      "Epoch 2991 : Total Loss 5.358\n",
      "Epoch 3001 : Total Loss 5.357\n",
      "Epoch 3011 : Total Loss 5.356\n",
      "Epoch 3021 : Total Loss 5.355\n",
      "Epoch 3031 : Total Loss 5.354\n",
      "Epoch 3041 : Total Loss 5.353\n",
      "Epoch 3051 : Total Loss 5.352\n",
      "Epoch 3061 : Total Loss 5.351\n",
      "Epoch 3071 : Total Loss 5.350\n",
      "Epoch 3081 : Total Loss 5.349\n",
      "Epoch 3091 : Total Loss 5.347\n",
      "Epoch 3101 : Total Loss 5.346\n",
      "Epoch 3111 : Total Loss 5.345\n",
      "Epoch 3121 : Total Loss 5.344\n",
      "Epoch 3131 : Total Loss 5.343\n",
      "Epoch 3141 : Total Loss 5.342\n",
      "Epoch 3151 : Total Loss 5.341\n",
      "Epoch 3161 : Total Loss 5.340\n",
      "Epoch 3171 : Total Loss 5.338\n",
      "Epoch 3181 : Total Loss 5.337\n",
      "Epoch 3191 : Total Loss 5.336\n",
      "Epoch 3201 : Total Loss 5.335\n",
      "Epoch 3211 : Total Loss 5.334\n",
      "Epoch 3221 : Total Loss 5.333\n",
      "Epoch 3231 : Total Loss 5.331\n",
      "Epoch 3241 : Total Loss 5.330\n",
      "Epoch 3251 : Total Loss 5.329\n",
      "Epoch 3261 : Total Loss 5.328\n",
      "Epoch 3271 : Total Loss 5.326\n",
      "Epoch 3281 : Total Loss 5.325\n",
      "Epoch 3291 : Total Loss 5.324\n",
      "Epoch 3301 : Total Loss 5.323\n",
      "Epoch 3311 : Total Loss 5.321\n",
      "Epoch 3321 : Total Loss 5.320\n",
      "Epoch 3331 : Total Loss 5.319\n",
      "Epoch 3341 : Total Loss 5.317\n",
      "Epoch 3351 : Total Loss 5.316\n",
      "Epoch 3361 : Total Loss 5.315\n",
      "Epoch 3371 : Total Loss 5.313\n",
      "Epoch 3381 : Total Loss 5.312\n",
      "Epoch 3391 : Total Loss 5.311\n",
      "Epoch 3401 : Total Loss 5.309\n",
      "Epoch 3411 : Total Loss 5.308\n",
      "Epoch 3421 : Total Loss 5.306\n",
      "Epoch 3431 : Total Loss 5.305\n",
      "Epoch 3441 : Total Loss 5.303\n",
      "Epoch 3451 : Total Loss 5.302\n",
      "Epoch 3461 : Total Loss 5.301\n",
      "Epoch 3471 : Total Loss 5.299\n",
      "Epoch 3481 : Total Loss 5.298\n",
      "Epoch 3491 : Total Loss 5.296\n",
      "Epoch 3501 : Total Loss 5.295\n",
      "Epoch 3511 : Total Loss 5.293\n",
      "Epoch 3521 : Total Loss 5.292\n",
      "Epoch 3531 : Total Loss 5.290\n",
      "Epoch 3541 : Total Loss 5.289\n",
      "Epoch 3551 : Total Loss 5.287\n",
      "Epoch 3561 : Total Loss 5.285\n",
      "Epoch 3571 : Total Loss 5.284\n",
      "Epoch 3581 : Total Loss 5.282\n",
      "Epoch 3591 : Total Loss 5.281\n",
      "Epoch 3601 : Total Loss 5.279\n",
      "Epoch 3611 : Total Loss 5.277\n",
      "Epoch 3621 : Total Loss 5.276\n",
      "Epoch 3631 : Total Loss 5.274\n",
      "Epoch 3641 : Total Loss 5.272\n",
      "Epoch 3651 : Total Loss 5.271\n",
      "Epoch 3661 : Total Loss 5.269\n",
      "Epoch 3671 : Total Loss 5.267\n",
      "Epoch 3681 : Total Loss 5.266\n",
      "Epoch 3691 : Total Loss 5.264\n",
      "Epoch 3701 : Total Loss 5.262\n",
      "Epoch 3711 : Total Loss 5.260\n",
      "Epoch 3721 : Total Loss 5.259\n",
      "Epoch 3731 : Total Loss 5.257\n",
      "Epoch 3741 : Total Loss 5.255\n",
      "Epoch 3751 : Total Loss 5.253\n",
      "Epoch 3761 : Total Loss 5.251\n",
      "Epoch 3771 : Total Loss 5.249\n",
      "Epoch 3781 : Total Loss 5.248\n",
      "Epoch 3791 : Total Loss 5.246\n",
      "Epoch 3801 : Total Loss 5.244\n",
      "Epoch 3811 : Total Loss 5.242\n",
      "Epoch 3821 : Total Loss 5.240\n",
      "Epoch 3831 : Total Loss 5.238\n",
      "Epoch 3841 : Total Loss 5.236\n",
      "Epoch 3851 : Total Loss 5.234\n",
      "Epoch 3861 : Total Loss 5.232\n",
      "Epoch 3871 : Total Loss 5.230\n",
      "Epoch 3881 : Total Loss 5.228\n",
      "Epoch 3891 : Total Loss 5.226\n",
      "Epoch 3901 : Total Loss 5.224\n",
      "Epoch 3911 : Total Loss 5.222\n",
      "Epoch 3921 : Total Loss 5.220\n",
      "Epoch 3931 : Total Loss 5.218\n",
      "Epoch 3941 : Total Loss 5.216\n",
      "Epoch 3951 : Total Loss 5.214\n",
      "Epoch 3961 : Total Loss 5.212\n",
      "Epoch 3971 : Total Loss 5.209\n",
      "Epoch 3981 : Total Loss 5.207\n",
      "Epoch 3991 : Total Loss 5.205\n",
      "Epoch 4001 : Total Loss 5.203\n",
      "Epoch 4011 : Total Loss 5.201\n",
      "Epoch 4021 : Total Loss 5.199\n",
      "Epoch 4031 : Total Loss 5.196\n",
      "Epoch 4041 : Total Loss 5.194\n",
      "Epoch 4051 : Total Loss 5.192\n",
      "Epoch 4061 : Total Loss 5.189\n",
      "Epoch 4071 : Total Loss 5.187\n",
      "Epoch 4081 : Total Loss 5.185\n",
      "Epoch 4091 : Total Loss 5.183\n",
      "Epoch 4101 : Total Loss 5.180\n",
      "Epoch 4111 : Total Loss 5.178\n",
      "Epoch 4121 : Total Loss 5.175\n",
      "Epoch 4131 : Total Loss 5.173\n",
      "Epoch 4141 : Total Loss 5.171\n",
      "Epoch 4151 : Total Loss 5.168\n",
      "Epoch 4161 : Total Loss 5.166\n",
      "Epoch 4171 : Total Loss 5.163\n",
      "Epoch 4181 : Total Loss 5.161\n",
      "Epoch 4191 : Total Loss 5.158\n",
      "Epoch 4201 : Total Loss 5.156\n",
      "Epoch 4211 : Total Loss 5.153\n",
      "Epoch 4221 : Total Loss 5.151\n",
      "Epoch 4231 : Total Loss 5.148\n",
      "Epoch 4241 : Total Loss 5.145\n",
      "Epoch 4251 : Total Loss 5.143\n",
      "Epoch 4261 : Total Loss 5.140\n",
      "Epoch 4271 : Total Loss 5.138\n",
      "Epoch 4281 : Total Loss 5.135\n",
      "Epoch 4291 : Total Loss 5.132\n",
      "Epoch 4301 : Total Loss 5.129\n",
      "Epoch 4311 : Total Loss 5.127\n",
      "Epoch 4321 : Total Loss 5.124\n",
      "Epoch 4331 : Total Loss 5.121\n",
      "Epoch 4341 : Total Loss 5.118\n",
      "Epoch 4351 : Total Loss 5.116\n",
      "Epoch 4361 : Total Loss 5.113\n",
      "Epoch 4371 : Total Loss 5.110\n",
      "Epoch 4381 : Total Loss 5.107\n",
      "Epoch 4391 : Total Loss 5.104\n",
      "Epoch 4401 : Total Loss 5.101\n",
      "Epoch 4411 : Total Loss 5.098\n",
      "Epoch 4421 : Total Loss 5.095\n",
      "Epoch 4431 : Total Loss 5.093\n",
      "Epoch 4441 : Total Loss 5.090\n",
      "Epoch 4451 : Total Loss 5.087\n",
      "Epoch 4461 : Total Loss 5.084\n",
      "Epoch 4471 : Total Loss 5.080\n",
      "Epoch 4481 : Total Loss 5.077\n",
      "Epoch 4491 : Total Loss 5.074\n",
      "Epoch 4501 : Total Loss 5.071\n",
      "Epoch 4511 : Total Loss 5.068\n",
      "Epoch 4521 : Total Loss 5.065\n",
      "Epoch 4531 : Total Loss 5.062\n",
      "Epoch 4541 : Total Loss 5.059\n",
      "Epoch 4551 : Total Loss 5.055\n",
      "Epoch 4561 : Total Loss 5.052\n",
      "Epoch 4571 : Total Loss 5.049\n",
      "Epoch 4581 : Total Loss 5.046\n",
      "Epoch 4591 : Total Loss 5.042\n",
      "Epoch 4601 : Total Loss 5.039\n",
      "Epoch 4611 : Total Loss 5.036\n",
      "Epoch 4621 : Total Loss 5.032\n",
      "Epoch 4631 : Total Loss 5.029\n",
      "Epoch 4641 : Total Loss 5.026\n",
      "Epoch 4651 : Total Loss 5.022\n",
      "Epoch 4661 : Total Loss 5.019\n",
      "Epoch 4671 : Total Loss 5.015\n",
      "Epoch 4681 : Total Loss 5.012\n",
      "Epoch 4691 : Total Loss 5.008\n",
      "Epoch 4701 : Total Loss 5.005\n",
      "Epoch 4711 : Total Loss 5.001\n",
      "Epoch 4721 : Total Loss 4.998\n",
      "Epoch 4731 : Total Loss 4.994\n",
      "Epoch 4741 : Total Loss 4.991\n",
      "Epoch 4751 : Total Loss 4.987\n",
      "Epoch 4761 : Total Loss 4.983\n",
      "Epoch 4771 : Total Loss 4.980\n",
      "Epoch 4781 : Total Loss 4.976\n",
      "Epoch 4791 : Total Loss 4.972\n",
      "Epoch 4801 : Total Loss 4.968\n",
      "Epoch 4811 : Total Loss 4.965\n",
      "Epoch 4821 : Total Loss 4.961\n",
      "Epoch 4831 : Total Loss 4.957\n",
      "Epoch 4841 : Total Loss 4.953\n",
      "Epoch 4851 : Total Loss 4.949\n",
      "Epoch 4861 : Total Loss 4.946\n",
      "Epoch 4871 : Total Loss 4.942\n",
      "Epoch 4881 : Total Loss 4.938\n",
      "Epoch 4891 : Total Loss 4.934\n",
      "Epoch 4901 : Total Loss 4.930\n",
      "Epoch 4911 : Total Loss 4.926\n",
      "Epoch 4921 : Total Loss 4.922\n",
      "Epoch 4931 : Total Loss 4.918\n",
      "Epoch 4941 : Total Loss 4.914\n",
      "Epoch 4951 : Total Loss 4.910\n",
      "Epoch 4961 : Total Loss 4.905\n",
      "Epoch 4971 : Total Loss 4.901\n",
      "Epoch 4981 : Total Loss 4.897\n",
      "Epoch 4991 : Total Loss 4.893\n",
      "The runtime of the regular matrix factorization is  5.8311837000073865 seconds.\n"
     ]
    }
   ],
   "source": [
    "start1 = t.default_timer()\n",
    "\n",
    "U_hat, V_hat = matrix_factorization(A, U, V, user_features, 5000)\n",
    "\n",
    "end1 = t.default_timer()\n",
    "\n",
    "print('The runtime of the regular matrix factorization is ', end1 - start1, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6c3f3",
   "metadata": {},
   "source": [
    "**Sparse Matrix Factorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dac08b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 4.899\n",
      "Epoch 11 : Total Loss 4.895\n",
      "Epoch 21 : Total Loss 4.891\n",
      "Epoch 31 : Total Loss 4.886\n",
      "Epoch 41 : Total Loss 4.882\n",
      "Epoch 51 : Total Loss 4.878\n",
      "Epoch 61 : Total Loss 4.873\n",
      "Epoch 71 : Total Loss 4.869\n",
      "Epoch 81 : Total Loss 4.864\n",
      "Epoch 91 : Total Loss 4.860\n",
      "Epoch 101 : Total Loss 4.856\n",
      "Epoch 111 : Total Loss 4.851\n",
      "Epoch 121 : Total Loss 4.847\n",
      "Epoch 131 : Total Loss 4.842\n",
      "Epoch 141 : Total Loss 4.838\n",
      "Epoch 151 : Total Loss 4.833\n",
      "Epoch 161 : Total Loss 4.828\n",
      "Epoch 171 : Total Loss 4.824\n",
      "Epoch 181 : Total Loss 4.819\n",
      "Epoch 191 : Total Loss 4.815\n",
      "Epoch 201 : Total Loss 4.810\n",
      "Epoch 211 : Total Loss 4.805\n",
      "Epoch 221 : Total Loss 4.800\n",
      "Epoch 231 : Total Loss 4.796\n",
      "Epoch 241 : Total Loss 4.791\n",
      "Epoch 251 : Total Loss 4.786\n",
      "Epoch 261 : Total Loss 4.781\n",
      "Epoch 271 : Total Loss 4.776\n",
      "Epoch 281 : Total Loss 4.772\n",
      "Epoch 291 : Total Loss 4.767\n",
      "Epoch 301 : Total Loss 4.762\n",
      "Epoch 311 : Total Loss 4.757\n",
      "Epoch 321 : Total Loss 4.752\n",
      "Epoch 331 : Total Loss 4.747\n",
      "Epoch 341 : Total Loss 4.742\n",
      "Epoch 351 : Total Loss 4.737\n",
      "Epoch 361 : Total Loss 4.732\n",
      "Epoch 371 : Total Loss 4.727\n",
      "Epoch 381 : Total Loss 4.721\n",
      "Epoch 391 : Total Loss 4.716\n",
      "Epoch 401 : Total Loss 4.711\n",
      "Epoch 411 : Total Loss 4.706\n",
      "Epoch 421 : Total Loss 4.701\n",
      "Epoch 431 : Total Loss 4.696\n",
      "Epoch 441 : Total Loss 4.690\n",
      "Epoch 451 : Total Loss 4.685\n",
      "Epoch 461 : Total Loss 4.680\n",
      "Epoch 471 : Total Loss 4.674\n",
      "Epoch 481 : Total Loss 4.669\n",
      "Epoch 491 : Total Loss 4.664\n",
      "Epoch 501 : Total Loss 4.658\n",
      "Epoch 511 : Total Loss 4.653\n",
      "Epoch 521 : Total Loss 4.648\n",
      "Epoch 531 : Total Loss 4.642\n",
      "Epoch 541 : Total Loss 4.637\n",
      "Epoch 551 : Total Loss 4.631\n",
      "Epoch 561 : Total Loss 4.626\n",
      "Epoch 571 : Total Loss 4.620\n",
      "Epoch 581 : Total Loss 4.614\n",
      "Epoch 591 : Total Loss 4.609\n",
      "Epoch 601 : Total Loss 4.603\n",
      "Epoch 611 : Total Loss 4.598\n",
      "Epoch 621 : Total Loss 4.592\n",
      "Epoch 631 : Total Loss 4.586\n",
      "Epoch 641 : Total Loss 4.581\n",
      "Epoch 651 : Total Loss 4.575\n",
      "Epoch 661 : Total Loss 4.569\n",
      "Epoch 671 : Total Loss 4.563\n",
      "Epoch 681 : Total Loss 4.558\n",
      "Epoch 691 : Total Loss 4.552\n",
      "Epoch 701 : Total Loss 4.546\n",
      "Epoch 711 : Total Loss 4.540\n",
      "Epoch 721 : Total Loss 4.534\n",
      "Epoch 731 : Total Loss 4.528\n",
      "Epoch 741 : Total Loss 4.522\n",
      "Epoch 751 : Total Loss 4.517\n",
      "Epoch 761 : Total Loss 4.511\n",
      "Epoch 771 : Total Loss 4.505\n",
      "Epoch 781 : Total Loss 4.499\n",
      "Epoch 791 : Total Loss 4.493\n",
      "Epoch 801 : Total Loss 4.487\n",
      "Epoch 811 : Total Loss 4.481\n",
      "Epoch 821 : Total Loss 4.474\n",
      "Epoch 831 : Total Loss 4.468\n",
      "Epoch 841 : Total Loss 4.462\n",
      "Epoch 851 : Total Loss 4.456\n",
      "Epoch 861 : Total Loss 4.450\n",
      "Epoch 871 : Total Loss 4.444\n",
      "Epoch 881 : Total Loss 4.438\n",
      "Epoch 891 : Total Loss 4.431\n",
      "Epoch 901 : Total Loss 4.425\n",
      "Epoch 911 : Total Loss 4.419\n",
      "Epoch 921 : Total Loss 4.413\n",
      "Epoch 931 : Total Loss 4.406\n",
      "Epoch 941 : Total Loss 4.400\n",
      "Epoch 951 : Total Loss 4.394\n",
      "Epoch 961 : Total Loss 4.388\n",
      "Epoch 971 : Total Loss 4.381\n",
      "Epoch 981 : Total Loss 4.375\n",
      "Epoch 991 : Total Loss 4.368\n",
      "Epoch 1001 : Total Loss 4.362\n",
      "Epoch 1011 : Total Loss 4.356\n",
      "Epoch 1021 : Total Loss 4.349\n",
      "Epoch 1031 : Total Loss 4.343\n",
      "Epoch 1041 : Total Loss 4.336\n",
      "Epoch 1051 : Total Loss 4.330\n",
      "Epoch 1061 : Total Loss 4.323\n",
      "Epoch 1071 : Total Loss 4.317\n",
      "Epoch 1081 : Total Loss 4.310\n",
      "Epoch 1091 : Total Loss 4.304\n",
      "Epoch 1101 : Total Loss 4.297\n",
      "Epoch 1111 : Total Loss 4.291\n",
      "Epoch 1121 : Total Loss 4.284\n",
      "Epoch 1131 : Total Loss 4.278\n",
      "Epoch 1141 : Total Loss 4.271\n",
      "Epoch 1151 : Total Loss 4.264\n",
      "Epoch 1161 : Total Loss 4.258\n",
      "Epoch 1171 : Total Loss 4.251\n",
      "Epoch 1181 : Total Loss 4.244\n",
      "Epoch 1191 : Total Loss 4.238\n",
      "Epoch 1201 : Total Loss 4.231\n",
      "Epoch 1211 : Total Loss 4.224\n",
      "Epoch 1221 : Total Loss 4.218\n",
      "Epoch 1231 : Total Loss 4.211\n",
      "Epoch 1241 : Total Loss 4.204\n",
      "Epoch 1251 : Total Loss 4.198\n",
      "Epoch 1261 : Total Loss 4.191\n",
      "Epoch 1271 : Total Loss 4.184\n",
      "Epoch 1281 : Total Loss 4.177\n",
      "Epoch 1291 : Total Loss 4.170\n",
      "Epoch 1301 : Total Loss 4.164\n",
      "Epoch 1311 : Total Loss 4.157\n",
      "Epoch 1321 : Total Loss 4.150\n",
      "Epoch 1331 : Total Loss 4.143\n",
      "Epoch 1341 : Total Loss 4.136\n",
      "Epoch 1351 : Total Loss 4.130\n",
      "Epoch 1361 : Total Loss 4.123\n",
      "Epoch 1371 : Total Loss 4.116\n",
      "Epoch 1381 : Total Loss 4.109\n",
      "Epoch 1391 : Total Loss 4.102\n",
      "Epoch 1401 : Total Loss 4.095\n",
      "Epoch 1411 : Total Loss 4.088\n",
      "Epoch 1421 : Total Loss 4.082\n",
      "Epoch 1431 : Total Loss 4.075\n",
      "Epoch 1441 : Total Loss 4.068\n",
      "Epoch 1451 : Total Loss 4.061\n",
      "Epoch 1461 : Total Loss 4.054\n",
      "Epoch 1471 : Total Loss 4.047\n",
      "Epoch 1481 : Total Loss 4.040\n",
      "Epoch 1491 : Total Loss 4.033\n",
      "Epoch 1501 : Total Loss 4.026\n",
      "Epoch 1511 : Total Loss 4.019\n",
      "Epoch 1521 : Total Loss 4.012\n",
      "Epoch 1531 : Total Loss 4.005\n",
      "Epoch 1541 : Total Loss 3.998\n",
      "Epoch 1551 : Total Loss 3.991\n",
      "Epoch 1561 : Total Loss 3.985\n",
      "Epoch 1571 : Total Loss 3.978\n",
      "Epoch 1581 : Total Loss 3.971\n",
      "Epoch 1591 : Total Loss 3.964\n",
      "Epoch 1601 : Total Loss 3.957\n",
      "Epoch 1611 : Total Loss 3.950\n",
      "Epoch 1621 : Total Loss 3.943\n",
      "Epoch 1631 : Total Loss 3.936\n",
      "Epoch 1641 : Total Loss 3.929\n",
      "Epoch 1651 : Total Loss 3.922\n",
      "Epoch 1661 : Total Loss 3.915\n",
      "Epoch 1671 : Total Loss 3.908\n",
      "Epoch 1681 : Total Loss 3.901\n",
      "Epoch 1691 : Total Loss 3.894\n",
      "Epoch 1701 : Total Loss 3.887\n",
      "Epoch 1711 : Total Loss 3.880\n",
      "Epoch 1721 : Total Loss 3.873\n",
      "Epoch 1731 : Total Loss 3.866\n",
      "Epoch 1741 : Total Loss 3.859\n",
      "Epoch 1751 : Total Loss 3.852\n",
      "Epoch 1761 : Total Loss 3.845\n",
      "Epoch 1771 : Total Loss 3.838\n",
      "Epoch 1781 : Total Loss 3.831\n",
      "Epoch 1791 : Total Loss 3.824\n",
      "Epoch 1801 : Total Loss 3.817\n",
      "Epoch 1811 : Total Loss 3.810\n",
      "Epoch 1821 : Total Loss 3.803\n",
      "Epoch 1831 : Total Loss 3.796\n",
      "Epoch 1841 : Total Loss 3.790\n",
      "Epoch 1851 : Total Loss 3.783\n",
      "Epoch 1861 : Total Loss 3.776\n",
      "Epoch 1871 : Total Loss 3.769\n",
      "Epoch 1881 : Total Loss 3.762\n",
      "Epoch 1891 : Total Loss 3.755\n",
      "Epoch 1901 : Total Loss 3.748\n",
      "Epoch 1911 : Total Loss 3.741\n",
      "Epoch 1921 : Total Loss 3.734\n",
      "Epoch 1931 : Total Loss 3.727\n",
      "Epoch 1941 : Total Loss 3.720\n",
      "Epoch 1951 : Total Loss 3.714\n",
      "Epoch 1961 : Total Loss 3.707\n",
      "Epoch 1971 : Total Loss 3.700\n",
      "Epoch 1981 : Total Loss 3.693\n",
      "Epoch 1991 : Total Loss 3.686\n",
      "Epoch 2001 : Total Loss 3.679\n",
      "Epoch 2011 : Total Loss 3.673\n",
      "Epoch 2021 : Total Loss 3.666\n",
      "Epoch 2031 : Total Loss 3.659\n",
      "Epoch 2041 : Total Loss 3.652\n",
      "Epoch 2051 : Total Loss 3.645\n",
      "Epoch 2061 : Total Loss 3.639\n",
      "Epoch 2071 : Total Loss 3.632\n",
      "Epoch 2081 : Total Loss 3.625\n",
      "Epoch 2091 : Total Loss 3.618\n",
      "Epoch 2101 : Total Loss 3.612\n",
      "Epoch 2111 : Total Loss 3.605\n",
      "Epoch 2121 : Total Loss 3.598\n",
      "Epoch 2131 : Total Loss 3.591\n",
      "Epoch 2141 : Total Loss 3.585\n",
      "Epoch 2151 : Total Loss 3.578\n",
      "Epoch 2161 : Total Loss 3.571\n",
      "Epoch 2171 : Total Loss 3.565\n",
      "Epoch 2181 : Total Loss 3.558\n",
      "Epoch 2191 : Total Loss 3.551\n",
      "Epoch 2201 : Total Loss 3.545\n",
      "Epoch 2211 : Total Loss 3.538\n",
      "Epoch 2221 : Total Loss 3.532\n",
      "Epoch 2231 : Total Loss 3.525\n",
      "Epoch 2241 : Total Loss 3.518\n",
      "Epoch 2251 : Total Loss 3.512\n",
      "Epoch 2261 : Total Loss 3.505\n",
      "Epoch 2271 : Total Loss 3.499\n",
      "Epoch 2281 : Total Loss 3.492\n",
      "Epoch 2291 : Total Loss 3.486\n",
      "Epoch 2301 : Total Loss 3.479\n",
      "Epoch 2311 : Total Loss 3.473\n",
      "Epoch 2321 : Total Loss 3.467\n",
      "Epoch 2331 : Total Loss 3.460\n",
      "Epoch 2341 : Total Loss 3.454\n",
      "Epoch 2351 : Total Loss 3.447\n",
      "Epoch 2361 : Total Loss 3.441\n",
      "Epoch 2371 : Total Loss 3.435\n",
      "Epoch 2381 : Total Loss 3.428\n",
      "Epoch 2391 : Total Loss 3.422\n",
      "Epoch 2401 : Total Loss 3.416\n",
      "Epoch 2411 : Total Loss 3.409\n",
      "Epoch 2421 : Total Loss 3.403\n",
      "Epoch 2431 : Total Loss 3.397\n",
      "Epoch 2441 : Total Loss 3.390\n",
      "Epoch 2451 : Total Loss 3.384\n",
      "Epoch 2461 : Total Loss 3.378\n",
      "Epoch 2471 : Total Loss 3.372\n",
      "Epoch 2481 : Total Loss 3.366\n",
      "Epoch 2491 : Total Loss 3.359\n",
      "Epoch 2501 : Total Loss 3.353\n",
      "Epoch 2511 : Total Loss 3.347\n",
      "Epoch 2521 : Total Loss 3.341\n",
      "Epoch 2531 : Total Loss 3.335\n",
      "Epoch 2541 : Total Loss 3.329\n",
      "Epoch 2551 : Total Loss 3.323\n",
      "Epoch 2561 : Total Loss 3.317\n",
      "Epoch 2571 : Total Loss 3.311\n",
      "Epoch 2581 : Total Loss 3.305\n",
      "Epoch 2591 : Total Loss 3.299\n",
      "Epoch 2601 : Total Loss 3.293\n",
      "Epoch 2611 : Total Loss 3.287\n",
      "Epoch 2621 : Total Loss 3.281\n",
      "Epoch 2631 : Total Loss 3.275\n",
      "Epoch 2641 : Total Loss 3.269\n",
      "Epoch 2651 : Total Loss 3.264\n",
      "Epoch 2661 : Total Loss 3.258\n",
      "Epoch 2671 : Total Loss 3.252\n",
      "Epoch 2681 : Total Loss 3.246\n",
      "Epoch 2691 : Total Loss 3.240\n",
      "Epoch 2701 : Total Loss 3.235\n",
      "Epoch 2711 : Total Loss 3.229\n",
      "Epoch 2721 : Total Loss 3.223\n",
      "Epoch 2731 : Total Loss 3.217\n",
      "Epoch 2741 : Total Loss 3.212\n",
      "Epoch 2751 : Total Loss 3.206\n",
      "Epoch 2761 : Total Loss 3.200\n",
      "Epoch 2771 : Total Loss 3.195\n",
      "Epoch 2781 : Total Loss 3.189\n",
      "Epoch 2791 : Total Loss 3.184\n",
      "Epoch 2801 : Total Loss 3.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2811 : Total Loss 3.173\n",
      "Epoch 2821 : Total Loss 3.167\n",
      "Epoch 2831 : Total Loss 3.162\n",
      "Epoch 2841 : Total Loss 3.156\n",
      "Epoch 2851 : Total Loss 3.151\n",
      "Epoch 2861 : Total Loss 3.145\n",
      "Epoch 2871 : Total Loss 3.140\n",
      "Epoch 2881 : Total Loss 3.135\n",
      "Epoch 2891 : Total Loss 3.129\n",
      "Epoch 2901 : Total Loss 3.124\n",
      "Epoch 2911 : Total Loss 3.119\n",
      "Epoch 2921 : Total Loss 3.113\n",
      "Epoch 2931 : Total Loss 3.108\n",
      "Epoch 2941 : Total Loss 3.103\n",
      "Epoch 2951 : Total Loss 3.098\n",
      "Epoch 2961 : Total Loss 3.092\n",
      "Epoch 2971 : Total Loss 3.087\n",
      "Epoch 2981 : Total Loss 3.082\n",
      "Epoch 2991 : Total Loss 3.077\n",
      "Epoch 3001 : Total Loss 3.072\n",
      "Epoch 3011 : Total Loss 3.067\n",
      "Epoch 3021 : Total Loss 3.062\n",
      "Epoch 3031 : Total Loss 3.057\n",
      "Epoch 3041 : Total Loss 3.052\n",
      "Epoch 3051 : Total Loss 3.047\n",
      "Epoch 3061 : Total Loss 3.042\n",
      "Epoch 3071 : Total Loss 3.037\n",
      "Epoch 3081 : Total Loss 3.032\n",
      "Epoch 3091 : Total Loss 3.027\n",
      "Epoch 3101 : Total Loss 3.022\n",
      "Epoch 3111 : Total Loss 3.017\n",
      "Epoch 3121 : Total Loss 3.012\n",
      "Epoch 3131 : Total Loss 3.007\n",
      "Epoch 3141 : Total Loss 3.002\n",
      "Epoch 3151 : Total Loss 2.998\n",
      "Epoch 3161 : Total Loss 2.993\n",
      "Epoch 3171 : Total Loss 2.988\n",
      "Epoch 3181 : Total Loss 2.983\n",
      "Epoch 3191 : Total Loss 2.979\n",
      "Epoch 3201 : Total Loss 2.974\n",
      "Epoch 3211 : Total Loss 2.969\n",
      "Epoch 3221 : Total Loss 2.965\n",
      "Epoch 3231 : Total Loss 2.960\n",
      "Epoch 3241 : Total Loss 2.956\n",
      "Epoch 3251 : Total Loss 2.951\n",
      "Epoch 3261 : Total Loss 2.946\n",
      "Epoch 3271 : Total Loss 2.942\n",
      "Epoch 3281 : Total Loss 2.937\n",
      "Epoch 3291 : Total Loss 2.933\n",
      "Epoch 3301 : Total Loss 2.928\n",
      "Epoch 3311 : Total Loss 2.924\n",
      "Epoch 3321 : Total Loss 2.920\n",
      "Epoch 3331 : Total Loss 2.915\n",
      "Epoch 3341 : Total Loss 2.911\n",
      "Epoch 3351 : Total Loss 2.907\n",
      "Epoch 3361 : Total Loss 2.902\n",
      "Epoch 3371 : Total Loss 2.898\n",
      "Epoch 3381 : Total Loss 2.894\n",
      "Epoch 3391 : Total Loss 2.889\n",
      "Epoch 3401 : Total Loss 2.885\n",
      "Epoch 3411 : Total Loss 2.881\n",
      "Epoch 3421 : Total Loss 2.877\n",
      "Epoch 3431 : Total Loss 2.872\n",
      "Epoch 3441 : Total Loss 2.868\n",
      "Epoch 3451 : Total Loss 2.864\n",
      "Epoch 3461 : Total Loss 2.860\n",
      "Epoch 3471 : Total Loss 2.856\n",
      "Epoch 3481 : Total Loss 2.852\n",
      "Epoch 3491 : Total Loss 2.848\n",
      "Epoch 3501 : Total Loss 2.844\n",
      "Epoch 3511 : Total Loss 2.840\n",
      "Epoch 3521 : Total Loss 2.836\n",
      "Epoch 3531 : Total Loss 2.832\n",
      "Epoch 3541 : Total Loss 2.828\n",
      "Epoch 3551 : Total Loss 2.824\n",
      "Epoch 3561 : Total Loss 2.820\n",
      "Epoch 3571 : Total Loss 2.816\n",
      "Epoch 3581 : Total Loss 2.812\n",
      "Epoch 3591 : Total Loss 2.808\n",
      "Epoch 3601 : Total Loss 2.804\n",
      "Epoch 3611 : Total Loss 2.801\n",
      "Epoch 3621 : Total Loss 2.797\n",
      "Epoch 3631 : Total Loss 2.793\n",
      "Epoch 3641 : Total Loss 2.789\n",
      "Epoch 3651 : Total Loss 2.786\n",
      "Epoch 3661 : Total Loss 2.782\n",
      "Epoch 3671 : Total Loss 2.778\n",
      "Epoch 3681 : Total Loss 2.774\n",
      "Epoch 3691 : Total Loss 2.771\n",
      "Epoch 3701 : Total Loss 2.767\n",
      "Epoch 3711 : Total Loss 2.763\n",
      "Epoch 3721 : Total Loss 2.760\n",
      "Epoch 3731 : Total Loss 2.756\n",
      "Epoch 3741 : Total Loss 2.753\n",
      "Epoch 3751 : Total Loss 2.749\n",
      "Epoch 3761 : Total Loss 2.746\n",
      "Epoch 3771 : Total Loss 2.742\n",
      "Epoch 3781 : Total Loss 2.739\n",
      "Epoch 3791 : Total Loss 2.735\n",
      "Epoch 3801 : Total Loss 2.732\n",
      "Epoch 3811 : Total Loss 2.728\n",
      "Epoch 3821 : Total Loss 2.725\n",
      "Epoch 3831 : Total Loss 2.721\n",
      "Epoch 3841 : Total Loss 2.718\n",
      "Epoch 3851 : Total Loss 2.715\n",
      "Epoch 3861 : Total Loss 2.711\n",
      "Epoch 3871 : Total Loss 2.708\n",
      "Epoch 3881 : Total Loss 2.705\n",
      "Epoch 3891 : Total Loss 2.701\n",
      "Epoch 3901 : Total Loss 2.698\n",
      "Epoch 3911 : Total Loss 2.695\n",
      "Epoch 3921 : Total Loss 2.692\n",
      "Epoch 3931 : Total Loss 2.688\n",
      "Epoch 3941 : Total Loss 2.685\n",
      "Epoch 3951 : Total Loss 2.682\n",
      "Epoch 3961 : Total Loss 2.679\n",
      "Epoch 3971 : Total Loss 2.676\n",
      "Epoch 3981 : Total Loss 2.673\n",
      "Epoch 3991 : Total Loss 2.669\n",
      "Epoch 4001 : Total Loss 2.666\n",
      "Epoch 4011 : Total Loss 2.663\n",
      "Epoch 4021 : Total Loss 2.660\n",
      "Epoch 4031 : Total Loss 2.657\n",
      "Epoch 4041 : Total Loss 2.654\n",
      "Epoch 4051 : Total Loss 2.651\n",
      "Epoch 4061 : Total Loss 2.648\n",
      "Epoch 4071 : Total Loss 2.645\n",
      "Epoch 4081 : Total Loss 2.642\n",
      "Epoch 4091 : Total Loss 2.639\n",
      "Epoch 4101 : Total Loss 2.636\n",
      "Epoch 4111 : Total Loss 2.633\n",
      "Epoch 4121 : Total Loss 2.630\n",
      "Epoch 4131 : Total Loss 2.627\n",
      "Epoch 4141 : Total Loss 2.625\n",
      "Epoch 4151 : Total Loss 2.622\n",
      "Epoch 4161 : Total Loss 2.619\n",
      "Epoch 4171 : Total Loss 2.616\n",
      "Epoch 4181 : Total Loss 2.613\n",
      "Epoch 4191 : Total Loss 2.610\n",
      "Epoch 4201 : Total Loss 2.608\n",
      "Epoch 4211 : Total Loss 2.605\n",
      "Epoch 4221 : Total Loss 2.602\n",
      "Epoch 4231 : Total Loss 2.599\n",
      "Epoch 4241 : Total Loss 2.597\n",
      "Epoch 4251 : Total Loss 2.594\n",
      "Epoch 4261 : Total Loss 2.591\n",
      "Epoch 4271 : Total Loss 2.589\n",
      "Epoch 4281 : Total Loss 2.586\n",
      "Epoch 4291 : Total Loss 2.583\n",
      "Epoch 4301 : Total Loss 2.581\n",
      "Epoch 4311 : Total Loss 2.578\n",
      "Epoch 4321 : Total Loss 2.575\n",
      "Epoch 4331 : Total Loss 2.573\n",
      "Epoch 4341 : Total Loss 2.570\n",
      "Epoch 4351 : Total Loss 2.568\n",
      "Epoch 4361 : Total Loss 2.565\n",
      "Epoch 4371 : Total Loss 2.563\n",
      "Epoch 4381 : Total Loss 2.560\n",
      "Epoch 4391 : Total Loss 2.558\n",
      "Epoch 4401 : Total Loss 2.555\n",
      "Epoch 4411 : Total Loss 2.553\n",
      "Epoch 4421 : Total Loss 2.550\n",
      "Epoch 4431 : Total Loss 2.548\n",
      "Epoch 4441 : Total Loss 2.545\n",
      "Epoch 4451 : Total Loss 2.543\n",
      "Epoch 4461 : Total Loss 2.540\n",
      "Epoch 4471 : Total Loss 2.538\n",
      "Epoch 4481 : Total Loss 2.536\n",
      "Epoch 4491 : Total Loss 2.533\n",
      "Epoch 4501 : Total Loss 2.531\n",
      "Epoch 4511 : Total Loss 2.528\n",
      "Epoch 4521 : Total Loss 2.526\n",
      "Epoch 4531 : Total Loss 2.524\n",
      "Epoch 4541 : Total Loss 2.521\n",
      "Epoch 4551 : Total Loss 2.519\n",
      "Epoch 4561 : Total Loss 2.517\n",
      "Epoch 4571 : Total Loss 2.515\n",
      "Epoch 4581 : Total Loss 2.512\n",
      "Epoch 4591 : Total Loss 2.510\n",
      "Epoch 4601 : Total Loss 2.508\n",
      "Epoch 4611 : Total Loss 2.506\n",
      "Epoch 4621 : Total Loss 2.503\n",
      "Epoch 4631 : Total Loss 2.501\n",
      "Epoch 4641 : Total Loss 2.499\n",
      "Epoch 4651 : Total Loss 2.497\n",
      "Epoch 4661 : Total Loss 2.495\n",
      "Epoch 4671 : Total Loss 2.493\n",
      "Epoch 4681 : Total Loss 2.490\n",
      "Epoch 4691 : Total Loss 2.488\n",
      "Epoch 4701 : Total Loss 2.486\n",
      "Epoch 4711 : Total Loss 2.484\n",
      "Epoch 4721 : Total Loss 2.482\n",
      "Epoch 4731 : Total Loss 2.480\n",
      "Epoch 4741 : Total Loss 2.478\n",
      "Epoch 4751 : Total Loss 2.476\n",
      "Epoch 4761 : Total Loss 2.474\n",
      "Epoch 4771 : Total Loss 2.472\n",
      "Epoch 4781 : Total Loss 2.470\n",
      "Epoch 4791 : Total Loss 2.468\n",
      "Epoch 4801 : Total Loss 2.466\n",
      "Epoch 4811 : Total Loss 2.464\n",
      "Epoch 4821 : Total Loss 2.462\n",
      "Epoch 4831 : Total Loss 2.460\n",
      "Epoch 4841 : Total Loss 2.458\n",
      "Epoch 4851 : Total Loss 2.456\n",
      "Epoch 4861 : Total Loss 2.454\n",
      "Epoch 4871 : Total Loss 2.452\n",
      "Epoch 4881 : Total Loss 2.450\n",
      "Epoch 4891 : Total Loss 2.448\n",
      "Epoch 4901 : Total Loss 2.446\n",
      "Epoch 4911 : Total Loss 2.444\n",
      "Epoch 4921 : Total Loss 2.442\n",
      "Epoch 4931 : Total Loss 2.440\n",
      "Epoch 4941 : Total Loss 2.438\n",
      "Epoch 4951 : Total Loss 2.437\n",
      "Epoch 4961 : Total Loss 2.435\n",
      "Epoch 4971 : Total Loss 2.433\n",
      "Epoch 4981 : Total Loss 2.431\n",
      "Epoch 4991 : Total Loss 2.429\n",
      "The runtime of the sparse matrix factorization is  5.472963999956846 seconds.\n"
     ]
    }
   ],
   "source": [
    "start2 = t.default_timer()\n",
    "\n",
    "U_hat_sparse, V_hat_sparse = matrix_factorization_sparse(A, U, V, user_features, 5000)\n",
    "\n",
    "end2 = t.default_timer()\n",
    "\n",
    "print('The runtime of the sparse matrix factorization is ', end2 - start2, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86909283",
   "metadata": {},
   "source": [
    "There is not a significant difference on these, however, the $A$ that we used in the **above example is not sparse at all.** 14/25 entries are nonzero, so the sparsity is only 44%. **We will try this method on a significantly more sparse matrix.**\n",
    "\n",
    "## Test on a Large Sparse Matrix\n",
    "\n",
    "**This will be a 100 x 100 sparse matrix with 2 latent features, and it will be 99% sparse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f57a545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate random sparse matrix\n",
    "from scipy.sparse import random\n",
    "from scipy.stats import randint\n",
    "arr_rvs = randint(0, 6).rvs\n",
    "\n",
    "x = random(100,100, density = 0.01, data_rvs = arr_rvs)\n",
    "\n",
    "arr_100_100 = np.array(x.toarray())\n",
    "\n",
    "arr_100_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd8d275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4.0,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  5.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  3.0],\n",
       " 84)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_ind = [(i,j) for i in range(arr_100_100.shape[0]) for j in range(arr_100_100.shape[1]) if arr_100_100[i,j]!=0]\n",
    "\n",
    "li_arr_100_100 = [arr_100_100[i,j] for i,j in li_ind]\n",
    "\n",
    "li_arr_100_100, len(li_arr_100_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85a5c1",
   "metadata": {},
   "source": [
    "### Run the Sparse Matrix Factorization on our Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b300ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 598.880\n",
      "Epoch 11 : Total Loss 590.512\n",
      "Epoch 21 : Total Loss 582.027\n",
      "Epoch 31 : Total Loss 573.428\n",
      "Epoch 41 : Total Loss 564.718\n",
      "Epoch 51 : Total Loss 555.900\n",
      "Epoch 61 : Total Loss 546.979\n",
      "Epoch 71 : Total Loss 537.961\n",
      "Epoch 81 : Total Loss 528.851\n",
      "Epoch 91 : Total Loss 519.656\n",
      "Epoch 101 : Total Loss 510.385\n",
      "Epoch 111 : Total Loss 501.045\n",
      "Epoch 121 : Total Loss 491.646\n",
      "Epoch 131 : Total Loss 482.197\n",
      "Epoch 141 : Total Loss 472.709\n",
      "Epoch 151 : Total Loss 463.193\n",
      "Epoch 161 : Total Loss 453.660\n",
      "Epoch 171 : Total Loss 444.122\n",
      "Epoch 181 : Total Loss 434.591\n",
      "Epoch 191 : Total Loss 425.079\n",
      "Epoch 201 : Total Loss 415.598\n",
      "Epoch 211 : Total Loss 406.161\n",
      "Epoch 221 : Total Loss 396.780\n",
      "Epoch 231 : Total Loss 387.465\n",
      "Epoch 241 : Total Loss 378.230\n",
      "Epoch 251 : Total Loss 369.084\n",
      "Epoch 261 : Total Loss 360.038\n",
      "Epoch 271 : Total Loss 351.103\n",
      "Epoch 281 : Total Loss 342.286\n",
      "Epoch 291 : Total Loss 333.597\n",
      "Epoch 301 : Total Loss 325.042\n",
      "Epoch 311 : Total Loss 316.630\n",
      "Epoch 321 : Total Loss 308.365\n",
      "Epoch 331 : Total Loss 300.253\n",
      "Epoch 341 : Total Loss 292.298\n",
      "Epoch 351 : Total Loss 284.504\n",
      "Epoch 361 : Total Loss 276.873\n",
      "Epoch 371 : Total Loss 269.408\n",
      "Epoch 381 : Total Loss 262.110\n",
      "Epoch 391 : Total Loss 254.979\n",
      "Epoch 401 : Total Loss 248.016\n",
      "Epoch 411 : Total Loss 241.221\n",
      "Epoch 421 : Total Loss 234.594\n",
      "Epoch 431 : Total Loss 228.132\n",
      "Epoch 441 : Total Loss 221.835\n",
      "Epoch 451 : Total Loss 215.701\n",
      "Epoch 461 : Total Loss 209.729\n",
      "Epoch 471 : Total Loss 203.915\n",
      "Epoch 481 : Total Loss 198.259\n",
      "Epoch 491 : Total Loss 192.757\n",
      "Epoch 501 : Total Loss 187.408\n",
      "Epoch 511 : Total Loss 182.208\n",
      "Epoch 521 : Total Loss 177.155\n",
      "Epoch 531 : Total Loss 172.247\n",
      "Epoch 541 : Total Loss 167.481\n",
      "Epoch 551 : Total Loss 162.854\n",
      "Epoch 561 : Total Loss 158.364\n",
      "Epoch 571 : Total Loss 154.008\n",
      "Epoch 581 : Total Loss 149.783\n",
      "Epoch 591 : Total Loss 145.687\n",
      "Epoch 601 : Total Loss 141.717\n",
      "Epoch 611 : Total Loss 137.869\n",
      "Epoch 621 : Total Loss 134.143\n",
      "Epoch 631 : Total Loss 130.534\n",
      "Epoch 641 : Total Loss 127.039\n",
      "Epoch 651 : Total Loss 123.657\n",
      "Epoch 661 : Total Loss 120.384\n",
      "Epoch 671 : Total Loss 117.218\n",
      "Epoch 681 : Total Loss 114.156\n",
      "Epoch 691 : Total Loss 111.194\n",
      "Epoch 701 : Total Loss 108.331\n",
      "Epoch 711 : Total Loss 105.563\n",
      "Epoch 721 : Total Loss 102.887\n",
      "Epoch 731 : Total Loss 100.301\n",
      "Epoch 741 : Total Loss 97.802\n",
      "Epoch 751 : Total Loss 95.387\n",
      "Epoch 761 : Total Loss 93.053\n",
      "Epoch 771 : Total Loss 90.798\n",
      "Epoch 781 : Total Loss 88.618\n",
      "Epoch 791 : Total Loss 86.512\n",
      "Epoch 801 : Total Loss 84.476\n",
      "Epoch 811 : Total Loss 82.508\n",
      "Epoch 821 : Total Loss 80.606\n",
      "Epoch 831 : Total Loss 78.766\n",
      "Epoch 841 : Total Loss 76.987\n",
      "Epoch 851 : Total Loss 75.265\n",
      "Epoch 861 : Total Loss 73.600\n",
      "Epoch 871 : Total Loss 71.988\n",
      "Epoch 881 : Total Loss 70.427\n",
      "Epoch 891 : Total Loss 68.916\n",
      "Epoch 901 : Total Loss 67.451\n",
      "Epoch 911 : Total Loss 66.032\n",
      "Epoch 921 : Total Loss 64.657\n",
      "Epoch 931 : Total Loss 63.323\n",
      "Epoch 941 : Total Loss 62.029\n",
      "Epoch 951 : Total Loss 60.773\n",
      "Epoch 961 : Total Loss 59.553\n",
      "Epoch 971 : Total Loss 58.369\n",
      "Epoch 981 : Total Loss 57.218\n",
      "Epoch 991 : Total Loss 56.100\n",
      "Epoch 1001 : Total Loss 55.013\n",
      "Epoch 1011 : Total Loss 53.955\n",
      "Epoch 1021 : Total Loss 52.926\n",
      "Epoch 1031 : Total Loss 51.924\n",
      "Epoch 1041 : Total Loss 50.949\n",
      "Epoch 1051 : Total Loss 49.999\n",
      "Epoch 1061 : Total Loss 49.074\n",
      "Epoch 1071 : Total Loss 48.172\n",
      "Epoch 1081 : Total Loss 47.292\n",
      "Epoch 1091 : Total Loss 46.435\n",
      "Epoch 1101 : Total Loss 45.599\n",
      "Epoch 1111 : Total Loss 44.783\n",
      "Epoch 1121 : Total Loss 43.987\n",
      "Epoch 1131 : Total Loss 43.209\n",
      "Epoch 1141 : Total Loss 42.451\n",
      "Epoch 1151 : Total Loss 41.710\n",
      "Epoch 1161 : Total Loss 40.986\n",
      "Epoch 1171 : Total Loss 40.280\n",
      "Epoch 1181 : Total Loss 39.589\n",
      "Epoch 1191 : Total Loss 38.915\n",
      "Epoch 1201 : Total Loss 38.256\n",
      "Epoch 1211 : Total Loss 37.612\n",
      "Epoch 1221 : Total Loss 36.982\n",
      "Epoch 1231 : Total Loss 36.367\n",
      "Epoch 1241 : Total Loss 35.765\n",
      "Epoch 1251 : Total Loss 35.177\n",
      "Epoch 1261 : Total Loss 34.602\n",
      "Epoch 1271 : Total Loss 34.040\n",
      "Epoch 1281 : Total Loss 33.491\n",
      "Epoch 1291 : Total Loss 32.953\n",
      "Epoch 1301 : Total Loss 32.428\n",
      "Epoch 1311 : Total Loss 31.914\n",
      "Epoch 1321 : Total Loss 31.412\n",
      "Epoch 1331 : Total Loss 30.921\n",
      "Epoch 1341 : Total Loss 30.440\n",
      "Epoch 1351 : Total Loss 29.970\n",
      "Epoch 1361 : Total Loss 29.511\n",
      "Epoch 1371 : Total Loss 29.062\n",
      "Epoch 1381 : Total Loss 28.622\n",
      "Epoch 1391 : Total Loss 28.193\n",
      "Epoch 1401 : Total Loss 27.772\n",
      "Epoch 1411 : Total Loss 27.362\n",
      "Epoch 1421 : Total Loss 26.960\n",
      "Epoch 1431 : Total Loss 26.567\n",
      "Epoch 1441 : Total Loss 26.183\n",
      "Epoch 1451 : Total Loss 25.807\n",
      "Epoch 1461 : Total Loss 25.440\n",
      "Epoch 1471 : Total Loss 25.080\n",
      "Epoch 1481 : Total Loss 24.729\n",
      "Epoch 1491 : Total Loss 24.386\n",
      "Epoch 1501 : Total Loss 24.050\n",
      "Epoch 1511 : Total Loss 23.721\n",
      "Epoch 1521 : Total Loss 23.400\n",
      "Epoch 1531 : Total Loss 23.086\n",
      "Epoch 1541 : Total Loss 22.779\n",
      "Epoch 1551 : Total Loss 22.479\n",
      "Epoch 1561 : Total Loss 22.185\n",
      "Epoch 1571 : Total Loss 21.898\n",
      "Epoch 1581 : Total Loss 21.617\n",
      "Epoch 1591 : Total Loss 21.343\n",
      "Epoch 1601 : Total Loss 21.075\n",
      "Epoch 1611 : Total Loss 20.812\n",
      "Epoch 1621 : Total Loss 20.556\n",
      "Epoch 1631 : Total Loss 20.305\n",
      "Epoch 1641 : Total Loss 20.059\n",
      "Epoch 1651 : Total Loss 19.819\n",
      "Epoch 1661 : Total Loss 19.585\n",
      "Epoch 1671 : Total Loss 19.355\n",
      "Epoch 1681 : Total Loss 19.131\n",
      "Epoch 1691 : Total Loss 18.911\n",
      "Epoch 1701 : Total Loss 18.697\n",
      "Epoch 1711 : Total Loss 18.487\n",
      "Epoch 1721 : Total Loss 18.282\n",
      "Epoch 1731 : Total Loss 18.081\n",
      "Epoch 1741 : Total Loss 17.885\n",
      "Epoch 1751 : Total Loss 17.693\n",
      "Epoch 1761 : Total Loss 17.505\n",
      "Epoch 1771 : Total Loss 17.321\n",
      "Epoch 1781 : Total Loss 17.141\n",
      "Epoch 1791 : Total Loss 16.966\n",
      "Epoch 1801 : Total Loss 16.794\n",
      "Epoch 1811 : Total Loss 16.625\n",
      "Epoch 1821 : Total Loss 16.461\n",
      "Epoch 1831 : Total Loss 16.300\n",
      "Epoch 1841 : Total Loss 16.142\n",
      "Epoch 1851 : Total Loss 15.988\n",
      "Epoch 1861 : Total Loss 15.837\n",
      "Epoch 1871 : Total Loss 15.689\n",
      "Epoch 1881 : Total Loss 15.545\n",
      "Epoch 1891 : Total Loss 15.403\n",
      "Epoch 1901 : Total Loss 15.265\n",
      "Epoch 1911 : Total Loss 15.130\n",
      "Epoch 1921 : Total Loss 14.997\n",
      "Epoch 1931 : Total Loss 14.867\n",
      "Epoch 1941 : Total Loss 14.740\n",
      "Epoch 1951 : Total Loss 14.615\n",
      "Epoch 1961 : Total Loss 14.493\n",
      "Epoch 1971 : Total Loss 14.374\n",
      "Epoch 1981 : Total Loss 14.257\n",
      "Epoch 1991 : Total Loss 14.142\n",
      "Epoch 2001 : Total Loss 14.030\n",
      "Epoch 2011 : Total Loss 13.920\n",
      "Epoch 2021 : Total Loss 13.812\n",
      "Epoch 2031 : Total Loss 13.707\n",
      "Epoch 2041 : Total Loss 13.603\n",
      "Epoch 2051 : Total Loss 13.502\n",
      "Epoch 2061 : Total Loss 13.403\n",
      "Epoch 2071 : Total Loss 13.305\n",
      "Epoch 2081 : Total Loss 13.210\n",
      "Epoch 2091 : Total Loss 13.116\n",
      "Epoch 2101 : Total Loss 13.024\n",
      "Epoch 2111 : Total Loss 12.934\n",
      "Epoch 2121 : Total Loss 12.846\n",
      "Epoch 2131 : Total Loss 12.759\n",
      "Epoch 2141 : Total Loss 12.674\n",
      "Epoch 2151 : Total Loss 12.590\n",
      "Epoch 2161 : Total Loss 12.508\n",
      "Epoch 2171 : Total Loss 12.428\n",
      "Epoch 2181 : Total Loss 12.349\n",
      "Epoch 2191 : Total Loss 12.272\n",
      "Epoch 2201 : Total Loss 12.195\n",
      "Epoch 2211 : Total Loss 12.121\n",
      "Epoch 2221 : Total Loss 12.047\n",
      "Epoch 2231 : Total Loss 11.975\n",
      "Epoch 2241 : Total Loss 11.904\n",
      "Epoch 2251 : Total Loss 11.835\n",
      "Epoch 2261 : Total Loss 11.766\n",
      "Epoch 2271 : Total Loss 11.699\n",
      "Epoch 2281 : Total Loss 11.633\n",
      "Epoch 2291 : Total Loss 11.568\n",
      "Epoch 2301 : Total Loss 11.504\n",
      "Epoch 2311 : Total Loss 11.441\n",
      "Epoch 2321 : Total Loss 11.379\n",
      "Epoch 2331 : Total Loss 11.318\n",
      "Epoch 2341 : Total Loss 11.258\n",
      "Epoch 2351 : Total Loss 11.199\n",
      "Epoch 2361 : Total Loss 11.141\n",
      "Epoch 2371 : Total Loss 11.084\n",
      "Epoch 2381 : Total Loss 11.028\n",
      "Epoch 2391 : Total Loss 10.973\n",
      "Epoch 2401 : Total Loss 10.918\n",
      "Epoch 2411 : Total Loss 10.865\n",
      "Epoch 2421 : Total Loss 10.812\n",
      "Epoch 2431 : Total Loss 10.760\n",
      "Epoch 2441 : Total Loss 10.708\n",
      "Epoch 2451 : Total Loss 10.658\n",
      "Epoch 2461 : Total Loss 10.608\n",
      "Epoch 2471 : Total Loss 10.559\n",
      "Epoch 2481 : Total Loss 10.510\n",
      "Epoch 2491 : Total Loss 10.462\n",
      "Epoch 2501 : Total Loss 10.415\n",
      "Epoch 2511 : Total Loss 10.369\n",
      "Epoch 2521 : Total Loss 10.323\n",
      "Epoch 2531 : Total Loss 10.278\n",
      "Epoch 2541 : Total Loss 10.233\n",
      "Epoch 2551 : Total Loss 10.189\n",
      "Epoch 2561 : Total Loss 10.145\n",
      "Epoch 2571 : Total Loss 10.102\n",
      "Epoch 2581 : Total Loss 10.060\n",
      "Epoch 2591 : Total Loss 10.018\n",
      "Epoch 2601 : Total Loss 9.977\n",
      "Epoch 2611 : Total Loss 9.936\n",
      "Epoch 2621 : Total Loss 9.896\n",
      "Epoch 2631 : Total Loss 9.856\n",
      "Epoch 2641 : Total Loss 9.817\n",
      "Epoch 2651 : Total Loss 9.778\n",
      "Epoch 2661 : Total Loss 9.739\n",
      "Epoch 2671 : Total Loss 9.702\n",
      "Epoch 2681 : Total Loss 9.664\n",
      "Epoch 2691 : Total Loss 9.627\n",
      "Epoch 2701 : Total Loss 9.590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2711 : Total Loss 9.554\n",
      "Epoch 2721 : Total Loss 9.518\n",
      "Epoch 2731 : Total Loss 9.483\n",
      "Epoch 2741 : Total Loss 9.448\n",
      "Epoch 2751 : Total Loss 9.413\n",
      "Epoch 2761 : Total Loss 9.379\n",
      "Epoch 2771 : Total Loss 9.345\n",
      "Epoch 2781 : Total Loss 9.312\n",
      "Epoch 2791 : Total Loss 9.279\n",
      "Epoch 2801 : Total Loss 9.246\n",
      "Epoch 2811 : Total Loss 9.213\n",
      "Epoch 2821 : Total Loss 9.181\n",
      "Epoch 2831 : Total Loss 9.150\n",
      "Epoch 2841 : Total Loss 9.118\n",
      "Epoch 2851 : Total Loss 9.087\n",
      "Epoch 2861 : Total Loss 9.056\n",
      "Epoch 2871 : Total Loss 9.026\n",
      "Epoch 2881 : Total Loss 8.996\n",
      "Epoch 2891 : Total Loss 8.966\n",
      "Epoch 2901 : Total Loss 8.936\n",
      "Epoch 2911 : Total Loss 8.907\n",
      "Epoch 2921 : Total Loss 8.878\n",
      "Epoch 2931 : Total Loss 8.849\n",
      "Epoch 2941 : Total Loss 8.821\n",
      "Epoch 2951 : Total Loss 8.793\n",
      "Epoch 2961 : Total Loss 8.765\n",
      "Epoch 2971 : Total Loss 8.737\n",
      "Epoch 2981 : Total Loss 8.710\n",
      "Epoch 2991 : Total Loss 8.683\n",
      "Epoch 3001 : Total Loss 8.656\n",
      "Epoch 3011 : Total Loss 8.629\n",
      "Epoch 3021 : Total Loss 8.603\n",
      "Epoch 3031 : Total Loss 8.577\n",
      "Epoch 3041 : Total Loss 8.551\n",
      "Epoch 3051 : Total Loss 8.526\n",
      "Epoch 3061 : Total Loss 8.500\n",
      "Epoch 3071 : Total Loss 8.475\n",
      "Epoch 3081 : Total Loss 8.450\n",
      "Epoch 3091 : Total Loss 8.426\n",
      "Epoch 3101 : Total Loss 8.401\n",
      "Epoch 3111 : Total Loss 8.377\n",
      "Epoch 3121 : Total Loss 8.353\n",
      "Epoch 3131 : Total Loss 8.329\n",
      "Epoch 3141 : Total Loss 8.306\n",
      "Epoch 3151 : Total Loss 8.283\n",
      "Epoch 3161 : Total Loss 8.260\n",
      "Epoch 3171 : Total Loss 8.237\n",
      "Epoch 3181 : Total Loss 8.214\n",
      "Epoch 3191 : Total Loss 8.191\n",
      "Epoch 3201 : Total Loss 8.169\n",
      "Epoch 3211 : Total Loss 8.147\n",
      "Epoch 3221 : Total Loss 8.125\n",
      "Epoch 3231 : Total Loss 8.103\n",
      "Epoch 3241 : Total Loss 8.082\n",
      "Epoch 3251 : Total Loss 8.061\n",
      "Epoch 3261 : Total Loss 8.040\n",
      "Epoch 3271 : Total Loss 8.019\n",
      "Epoch 3281 : Total Loss 7.998\n",
      "Epoch 3291 : Total Loss 7.977\n",
      "Epoch 3301 : Total Loss 7.957\n",
      "Epoch 3311 : Total Loss 7.937\n",
      "Epoch 3321 : Total Loss 7.917\n",
      "Epoch 3331 : Total Loss 7.897\n",
      "Epoch 3341 : Total Loss 7.877\n",
      "Epoch 3351 : Total Loss 7.858\n",
      "Epoch 3361 : Total Loss 7.838\n",
      "Epoch 3371 : Total Loss 7.819\n",
      "Epoch 3381 : Total Loss 7.800\n",
      "Epoch 3391 : Total Loss 7.781\n",
      "Epoch 3401 : Total Loss 7.763\n",
      "Epoch 3411 : Total Loss 7.744\n",
      "Epoch 3421 : Total Loss 7.726\n",
      "Epoch 3431 : Total Loss 7.708\n",
      "Epoch 3441 : Total Loss 7.690\n",
      "Epoch 3451 : Total Loss 7.672\n",
      "Epoch 3461 : Total Loss 7.654\n",
      "Epoch 3471 : Total Loss 7.636\n",
      "Epoch 3481 : Total Loss 7.619\n",
      "Epoch 3491 : Total Loss 7.602\n",
      "Epoch 3501 : Total Loss 7.585\n",
      "Epoch 3511 : Total Loss 7.568\n",
      "Epoch 3521 : Total Loss 7.551\n",
      "Epoch 3531 : Total Loss 7.534\n",
      "Epoch 3541 : Total Loss 7.518\n",
      "Epoch 3551 : Total Loss 7.501\n",
      "Epoch 3561 : Total Loss 7.485\n",
      "Epoch 3571 : Total Loss 7.469\n",
      "Epoch 3581 : Total Loss 7.453\n",
      "Epoch 3591 : Total Loss 7.437\n",
      "Epoch 3601 : Total Loss 7.422\n",
      "Epoch 3611 : Total Loss 7.406\n",
      "Epoch 3621 : Total Loss 7.391\n",
      "Epoch 3631 : Total Loss 7.376\n",
      "Epoch 3641 : Total Loss 7.361\n",
      "Epoch 3651 : Total Loss 7.346\n",
      "Epoch 3661 : Total Loss 7.331\n",
      "Epoch 3671 : Total Loss 7.316\n",
      "Epoch 3681 : Total Loss 7.301\n",
      "Epoch 3691 : Total Loss 7.287\n",
      "Epoch 3701 : Total Loss 7.273\n",
      "Epoch 3711 : Total Loss 7.258\n",
      "Epoch 3721 : Total Loss 7.244\n",
      "Epoch 3731 : Total Loss 7.230\n",
      "Epoch 3741 : Total Loss 7.217\n",
      "Epoch 3751 : Total Loss 7.203\n",
      "Epoch 3761 : Total Loss 7.189\n",
      "Epoch 3771 : Total Loss 7.176\n",
      "Epoch 3781 : Total Loss 7.162\n",
      "Epoch 3791 : Total Loss 7.149\n",
      "Epoch 3801 : Total Loss 7.136\n",
      "Epoch 3811 : Total Loss 7.123\n",
      "Epoch 3821 : Total Loss 7.110\n",
      "Epoch 3831 : Total Loss 7.098\n",
      "Epoch 3841 : Total Loss 7.085\n",
      "Epoch 3851 : Total Loss 7.072\n",
      "Epoch 3861 : Total Loss 7.060\n",
      "Epoch 3871 : Total Loss 7.048\n",
      "Epoch 3881 : Total Loss 7.035\n",
      "Epoch 3891 : Total Loss 7.023\n",
      "Epoch 3901 : Total Loss 7.011\n",
      "Epoch 3911 : Total Loss 7.000\n",
      "Epoch 3921 : Total Loss 6.988\n",
      "Epoch 3931 : Total Loss 6.976\n",
      "Epoch 3941 : Total Loss 6.965\n",
      "Epoch 3951 : Total Loss 6.953\n",
      "Epoch 3961 : Total Loss 6.942\n",
      "Epoch 3971 : Total Loss 6.931\n",
      "Epoch 3981 : Total Loss 6.919\n",
      "Epoch 3991 : Total Loss 6.908\n",
      "Epoch 4001 : Total Loss 6.897\n",
      "Epoch 4011 : Total Loss 6.887\n",
      "Epoch 4021 : Total Loss 6.876\n",
      "Epoch 4031 : Total Loss 6.865\n",
      "Epoch 4041 : Total Loss 6.855\n",
      "Epoch 4051 : Total Loss 6.844\n",
      "Epoch 4061 : Total Loss 6.834\n",
      "Epoch 4071 : Total Loss 6.824\n",
      "Epoch 4081 : Total Loss 6.813\n",
      "Epoch 4091 : Total Loss 6.803\n",
      "Epoch 4101 : Total Loss 6.793\n",
      "Epoch 4111 : Total Loss 6.783\n",
      "Epoch 4121 : Total Loss 6.774\n",
      "Epoch 4131 : Total Loss 6.764\n",
      "Epoch 4141 : Total Loss 6.754\n",
      "Epoch 4151 : Total Loss 6.745\n",
      "Epoch 4161 : Total Loss 6.735\n",
      "Epoch 4171 : Total Loss 6.726\n",
      "Epoch 4181 : Total Loss 6.717\n",
      "Epoch 4191 : Total Loss 6.707\n",
      "Epoch 4201 : Total Loss 6.698\n",
      "Epoch 4211 : Total Loss 6.689\n",
      "Epoch 4221 : Total Loss 6.680\n",
      "Epoch 4231 : Total Loss 6.672\n",
      "Epoch 4241 : Total Loss 6.663\n",
      "Epoch 4251 : Total Loss 6.654\n",
      "Epoch 4261 : Total Loss 6.645\n",
      "Epoch 4271 : Total Loss 6.637\n",
      "Epoch 4281 : Total Loss 6.628\n",
      "Epoch 4291 : Total Loss 6.620\n",
      "Epoch 4301 : Total Loss 6.612\n",
      "Epoch 4311 : Total Loss 6.604\n",
      "Epoch 4321 : Total Loss 6.595\n",
      "Epoch 4331 : Total Loss 6.587\n",
      "Epoch 4341 : Total Loss 6.579\n",
      "Epoch 4351 : Total Loss 6.571\n",
      "Epoch 4361 : Total Loss 6.564\n",
      "Epoch 4371 : Total Loss 6.556\n",
      "Epoch 4381 : Total Loss 6.548\n",
      "Epoch 4391 : Total Loss 6.540\n",
      "Epoch 4401 : Total Loss 6.533\n",
      "Epoch 4411 : Total Loss 6.525\n",
      "Epoch 4421 : Total Loss 6.518\n",
      "Epoch 4431 : Total Loss 6.510\n",
      "Epoch 4441 : Total Loss 6.503\n",
      "Epoch 4451 : Total Loss 6.496\n",
      "Epoch 4461 : Total Loss 6.489\n",
      "Epoch 4471 : Total Loss 6.482\n",
      "Epoch 4481 : Total Loss 6.475\n",
      "Epoch 4491 : Total Loss 6.468\n",
      "Epoch 4501 : Total Loss 6.461\n",
      "Epoch 4511 : Total Loss 6.454\n",
      "Epoch 4521 : Total Loss 6.447\n",
      "Epoch 4531 : Total Loss 6.440\n",
      "Epoch 4541 : Total Loss 6.434\n",
      "Epoch 4551 : Total Loss 6.427\n",
      "Epoch 4561 : Total Loss 6.421\n",
      "Epoch 4571 : Total Loss 6.414\n",
      "Epoch 4581 : Total Loss 6.408\n",
      "Epoch 4591 : Total Loss 6.401\n",
      "Epoch 4601 : Total Loss 6.395\n",
      "Epoch 4611 : Total Loss 6.389\n",
      "Epoch 4621 : Total Loss 6.383\n",
      "Epoch 4631 : Total Loss 6.377\n",
      "Epoch 4641 : Total Loss 6.371\n",
      "Epoch 4651 : Total Loss 6.365\n",
      "Epoch 4661 : Total Loss 6.359\n",
      "Epoch 4671 : Total Loss 6.353\n",
      "Epoch 4681 : Total Loss 6.347\n",
      "Epoch 4691 : Total Loss 6.341\n",
      "Epoch 4701 : Total Loss 6.335\n",
      "Epoch 4711 : Total Loss 6.330\n",
      "Epoch 4721 : Total Loss 6.324\n",
      "Epoch 4731 : Total Loss 6.318\n",
      "Epoch 4741 : Total Loss 6.313\n",
      "Epoch 4751 : Total Loss 6.307\n",
      "Epoch 4761 : Total Loss 6.302\n",
      "Epoch 4771 : Total Loss 6.297\n",
      "Epoch 4781 : Total Loss 6.291\n",
      "Epoch 4791 : Total Loss 6.286\n",
      "Epoch 4801 : Total Loss 6.281\n",
      "Epoch 4811 : Total Loss 6.276\n",
      "Epoch 4821 : Total Loss 6.270\n",
      "Epoch 4831 : Total Loss 6.265\n",
      "Epoch 4841 : Total Loss 6.260\n",
      "Epoch 4851 : Total Loss 6.255\n",
      "Epoch 4861 : Total Loss 6.250\n",
      "Epoch 4871 : Total Loss 6.245\n",
      "Epoch 4881 : Total Loss 6.240\n",
      "Epoch 4891 : Total Loss 6.236\n",
      "Epoch 4901 : Total Loss 6.231\n",
      "Epoch 4911 : Total Loss 6.226\n",
      "Epoch 4921 : Total Loss 6.221\n",
      "Epoch 4931 : Total Loss 6.217\n",
      "Epoch 4941 : Total Loss 6.212\n",
      "Epoch 4951 : Total Loss 6.208\n",
      "Epoch 4961 : Total Loss 6.203\n",
      "Epoch 4971 : Total Loss 6.199\n",
      "Epoch 4981 : Total Loss 6.194\n",
      "Epoch 4991 : Total Loss 6.190\n",
      "The Mean Squared Error is 0.007981692692397194.\n",
      "The Root Mean Squared Error is 0.08934031952258283.\n",
      "\n",
      "The Runtime of this algorithm of this 100 x 100, 99.160% sparse matrix, is 15.160948400036432 seconds.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "m, n = arr_100_100.shape[0], arr_100_100.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "U_3 = np.random.rand(m, user_features)\n",
    "V_3 = np.random.rand(n, user_features)\n",
    "\n",
    "# run through pipeline\n",
    "\n",
    "dict_100_sparse = evaluation_pipeline(arr_100_100, U_3, V_3, 5000, 2, matrix_factorization_sparse)\n",
    "\n",
    "call_eval_pipeline(dict_100_sparse, flag_output = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07af24",
   "metadata": {},
   "source": [
    "### Run Regular Matrix Factorization on Sparse Matrix\n",
    "\n",
    "Just for comparison, let us check how much more efficient our sparse matrix factorization is compared to the regular method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4315a4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Total Loss 598.166\n",
      "Epoch 11 : Total Loss 589.788\n",
      "Epoch 21 : Total Loss 581.294\n",
      "Epoch 31 : Total Loss 572.685\n",
      "Epoch 41 : Total Loss 563.966\n",
      "Epoch 51 : Total Loss 555.139\n",
      "Epoch 61 : Total Loss 546.209\n",
      "Epoch 71 : Total Loss 537.182\n",
      "Epoch 81 : Total Loss 528.064\n",
      "Epoch 91 : Total Loss 518.862\n",
      "Epoch 101 : Total Loss 509.584\n",
      "Epoch 111 : Total Loss 500.238\n",
      "Epoch 121 : Total Loss 490.834\n",
      "Epoch 131 : Total Loss 481.381\n",
      "Epoch 141 : Total Loss 471.889\n",
      "Epoch 151 : Total Loss 462.370\n",
      "Epoch 161 : Total Loss 452.835\n",
      "Epoch 171 : Total Loss 443.296\n",
      "Epoch 181 : Total Loss 433.765\n",
      "Epoch 191 : Total Loss 424.254\n",
      "Epoch 201 : Total Loss 414.775\n",
      "Epoch 211 : Total Loss 405.340\n",
      "Epoch 221 : Total Loss 395.962\n",
      "Epoch 231 : Total Loss 386.653\n",
      "Epoch 241 : Total Loss 377.422\n",
      "Epoch 251 : Total Loss 368.283\n",
      "Epoch 261 : Total Loss 359.244\n",
      "Epoch 271 : Total Loss 350.317\n",
      "Epoch 281 : Total Loss 341.508\n",
      "Epoch 291 : Total Loss 332.828\n",
      "Epoch 301 : Total Loss 324.284\n",
      "Epoch 311 : Total Loss 315.882\n",
      "Epoch 321 : Total Loss 307.628\n",
      "Epoch 331 : Total Loss 299.527\n",
      "Epoch 341 : Total Loss 291.584\n",
      "Epoch 351 : Total Loss 283.802\n",
      "Epoch 361 : Total Loss 276.183\n",
      "Epoch 371 : Total Loss 268.730\n",
      "Epoch 381 : Total Loss 261.445\n",
      "Epoch 391 : Total Loss 254.327\n",
      "Epoch 401 : Total Loss 247.377\n",
      "Epoch 411 : Total Loss 240.595\n",
      "Epoch 421 : Total Loss 233.980\n",
      "Epoch 431 : Total Loss 227.531\n",
      "Epoch 441 : Total Loss 221.247\n",
      "Epoch 451 : Total Loss 215.126\n",
      "Epoch 461 : Total Loss 209.166\n",
      "Epoch 471 : Total Loss 203.366\n",
      "Epoch 481 : Total Loss 197.722\n",
      "Epoch 491 : Total Loss 192.232\n",
      "Epoch 501 : Total Loss 186.895\n",
      "Epoch 511 : Total Loss 181.708\n",
      "Epoch 521 : Total Loss 176.667\n",
      "Epoch 531 : Total Loss 171.771\n",
      "Epoch 541 : Total Loss 167.017\n",
      "Epoch 551 : Total Loss 162.402\n",
      "Epoch 561 : Total Loss 157.923\n",
      "Epoch 571 : Total Loss 153.578\n",
      "Epoch 581 : Total Loss 149.364\n",
      "Epoch 591 : Total Loss 145.279\n",
      "Epoch 601 : Total Loss 141.320\n",
      "Epoch 611 : Total Loss 137.483\n",
      "Epoch 621 : Total Loss 133.766\n",
      "Epoch 631 : Total Loss 130.168\n",
      "Epoch 641 : Total Loss 126.683\n",
      "Epoch 651 : Total Loss 123.311\n",
      "Epoch 661 : Total Loss 120.048\n",
      "Epoch 671 : Total Loss 116.891\n",
      "Epoch 681 : Total Loss 113.838\n",
      "Epoch 691 : Total Loss 110.885\n",
      "Epoch 701 : Total Loss 108.031\n",
      "Epoch 711 : Total Loss 105.271\n",
      "Epoch 721 : Total Loss 102.604\n",
      "Epoch 731 : Total Loss 100.026\n",
      "Epoch 741 : Total Loss 97.534\n",
      "Epoch 751 : Total Loss 95.127\n",
      "Epoch 761 : Total Loss 92.800\n",
      "Epoch 771 : Total Loss 90.552\n",
      "Epoch 781 : Total Loss 88.380\n",
      "Epoch 791 : Total Loss 86.280\n",
      "Epoch 801 : Total Loss 84.251\n",
      "Epoch 811 : Total Loss 82.289\n",
      "Epoch 821 : Total Loss 80.392\n",
      "Epoch 831 : Total Loss 78.558\n",
      "Epoch 841 : Total Loss 76.784\n",
      "Epoch 851 : Total Loss 75.069\n",
      "Epoch 861 : Total Loss 73.408\n",
      "Epoch 871 : Total Loss 71.801\n",
      "Epoch 881 : Total Loss 70.245\n",
      "Epoch 891 : Total Loss 68.738\n",
      "Epoch 901 : Total Loss 67.279\n",
      "Epoch 911 : Total Loss 65.864\n",
      "Epoch 921 : Total Loss 64.492\n",
      "Epoch 931 : Total Loss 63.162\n",
      "Epoch 941 : Total Loss 61.872\n",
      "Epoch 951 : Total Loss 60.620\n",
      "Epoch 961 : Total Loss 59.404\n",
      "Epoch 971 : Total Loss 58.223\n",
      "Epoch 981 : Total Loss 57.076\n",
      "Epoch 991 : Total Loss 55.960\n",
      "Epoch 1001 : Total Loss 54.876\n",
      "Epoch 1011 : Total Loss 53.821\n",
      "Epoch 1021 : Total Loss 52.795\n",
      "Epoch 1031 : Total Loss 51.796\n",
      "Epoch 1041 : Total Loss 50.824\n",
      "Epoch 1051 : Total Loss 49.876\n",
      "Epoch 1061 : Total Loss 48.953\n",
      "Epoch 1071 : Total Loss 48.054\n",
      "Epoch 1081 : Total Loss 47.177\n",
      "Epoch 1091 : Total Loss 46.322\n",
      "Epoch 1101 : Total Loss 45.487\n",
      "Epoch 1111 : Total Loss 44.674\n",
      "Epoch 1121 : Total Loss 43.880\n",
      "Epoch 1131 : Total Loss 43.104\n",
      "Epoch 1141 : Total Loss 42.348\n",
      "Epoch 1151 : Total Loss 41.609\n",
      "Epoch 1161 : Total Loss 40.887\n",
      "Epoch 1171 : Total Loss 40.182\n",
      "Epoch 1181 : Total Loss 39.494\n",
      "Epoch 1191 : Total Loss 38.821\n",
      "Epoch 1201 : Total Loss 38.164\n",
      "Epoch 1211 : Total Loss 37.521\n",
      "Epoch 1221 : Total Loss 36.893\n",
      "Epoch 1231 : Total Loss 36.280\n",
      "Epoch 1241 : Total Loss 35.680\n",
      "Epoch 1251 : Total Loss 35.093\n",
      "Epoch 1261 : Total Loss 34.520\n",
      "Epoch 1271 : Total Loss 33.959\n",
      "Epoch 1281 : Total Loss 33.411\n",
      "Epoch 1291 : Total Loss 32.875\n",
      "Epoch 1301 : Total Loss 32.351\n",
      "Epoch 1311 : Total Loss 31.839\n",
      "Epoch 1321 : Total Loss 31.337\n",
      "Epoch 1331 : Total Loss 30.847\n",
      "Epoch 1341 : Total Loss 30.368\n",
      "Epoch 1351 : Total Loss 29.900\n",
      "Epoch 1361 : Total Loss 29.442\n",
      "Epoch 1371 : Total Loss 28.993\n",
      "Epoch 1381 : Total Loss 28.555\n",
      "Epoch 1391 : Total Loss 28.127\n",
      "Epoch 1401 : Total Loss 27.708\n",
      "Epoch 1411 : Total Loss 27.298\n",
      "Epoch 1421 : Total Loss 26.897\n",
      "Epoch 1431 : Total Loss 26.506\n",
      "Epoch 1441 : Total Loss 26.122\n",
      "Epoch 1451 : Total Loss 25.748\n",
      "Epoch 1461 : Total Loss 25.382\n",
      "Epoch 1471 : Total Loss 25.023\n",
      "Epoch 1481 : Total Loss 24.673\n",
      "Epoch 1491 : Total Loss 24.330\n",
      "Epoch 1501 : Total Loss 23.996\n",
      "Epoch 1511 : Total Loss 23.668\n",
      "Epoch 1521 : Total Loss 23.348\n",
      "Epoch 1531 : Total Loss 23.035\n",
      "Epoch 1541 : Total Loss 22.729\n",
      "Epoch 1551 : Total Loss 22.429\n",
      "Epoch 1561 : Total Loss 22.136\n",
      "Epoch 1571 : Total Loss 21.850\n",
      "Epoch 1581 : Total Loss 21.570\n",
      "Epoch 1591 : Total Loss 21.297\n",
      "Epoch 1601 : Total Loss 21.029\n",
      "Epoch 1611 : Total Loss 20.767\n",
      "Epoch 1621 : Total Loss 20.512\n",
      "Epoch 1631 : Total Loss 20.261\n",
      "Epoch 1641 : Total Loss 20.017\n",
      "Epoch 1651 : Total Loss 19.778\n",
      "Epoch 1661 : Total Loss 19.544\n",
      "Epoch 1671 : Total Loss 19.315\n",
      "Epoch 1681 : Total Loss 19.091\n",
      "Epoch 1691 : Total Loss 18.872\n",
      "Epoch 1701 : Total Loss 18.659\n",
      "Epoch 1711 : Total Loss 18.449\n",
      "Epoch 1721 : Total Loss 18.245\n",
      "Epoch 1731 : Total Loss 18.045\n",
      "Epoch 1741 : Total Loss 17.849\n",
      "Epoch 1751 : Total Loss 17.657\n",
      "Epoch 1761 : Total Loss 17.470\n",
      "Epoch 1771 : Total Loss 17.287\n",
      "Epoch 1781 : Total Loss 17.108\n",
      "Epoch 1791 : Total Loss 16.933\n",
      "Epoch 1801 : Total Loss 16.761\n",
      "Epoch 1811 : Total Loss 16.593\n",
      "Epoch 1821 : Total Loss 16.429\n",
      "Epoch 1831 : Total Loss 16.269\n",
      "Epoch 1841 : Total Loss 16.112\n",
      "Epoch 1851 : Total Loss 15.958\n",
      "Epoch 1861 : Total Loss 15.808\n",
      "Epoch 1871 : Total Loss 15.660\n",
      "Epoch 1881 : Total Loss 15.516\n",
      "Epoch 1891 : Total Loss 15.375\n",
      "Epoch 1901 : Total Loss 15.237\n",
      "Epoch 1911 : Total Loss 15.102\n",
      "Epoch 1921 : Total Loss 14.970\n",
      "Epoch 1931 : Total Loss 14.840\n",
      "Epoch 1941 : Total Loss 14.714\n",
      "Epoch 1951 : Total Loss 14.590\n",
      "Epoch 1961 : Total Loss 14.468\n",
      "Epoch 1971 : Total Loss 14.349\n",
      "Epoch 1981 : Total Loss 14.232\n",
      "Epoch 1991 : Total Loss 14.118\n",
      "Epoch 2001 : Total Loss 14.006\n",
      "Epoch 2011 : Total Loss 13.897\n",
      "Epoch 2021 : Total Loss 13.789\n",
      "Epoch 2031 : Total Loss 13.684\n",
      "Epoch 2041 : Total Loss 13.581\n",
      "Epoch 2051 : Total Loss 13.480\n",
      "Epoch 2061 : Total Loss 13.381\n",
      "Epoch 2071 : Total Loss 13.284\n",
      "Epoch 2081 : Total Loss 13.188\n",
      "Epoch 2091 : Total Loss 13.095\n",
      "Epoch 2101 : Total Loss 13.003\n",
      "Epoch 2111 : Total Loss 12.914\n",
      "Epoch 2121 : Total Loss 12.825\n",
      "Epoch 2131 : Total Loss 12.739\n",
      "Epoch 2141 : Total Loss 12.654\n",
      "Epoch 2151 : Total Loss 12.571\n",
      "Epoch 2161 : Total Loss 12.489\n",
      "Epoch 2171 : Total Loss 12.409\n",
      "Epoch 2181 : Total Loss 12.330\n",
      "Epoch 2191 : Total Loss 12.253\n",
      "Epoch 2201 : Total Loss 12.177\n",
      "Epoch 2211 : Total Loss 12.103\n",
      "Epoch 2221 : Total Loss 12.030\n",
      "Epoch 2231 : Total Loss 11.958\n",
      "Epoch 2241 : Total Loss 11.887\n",
      "Epoch 2251 : Total Loss 11.818\n",
      "Epoch 2261 : Total Loss 11.749\n",
      "Epoch 2271 : Total Loss 11.682\n",
      "Epoch 2281 : Total Loss 11.616\n",
      "Epoch 2291 : Total Loss 11.552\n",
      "Epoch 2301 : Total Loss 11.488\n",
      "Epoch 2311 : Total Loss 11.425\n",
      "Epoch 2321 : Total Loss 11.363\n",
      "Epoch 2331 : Total Loss 11.303\n",
      "Epoch 2341 : Total Loss 11.243\n",
      "Epoch 2351 : Total Loss 11.184\n",
      "Epoch 2361 : Total Loss 11.126\n",
      "Epoch 2371 : Total Loss 11.070\n",
      "Epoch 2381 : Total Loss 11.013\n",
      "Epoch 2391 : Total Loss 10.958\n",
      "Epoch 2401 : Total Loss 10.904\n",
      "Epoch 2411 : Total Loss 10.850\n",
      "Epoch 2421 : Total Loss 10.798\n",
      "Epoch 2431 : Total Loss 10.746\n",
      "Epoch 2441 : Total Loss 10.695\n",
      "Epoch 2451 : Total Loss 10.644\n",
      "Epoch 2461 : Total Loss 10.594\n",
      "Epoch 2471 : Total Loss 10.545\n",
      "Epoch 2481 : Total Loss 10.497\n",
      "Epoch 2491 : Total Loss 10.449\n",
      "Epoch 2501 : Total Loss 10.402\n",
      "Epoch 2511 : Total Loss 10.356\n",
      "Epoch 2521 : Total Loss 10.310\n",
      "Epoch 2531 : Total Loss 10.265\n",
      "Epoch 2541 : Total Loss 10.221\n",
      "Epoch 2551 : Total Loss 10.177\n",
      "Epoch 2561 : Total Loss 10.133\n",
      "Epoch 2571 : Total Loss 10.090\n",
      "Epoch 2581 : Total Loss 10.048\n",
      "Epoch 2591 : Total Loss 10.006\n",
      "Epoch 2601 : Total Loss 9.965\n",
      "Epoch 2611 : Total Loss 9.925\n",
      "Epoch 2621 : Total Loss 9.884\n",
      "Epoch 2631 : Total Loss 9.845\n",
      "Epoch 2641 : Total Loss 9.805\n",
      "Epoch 2651 : Total Loss 9.767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2661 : Total Loss 9.728\n",
      "Epoch 2671 : Total Loss 9.691\n",
      "Epoch 2681 : Total Loss 9.653\n",
      "Epoch 2691 : Total Loss 9.616\n",
      "Epoch 2701 : Total Loss 9.580\n",
      "Epoch 2711 : Total Loss 9.544\n",
      "Epoch 2721 : Total Loss 9.508\n",
      "Epoch 2731 : Total Loss 9.473\n",
      "Epoch 2741 : Total Loss 9.438\n",
      "Epoch 2751 : Total Loss 9.403\n",
      "Epoch 2761 : Total Loss 9.369\n",
      "Epoch 2771 : Total Loss 9.335\n",
      "Epoch 2781 : Total Loss 9.302\n",
      "Epoch 2791 : Total Loss 9.269\n",
      "Epoch 2801 : Total Loss 9.236\n",
      "Epoch 2811 : Total Loss 9.204\n",
      "Epoch 2821 : Total Loss 9.172\n",
      "Epoch 2831 : Total Loss 9.140\n",
      "Epoch 2841 : Total Loss 9.109\n",
      "Epoch 2851 : Total Loss 9.078\n",
      "Epoch 2861 : Total Loss 9.047\n",
      "Epoch 2871 : Total Loss 9.017\n",
      "Epoch 2881 : Total Loss 8.987\n",
      "Epoch 2891 : Total Loss 8.957\n",
      "Epoch 2901 : Total Loss 8.927\n",
      "Epoch 2911 : Total Loss 8.898\n",
      "Epoch 2921 : Total Loss 8.869\n",
      "Epoch 2931 : Total Loss 8.841\n",
      "Epoch 2941 : Total Loss 8.812\n",
      "Epoch 2951 : Total Loss 8.784\n",
      "Epoch 2961 : Total Loss 8.756\n",
      "Epoch 2971 : Total Loss 8.729\n",
      "Epoch 2981 : Total Loss 8.702\n",
      "Epoch 2991 : Total Loss 8.675\n",
      "Epoch 3001 : Total Loss 8.648\n",
      "Epoch 3011 : Total Loss 8.621\n",
      "Epoch 3021 : Total Loss 8.595\n",
      "Epoch 3031 : Total Loss 8.569\n",
      "Epoch 3041 : Total Loss 8.543\n",
      "Epoch 3051 : Total Loss 8.518\n",
      "Epoch 3061 : Total Loss 8.493\n",
      "Epoch 3071 : Total Loss 8.468\n",
      "Epoch 3081 : Total Loss 8.443\n",
      "Epoch 3091 : Total Loss 8.418\n",
      "Epoch 3101 : Total Loss 8.394\n",
      "Epoch 3111 : Total Loss 8.370\n",
      "Epoch 3121 : Total Loss 8.346\n",
      "Epoch 3131 : Total Loss 8.322\n",
      "Epoch 3141 : Total Loss 8.299\n",
      "Epoch 3151 : Total Loss 8.275\n",
      "Epoch 3161 : Total Loss 8.252\n",
      "Epoch 3171 : Total Loss 8.230\n",
      "Epoch 3181 : Total Loss 8.207\n",
      "Epoch 3191 : Total Loss 8.184\n",
      "Epoch 3201 : Total Loss 8.162\n",
      "Epoch 3211 : Total Loss 8.140\n",
      "Epoch 3221 : Total Loss 8.118\n",
      "Epoch 3231 : Total Loss 8.097\n",
      "Epoch 3241 : Total Loss 8.075\n",
      "Epoch 3251 : Total Loss 8.054\n",
      "Epoch 3261 : Total Loss 8.033\n",
      "Epoch 3271 : Total Loss 8.012\n",
      "Epoch 3281 : Total Loss 7.991\n",
      "Epoch 3291 : Total Loss 7.971\n",
      "Epoch 3301 : Total Loss 7.951\n",
      "Epoch 3311 : Total Loss 7.930\n",
      "Epoch 3321 : Total Loss 7.910\n",
      "Epoch 3331 : Total Loss 7.891\n",
      "Epoch 3341 : Total Loss 7.871\n",
      "Epoch 3351 : Total Loss 7.851\n",
      "Epoch 3361 : Total Loss 7.832\n",
      "Epoch 3371 : Total Loss 7.813\n",
      "Epoch 3381 : Total Loss 7.794\n",
      "Epoch 3391 : Total Loss 7.775\n",
      "Epoch 3401 : Total Loss 7.757\n",
      "Epoch 3411 : Total Loss 7.738\n",
      "Epoch 3421 : Total Loss 7.720\n",
      "Epoch 3431 : Total Loss 7.702\n",
      "Epoch 3441 : Total Loss 7.684\n",
      "Epoch 3451 : Total Loss 7.666\n",
      "Epoch 3461 : Total Loss 7.648\n",
      "Epoch 3471 : Total Loss 7.631\n",
      "Epoch 3481 : Total Loss 7.614\n",
      "Epoch 3491 : Total Loss 7.596\n",
      "Epoch 3501 : Total Loss 7.579\n",
      "Epoch 3511 : Total Loss 7.562\n",
      "Epoch 3521 : Total Loss 7.546\n",
      "Epoch 3531 : Total Loss 7.529\n",
      "Epoch 3541 : Total Loss 7.513\n",
      "Epoch 3551 : Total Loss 7.496\n",
      "Epoch 3561 : Total Loss 7.480\n",
      "Epoch 3571 : Total Loss 7.464\n",
      "Epoch 3581 : Total Loss 7.448\n",
      "Epoch 3591 : Total Loss 7.432\n",
      "Epoch 3601 : Total Loss 7.417\n",
      "Epoch 3611 : Total Loss 7.401\n",
      "Epoch 3621 : Total Loss 7.386\n",
      "Epoch 3631 : Total Loss 7.371\n",
      "Epoch 3641 : Total Loss 7.356\n",
      "Epoch 3651 : Total Loss 7.341\n",
      "Epoch 3661 : Total Loss 7.326\n",
      "Epoch 3671 : Total Loss 7.311\n",
      "Epoch 3681 : Total Loss 7.297\n",
      "Epoch 3691 : Total Loss 7.282\n",
      "Epoch 3701 : Total Loss 7.268\n",
      "Epoch 3711 : Total Loss 7.254\n",
      "Epoch 3721 : Total Loss 7.240\n",
      "Epoch 3731 : Total Loss 7.226\n",
      "Epoch 3741 : Total Loss 7.212\n",
      "Epoch 3751 : Total Loss 7.198\n",
      "Epoch 3761 : Total Loss 7.185\n",
      "Epoch 3771 : Total Loss 7.172\n",
      "Epoch 3781 : Total Loss 7.158\n",
      "Epoch 3791 : Total Loss 7.145\n",
      "Epoch 3801 : Total Loss 7.132\n",
      "Epoch 3811 : Total Loss 7.119\n",
      "Epoch 3821 : Total Loss 7.106\n",
      "Epoch 3831 : Total Loss 7.093\n",
      "Epoch 3841 : Total Loss 7.081\n",
      "Epoch 3851 : Total Loss 7.068\n",
      "Epoch 3861 : Total Loss 7.056\n",
      "Epoch 3871 : Total Loss 7.044\n",
      "Epoch 3881 : Total Loss 7.032\n",
      "Epoch 3891 : Total Loss 7.019\n",
      "Epoch 3901 : Total Loss 7.008\n",
      "Epoch 3911 : Total Loss 6.996\n",
      "Epoch 3921 : Total Loss 6.984\n",
      "Epoch 3931 : Total Loss 6.972\n",
      "Epoch 3941 : Total Loss 6.961\n",
      "Epoch 3951 : Total Loss 6.949\n",
      "Epoch 3961 : Total Loss 6.938\n",
      "Epoch 3971 : Total Loss 6.927\n",
      "Epoch 3981 : Total Loss 6.916\n",
      "Epoch 3991 : Total Loss 6.905\n",
      "Epoch 4001 : Total Loss 6.894\n",
      "Epoch 4011 : Total Loss 6.883\n",
      "Epoch 4021 : Total Loss 6.872\n",
      "Epoch 4031 : Total Loss 6.862\n",
      "Epoch 4041 : Total Loss 6.851\n",
      "Epoch 4051 : Total Loss 6.841\n",
      "Epoch 4061 : Total Loss 6.830\n",
      "Epoch 4071 : Total Loss 6.820\n",
      "Epoch 4081 : Total Loss 6.810\n",
      "Epoch 4091 : Total Loss 6.800\n",
      "Epoch 4101 : Total Loss 6.790\n",
      "Epoch 4111 : Total Loss 6.780\n",
      "Epoch 4121 : Total Loss 6.770\n",
      "Epoch 4131 : Total Loss 6.761\n",
      "Epoch 4141 : Total Loss 6.751\n",
      "Epoch 4151 : Total Loss 6.742\n",
      "Epoch 4161 : Total Loss 6.732\n",
      "Epoch 4171 : Total Loss 6.723\n",
      "Epoch 4181 : Total Loss 6.714\n",
      "Epoch 4191 : Total Loss 6.704\n",
      "Epoch 4201 : Total Loss 6.695\n",
      "Epoch 4211 : Total Loss 6.686\n",
      "Epoch 4221 : Total Loss 6.677\n",
      "Epoch 4231 : Total Loss 6.669\n",
      "Epoch 4241 : Total Loss 6.660\n",
      "Epoch 4251 : Total Loss 6.651\n",
      "Epoch 4261 : Total Loss 6.643\n",
      "Epoch 4271 : Total Loss 6.634\n",
      "Epoch 4281 : Total Loss 6.626\n",
      "Epoch 4291 : Total Loss 6.617\n",
      "Epoch 4301 : Total Loss 6.609\n",
      "Epoch 4311 : Total Loss 6.601\n",
      "Epoch 4321 : Total Loss 6.593\n",
      "Epoch 4331 : Total Loss 6.585\n",
      "Epoch 4341 : Total Loss 6.577\n",
      "Epoch 4351 : Total Loss 6.569\n",
      "Epoch 4361 : Total Loss 6.561\n",
      "Epoch 4371 : Total Loss 6.553\n",
      "Epoch 4381 : Total Loss 6.545\n",
      "Epoch 4391 : Total Loss 6.538\n",
      "Epoch 4401 : Total Loss 6.530\n",
      "Epoch 4411 : Total Loss 6.523\n",
      "Epoch 4421 : Total Loss 6.515\n",
      "Epoch 4431 : Total Loss 6.508\n",
      "Epoch 4441 : Total Loss 6.501\n",
      "Epoch 4451 : Total Loss 6.493\n",
      "Epoch 4461 : Total Loss 6.486\n",
      "Epoch 4471 : Total Loss 6.479\n",
      "Epoch 4481 : Total Loss 6.472\n",
      "Epoch 4491 : Total Loss 6.465\n",
      "Epoch 4501 : Total Loss 6.458\n",
      "Epoch 4511 : Total Loss 6.452\n",
      "Epoch 4521 : Total Loss 6.445\n",
      "Epoch 4531 : Total Loss 6.438\n",
      "Epoch 4541 : Total Loss 6.432\n",
      "Epoch 4551 : Total Loss 6.425\n",
      "Epoch 4561 : Total Loss 6.418\n",
      "Epoch 4571 : Total Loss 6.412\n",
      "Epoch 4581 : Total Loss 6.406\n",
      "Epoch 4591 : Total Loss 6.399\n",
      "Epoch 4601 : Total Loss 6.393\n",
      "Epoch 4611 : Total Loss 6.387\n",
      "Epoch 4621 : Total Loss 6.381\n",
      "Epoch 4631 : Total Loss 6.374\n",
      "Epoch 4641 : Total Loss 6.368\n",
      "Epoch 4651 : Total Loss 6.362\n",
      "Epoch 4661 : Total Loss 6.357\n",
      "Epoch 4671 : Total Loss 6.351\n",
      "Epoch 4681 : Total Loss 6.345\n",
      "Epoch 4691 : Total Loss 6.339\n",
      "Epoch 4701 : Total Loss 6.333\n",
      "Epoch 4711 : Total Loss 6.328\n",
      "Epoch 4721 : Total Loss 6.322\n",
      "Epoch 4731 : Total Loss 6.316\n",
      "Epoch 4741 : Total Loss 6.311\n",
      "Epoch 4751 : Total Loss 6.305\n",
      "Epoch 4761 : Total Loss 6.300\n",
      "Epoch 4771 : Total Loss 6.295\n",
      "Epoch 4781 : Total Loss 6.289\n",
      "Epoch 4791 : Total Loss 6.284\n",
      "Epoch 4801 : Total Loss 6.279\n",
      "Epoch 4811 : Total Loss 6.274\n",
      "Epoch 4821 : Total Loss 6.269\n",
      "Epoch 4831 : Total Loss 6.263\n",
      "Epoch 4841 : Total Loss 6.258\n",
      "Epoch 4851 : Total Loss 6.253\n",
      "Epoch 4861 : Total Loss 6.248\n",
      "Epoch 4871 : Total Loss 6.244\n",
      "Epoch 4881 : Total Loss 6.239\n",
      "Epoch 4891 : Total Loss 6.234\n",
      "Epoch 4901 : Total Loss 6.229\n",
      "Epoch 4911 : Total Loss 6.224\n",
      "Epoch 4921 : Total Loss 6.220\n",
      "Epoch 4931 : Total Loss 6.215\n",
      "Epoch 4941 : Total Loss 6.210\n",
      "Epoch 4951 : Total Loss 6.206\n",
      "Epoch 4961 : Total Loss 6.201\n",
      "Epoch 4971 : Total Loss 6.197\n",
      "Epoch 4981 : Total Loss 6.192\n",
      "Epoch 4991 : Total Loss 6.188\n",
      "The Mean Squared Error is 0.007981692692397194.\n",
      "The Root Mean Squared Error is 0.08934031952258283.\n",
      "\n",
      "The Runtime of this algorithm of this 100 x 100, 99.160% sparse matrix, is 107.77499800000805 seconds.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "m, n = arr_100_100.shape[0], arr_100_100.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "U_3 = np.random.rand(m, user_features)\n",
    "V_3 = np.random.rand(n, user_features)\n",
    "\n",
    "dict_100_regular = evaluation_pipeline(arr_100_100, U_3, V_3, 5000, 2, matrix_factorization)\n",
    "\n",
    "call_eval_pipeline(dict_100_regular, flag_output = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d4d16",
   "metadata": {},
   "source": [
    "This is almost a 900% improvement in runtime!\n",
    "\n",
    "### Run on a 1000 x 1000 sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5620b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity of our random 1000 x 1000 interaction table is 0.991658.\n"
     ]
    }
   ],
   "source": [
    "# 1k x 1k sparse matrix initialization\n",
    "\n",
    "arr_rvs_2 = randint(0, 6).rvs\n",
    "\n",
    "x_1k = random(1000,1000, density = 0.01, data_rvs = arr_rvs)\n",
    "\n",
    "arr_1k = np.array(x_1k.toarray())\n",
    "\n",
    "# check sparsity of matrix\n",
    "\n",
    "li_nonzero_1k = [(i,j) for i in range(arr_1k.shape[0]) for j in range(arr_1k.shape[1]) if arr_1k[i,j]!=0]\n",
    "\n",
    "sparsity_1k = len(li_nonzero_1k)/(arr_1k.shape[0]*arr_1k.shape[1])\n",
    "\n",
    "print(f'The sparsity of our random 1000 x 1000 interaction table is {1 - sparsity_1k}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21bdfbf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.51729788 0.9469626 ]\n",
      " [0.76545976 0.28239584]\n",
      " [0.22104536 0.68622209]\n",
      " ...\n",
      " [0.81725319 0.69902473]\n",
      " [0.63405118 0.1848986 ]\n",
      " [0.65351892 0.97255327]]\n",
      "V is:\n",
      " [[0.17041509 0.77499716]\n",
      " [0.89300173 0.66987814]\n",
      " [0.960109   0.99091208]\n",
      " ...\n",
      " [0.87679857 0.42425804]\n",
      " [0.12411982 0.29636888]\n",
      " [0.73663472 0.99557962]]\n",
      "Epoch 1 : Total Loss 70857.643\n",
      "Epoch 11 : Total Loss 66719.825\n",
      "Epoch 21 : Total Loss 62222.614\n",
      "Epoch 31 : Total Loss 57446.104\n",
      "Epoch 41 : Total Loss 52505.498\n",
      "Epoch 51 : Total Loss 47542.677\n",
      "Epoch 61 : Total Loss 42711.010\n",
      "Epoch 71 : Total Loss 38156.121\n",
      "Epoch 81 : Total Loss 33997.255\n",
      "Epoch 91 : Total Loss 30314.089\n",
      "Epoch 101 : Total Loss 27141.978\n",
      "Epoch 111 : Total Loss 24475.682\n",
      "Epoch 121 : Total Loss 22279.020\n",
      "Epoch 131 : Total Loss 20496.831\n",
      "Epoch 141 : Total Loss 19066.065\n",
      "Epoch 151 : Total Loss 17924.180\n",
      "Epoch 161 : Total Loss 17014.357\n",
      "Epoch 171 : Total Loss 16287.974\n",
      "Epoch 181 : Total Loss 15705.124\n",
      "Epoch 191 : Total Loss 15233.998\n",
      "Epoch 201 : Total Loss 14849.737\n",
      "Epoch 211 : Total Loss 14533.151\n",
      "Epoch 221 : Total Loss 14269.520\n",
      "Epoch 231 : Total Loss 14047.576\n",
      "Epoch 241 : Total Loss 13858.681\n",
      "Epoch 251 : Total Loss 13696.183\n",
      "Epoch 261 : Total Loss 13554.932\n",
      "Epoch 271 : Total Loss 13430.910\n",
      "Epoch 281 : Total Loss 13320.958\n",
      "Epoch 291 : Total Loss 13222.571\n",
      "Epoch 301 : Total Loss 13133.748\n",
      "Epoch 311 : Total Loss 13052.876\n",
      "Epoch 321 : Total Loss 12978.648\n",
      "Epoch 331 : Total Loss 12909.992\n",
      "Epoch 341 : Total Loss 12846.027\n",
      "Epoch 351 : Total Loss 12786.024\n",
      "Epoch 361 : Total Loss 12729.374\n",
      "Epoch 371 : Total Loss 12675.567\n",
      "Epoch 381 : Total Loss 12624.175\n",
      "Epoch 391 : Total Loss 12574.833\n",
      "Epoch 401 : Total Loss 12527.235\n",
      "Epoch 411 : Total Loss 12481.116\n",
      "Epoch 421 : Total Loss 12436.254\n",
      "Epoch 431 : Total Loss 12392.455\n",
      "Epoch 441 : Total Loss 12349.556\n",
      "Epoch 451 : Total Loss 12307.413\n",
      "Epoch 461 : Total Loss 12265.906\n",
      "Epoch 471 : Total Loss 12224.930\n",
      "Epoch 481 : Total Loss 12184.395\n",
      "Epoch 491 : Total Loss 12144.224\n",
      "Epoch 501 : Total Loss 12104.351\n",
      "Epoch 511 : Total Loss 12064.721\n",
      "Epoch 521 : Total Loss 12025.287\n",
      "Epoch 531 : Total Loss 11986.008\n",
      "Epoch 541 : Total Loss 11946.854\n",
      "Epoch 551 : Total Loss 11907.796\n",
      "Epoch 561 : Total Loss 11868.815\n",
      "Epoch 571 : Total Loss 11829.893\n",
      "Epoch 581 : Total Loss 11791.018\n",
      "Epoch 591 : Total Loss 11752.183\n",
      "Epoch 601 : Total Loss 11713.382\n",
      "Epoch 611 : Total Loss 11674.614\n",
      "Epoch 621 : Total Loss 11635.881\n",
      "Epoch 631 : Total Loss 11597.184\n",
      "Epoch 641 : Total Loss 11558.531\n",
      "Epoch 651 : Total Loss 11519.928\n",
      "Epoch 661 : Total Loss 11481.384\n",
      "Epoch 671 : Total Loss 11442.911\n",
      "Epoch 681 : Total Loss 11404.519\n",
      "Epoch 691 : Total Loss 11366.223\n",
      "Epoch 701 : Total Loss 11328.035\n",
      "Epoch 711 : Total Loss 11289.970\n",
      "Epoch 721 : Total Loss 11252.043\n",
      "Epoch 731 : Total Loss 11214.269\n",
      "Epoch 741 : Total Loss 11176.663\n",
      "Epoch 751 : Total Loss 11139.242\n",
      "Epoch 761 : Total Loss 11102.021\n",
      "Epoch 771 : Total Loss 11065.016\n",
      "Epoch 781 : Total Loss 11028.242\n",
      "Epoch 791 : Total Loss 10991.714\n",
      "Epoch 801 : Total Loss 10955.447\n",
      "Epoch 811 : Total Loss 10919.455\n",
      "Epoch 821 : Total Loss 10883.752\n",
      "Epoch 831 : Total Loss 10848.351\n",
      "Epoch 841 : Total Loss 10813.265\n",
      "Epoch 851 : Total Loss 10778.505\n",
      "Epoch 861 : Total Loss 10744.083\n",
      "Epoch 871 : Total Loss 10710.008\n",
      "Epoch 881 : Total Loss 10676.292\n",
      "Epoch 891 : Total Loss 10642.941\n",
      "Epoch 901 : Total Loss 10609.966\n",
      "Epoch 911 : Total Loss 10577.371\n",
      "Epoch 921 : Total Loss 10545.165\n",
      "Epoch 931 : Total Loss 10513.352\n",
      "Epoch 941 : Total Loss 10481.938\n",
      "Epoch 951 : Total Loss 10450.926\n",
      "Epoch 961 : Total Loss 10420.320\n",
      "Epoch 971 : Total Loss 10390.121\n",
      "Epoch 981 : Total Loss 10360.333\n",
      "Epoch 991 : Total Loss 10330.955\n",
      "Epoch 1001 : Total Loss 10301.989\n",
      "Epoch 1011 : Total Loss 10273.433\n",
      "Epoch 1021 : Total Loss 10245.288\n",
      "Epoch 1031 : Total Loss 10217.551\n",
      "Epoch 1041 : Total Loss 10190.221\n",
      "Epoch 1051 : Total Loss 10163.295\n",
      "Epoch 1061 : Total Loss 10136.770\n",
      "Epoch 1071 : Total Loss 10110.643\n",
      "Epoch 1081 : Total Loss 10084.910\n",
      "Epoch 1091 : Total Loss 10059.567\n",
      "Epoch 1101 : Total Loss 10034.609\n",
      "Epoch 1111 : Total Loss 10010.032\n",
      "Epoch 1121 : Total Loss 9985.831\n",
      "Epoch 1131 : Total Loss 9962.000\n",
      "Epoch 1141 : Total Loss 9938.534\n",
      "Epoch 1151 : Total Loss 9915.427\n",
      "Epoch 1161 : Total Loss 9892.674\n",
      "Epoch 1171 : Total Loss 9870.270\n",
      "Epoch 1181 : Total Loss 9848.207\n",
      "Epoch 1191 : Total Loss 9826.479\n",
      "Epoch 1201 : Total Loss 9805.082\n",
      "Epoch 1211 : Total Loss 9784.009\n",
      "Epoch 1221 : Total Loss 9763.253\n",
      "Epoch 1231 : Total Loss 9742.809\n",
      "Epoch 1241 : Total Loss 9722.671\n",
      "Epoch 1251 : Total Loss 9702.833\n",
      "Epoch 1261 : Total Loss 9683.288\n",
      "Epoch 1271 : Total Loss 9664.031\n",
      "Epoch 1281 : Total Loss 9645.056\n",
      "Epoch 1291 : Total Loss 9626.358\n",
      "Epoch 1301 : Total Loss 9607.930\n",
      "Epoch 1311 : Total Loss 9589.768\n",
      "Epoch 1321 : Total Loss 9571.865\n",
      "Epoch 1331 : Total Loss 9554.217\n",
      "Epoch 1341 : Total Loss 9536.818\n",
      "Epoch 1351 : Total Loss 9519.663\n",
      "Epoch 1361 : Total Loss 9502.748\n",
      "Epoch 1371 : Total Loss 9486.066\n",
      "Epoch 1381 : Total Loss 9469.614\n",
      "Epoch 1391 : Total Loss 9453.387\n",
      "Epoch 1401 : Total Loss 9437.380\n",
      "Epoch 1411 : Total Loss 9421.589\n",
      "Epoch 1421 : Total Loss 9406.009\n",
      "Epoch 1431 : Total Loss 9390.636\n",
      "Epoch 1441 : Total Loss 9375.467\n",
      "Epoch 1451 : Total Loss 9360.497\n",
      "Epoch 1461 : Total Loss 9345.723\n",
      "Epoch 1471 : Total Loss 9331.139\n",
      "Epoch 1481 : Total Loss 9316.744\n",
      "Epoch 1491 : Total Loss 9302.533\n",
      "Epoch 1501 : Total Loss 9288.503\n",
      "Epoch 1511 : Total Loss 9274.650\n",
      "Epoch 1521 : Total Loss 9260.971\n",
      "Epoch 1531 : Total Loss 9247.464\n",
      "Epoch 1541 : Total Loss 9234.123\n",
      "Epoch 1551 : Total Loss 9220.948\n",
      "Epoch 1561 : Total Loss 9207.934\n",
      "Epoch 1571 : Total Loss 9195.079\n",
      "Epoch 1581 : Total Loss 9182.380\n",
      "Epoch 1591 : Total Loss 9169.834\n",
      "Epoch 1601 : Total Loss 9157.439\n",
      "Epoch 1611 : Total Loss 9145.192\n",
      "Epoch 1621 : Total Loss 9133.090\n",
      "Epoch 1631 : Total Loss 9121.131\n",
      "Epoch 1641 : Total Loss 9109.313\n",
      "Epoch 1651 : Total Loss 9097.633\n",
      "Epoch 1661 : Total Loss 9086.088\n",
      "Epoch 1671 : Total Loss 9074.677\n",
      "Epoch 1681 : Total Loss 9063.398\n",
      "Epoch 1691 : Total Loss 9052.247\n",
      "Epoch 1701 : Total Loss 9041.224\n",
      "Epoch 1711 : Total Loss 9030.326\n",
      "Epoch 1721 : Total Loss 9019.550\n",
      "Epoch 1731 : Total Loss 9008.896\n",
      "Epoch 1741 : Total Loss 8998.361\n",
      "Epoch 1751 : Total Loss 8987.943\n",
      "Epoch 1761 : Total Loss 8977.641\n",
      "Epoch 1771 : Total Loss 8967.452\n",
      "Epoch 1781 : Total Loss 8957.375\n",
      "Epoch 1791 : Total Loss 8947.408\n",
      "Epoch 1801 : Total Loss 8937.550\n",
      "Epoch 1811 : Total Loss 8927.798\n",
      "Epoch 1821 : Total Loss 8918.152\n",
      "Epoch 1831 : Total Loss 8908.609\n",
      "Epoch 1841 : Total Loss 8899.168\n",
      "Epoch 1851 : Total Loss 8889.827\n",
      "Epoch 1861 : Total Loss 8880.586\n",
      "Epoch 1871 : Total Loss 8871.442\n",
      "Epoch 1881 : Total Loss 8862.394\n",
      "Epoch 1891 : Total Loss 8853.441\n",
      "Epoch 1901 : Total Loss 8844.581\n",
      "Epoch 1911 : Total Loss 8835.813\n",
      "Epoch 1921 : Total Loss 8827.136\n",
      "Epoch 1931 : Total Loss 8818.549\n",
      "Epoch 1941 : Total Loss 8810.049\n",
      "Epoch 1951 : Total Loss 8801.636\n",
      "Epoch 1961 : Total Loss 8793.309\n",
      "Epoch 1971 : Total Loss 8785.066\n",
      "Epoch 1981 : Total Loss 8776.906\n",
      "Epoch 1991 : Total Loss 8768.829\n",
      "Epoch 2001 : Total Loss 8760.832\n",
      "Epoch 2011 : Total Loss 8752.915\n",
      "Epoch 2021 : Total Loss 8745.077\n",
      "Epoch 2031 : Total Loss 8737.317\n",
      "Epoch 2041 : Total Loss 8729.633\n",
      "Epoch 2051 : Total Loss 8722.024\n",
      "Epoch 2061 : Total Loss 8714.490\n",
      "Epoch 2071 : Total Loss 8707.030\n",
      "Epoch 2081 : Total Loss 8699.642\n",
      "Epoch 2091 : Total Loss 8692.326\n",
      "Epoch 2101 : Total Loss 8685.080\n",
      "Epoch 2111 : Total Loss 8677.904\n",
      "Epoch 2121 : Total Loss 8670.796\n",
      "Epoch 2131 : Total Loss 8663.757\n",
      "Epoch 2141 : Total Loss 8656.784\n",
      "Epoch 2151 : Total Loss 8649.877\n",
      "Epoch 2161 : Total Loss 8643.035\n",
      "Epoch 2171 : Total Loss 8636.258\n",
      "Epoch 2181 : Total Loss 8629.544\n",
      "Epoch 2191 : Total Loss 8622.892\n",
      "Epoch 2201 : Total Loss 8616.302\n",
      "Epoch 2211 : Total Loss 8609.773\n",
      "Epoch 2221 : Total Loss 8603.304\n",
      "Epoch 2231 : Total Loss 8596.894\n",
      "Epoch 2241 : Total Loss 8590.542\n",
      "Epoch 2251 : Total Loss 8584.249\n",
      "Epoch 2261 : Total Loss 8578.012\n",
      "Epoch 2271 : Total Loss 8571.831\n",
      "Epoch 2281 : Total Loss 8565.706\n",
      "Epoch 2291 : Total Loss 8559.635\n",
      "Epoch 2301 : Total Loss 8553.618\n",
      "Epoch 2311 : Total Loss 8547.654\n",
      "Epoch 2321 : Total Loss 8541.743\n",
      "Epoch 2331 : Total Loss 8535.884\n",
      "Epoch 2341 : Total Loss 8530.075\n",
      "Epoch 2351 : Total Loss 8524.317\n",
      "Epoch 2361 : Total Loss 8518.608\n",
      "Epoch 2371 : Total Loss 8512.949\n",
      "Epoch 2381 : Total Loss 8507.338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2391 : Total Loss 8501.774\n",
      "Epoch 2401 : Total Loss 8496.257\n",
      "Epoch 2411 : Total Loss 8490.787\n",
      "Epoch 2421 : Total Loss 8485.362\n",
      "Epoch 2431 : Total Loss 8479.983\n",
      "Epoch 2441 : Total Loss 8474.647\n",
      "Epoch 2451 : Total Loss 8469.356\n",
      "Epoch 2461 : Total Loss 8464.108\n",
      "Epoch 2471 : Total Loss 8458.903\n",
      "Epoch 2481 : Total Loss 8453.740\n",
      "Epoch 2491 : Total Loss 8448.618\n",
      "Epoch 2501 : Total Loss 8443.537\n",
      "Epoch 2511 : Total Loss 8438.496\n",
      "Epoch 2521 : Total Loss 8433.496\n",
      "Epoch 2531 : Total Loss 8428.535\n",
      "Epoch 2541 : Total Loss 8423.612\n",
      "Epoch 2551 : Total Loss 8418.728\n",
      "Epoch 2561 : Total Loss 8413.881\n",
      "Epoch 2571 : Total Loss 8409.072\n",
      "Epoch 2581 : Total Loss 8404.300\n",
      "Epoch 2591 : Total Loss 8399.564\n",
      "Epoch 2601 : Total Loss 8394.863\n",
      "Epoch 2611 : Total Loss 8390.198\n",
      "Epoch 2621 : Total Loss 8385.568\n",
      "Epoch 2631 : Total Loss 8380.973\n",
      "Epoch 2641 : Total Loss 8376.411\n",
      "Epoch 2651 : Total Loss 8371.884\n",
      "Epoch 2661 : Total Loss 8367.389\n",
      "Epoch 2671 : Total Loss 8362.928\n",
      "Epoch 2681 : Total Loss 8358.498\n",
      "Epoch 2691 : Total Loss 8354.101\n",
      "Epoch 2701 : Total Loss 8349.736\n",
      "Epoch 2711 : Total Loss 8345.402\n",
      "Epoch 2721 : Total Loss 8341.099\n",
      "Epoch 2731 : Total Loss 8336.827\n",
      "Epoch 2741 : Total Loss 8332.585\n",
      "Epoch 2751 : Total Loss 8328.373\n",
      "Epoch 2761 : Total Loss 8324.190\n",
      "Epoch 2771 : Total Loss 8320.037\n",
      "Epoch 2781 : Total Loss 8315.913\n",
      "Epoch 2791 : Total Loss 8311.818\n",
      "Epoch 2801 : Total Loss 8307.752\n",
      "Epoch 2811 : Total Loss 8303.714\n",
      "Epoch 2821 : Total Loss 8299.703\n",
      "Epoch 2831 : Total Loss 8295.720\n",
      "Epoch 2841 : Total Loss 8291.765\n",
      "Epoch 2851 : Total Loss 8287.837\n",
      "Epoch 2861 : Total Loss 8283.936\n",
      "Epoch 2871 : Total Loss 8280.061\n",
      "Epoch 2881 : Total Loss 8276.213\n",
      "Epoch 2891 : Total Loss 8272.391\n",
      "Epoch 2901 : Total Loss 8268.596\n",
      "Epoch 2911 : Total Loss 8264.826\n",
      "Epoch 2921 : Total Loss 8261.081\n",
      "Epoch 2931 : Total Loss 8257.362\n",
      "Epoch 2941 : Total Loss 8253.668\n",
      "Epoch 2951 : Total Loss 8249.999\n",
      "Epoch 2961 : Total Loss 8246.354\n",
      "Epoch 2971 : Total Loss 8242.734\n",
      "Epoch 2981 : Total Loss 8239.139\n",
      "Epoch 2991 : Total Loss 8235.567\n",
      "Epoch 3001 : Total Loss 8232.019\n",
      "Epoch 3011 : Total Loss 8228.495\n",
      "Epoch 3021 : Total Loss 8224.995\n",
      "Epoch 3031 : Total Loss 8221.518\n",
      "Epoch 3041 : Total Loss 8218.063\n",
      "Epoch 3051 : Total Loss 8214.632\n",
      "Epoch 3061 : Total Loss 8211.224\n",
      "Epoch 3071 : Total Loss 8207.838\n",
      "Epoch 3081 : Total Loss 8204.474\n",
      "Epoch 3091 : Total Loss 8201.133\n",
      "Epoch 3101 : Total Loss 8197.813\n",
      "Epoch 3111 : Total Loss 8194.515\n",
      "Epoch 3121 : Total Loss 8191.239\n",
      "Epoch 3131 : Total Loss 8187.984\n",
      "Epoch 3141 : Total Loss 8184.750\n",
      "Epoch 3151 : Total Loss 8181.537\n",
      "Epoch 3161 : Total Loss 8178.344\n",
      "Epoch 3171 : Total Loss 8175.172\n",
      "Epoch 3181 : Total Loss 8172.021\n",
      "Epoch 3191 : Total Loss 8168.889\n",
      "Epoch 3201 : Total Loss 8165.778\n",
      "Epoch 3211 : Total Loss 8162.686\n",
      "Epoch 3221 : Total Loss 8159.614\n",
      "Epoch 3231 : Total Loss 8156.560\n",
      "Epoch 3241 : Total Loss 8153.526\n",
      "Epoch 3251 : Total Loss 8150.511\n",
      "Epoch 3261 : Total Loss 8147.514\n",
      "Epoch 3271 : Total Loss 8144.536\n",
      "Epoch 3281 : Total Loss 8141.576\n",
      "Epoch 3291 : Total Loss 8138.634\n",
      "Epoch 3301 : Total Loss 8135.709\n",
      "Epoch 3311 : Total Loss 8132.802\n",
      "Epoch 3321 : Total Loss 8129.913\n",
      "Epoch 3331 : Total Loss 8127.040\n",
      "Epoch 3341 : Total Loss 8124.185\n",
      "Epoch 3351 : Total Loss 8121.345\n",
      "Epoch 3361 : Total Loss 8118.523\n",
      "Epoch 3371 : Total Loss 8115.716\n",
      "Epoch 3381 : Total Loss 8112.926\n",
      "Epoch 3391 : Total Loss 8110.151\n",
      "Epoch 3401 : Total Loss 8107.392\n",
      "Epoch 3411 : Total Loss 8104.648\n",
      "Epoch 3421 : Total Loss 8101.919\n",
      "Epoch 3431 : Total Loss 8099.205\n",
      "Epoch 3441 : Total Loss 8096.506\n",
      "Epoch 3451 : Total Loss 8093.821\n",
      "Epoch 3461 : Total Loss 8091.150\n",
      "Epoch 3471 : Total Loss 8088.494\n",
      "Epoch 3481 : Total Loss 8085.851\n",
      "Epoch 3491 : Total Loss 8083.222\n",
      "Epoch 3501 : Total Loss 8080.606\n",
      "Epoch 3511 : Total Loss 8078.003\n",
      "Epoch 3521 : Total Loss 8075.413\n",
      "Epoch 3531 : Total Loss 8072.836\n",
      "Epoch 3541 : Total Loss 8070.272\n",
      "Epoch 3551 : Total Loss 8067.720\n",
      "Epoch 3561 : Total Loss 8065.180\n",
      "Epoch 3571 : Total Loss 8062.652\n",
      "Epoch 3581 : Total Loss 8060.136\n",
      "Epoch 3591 : Total Loss 8057.631\n",
      "Epoch 3601 : Total Loss 8055.138\n",
      "Epoch 3611 : Total Loss 8052.656\n",
      "Epoch 3621 : Total Loss 8050.184\n",
      "Epoch 3631 : Total Loss 8047.724\n",
      "Epoch 3641 : Total Loss 8045.274\n",
      "Epoch 3651 : Total Loss 8042.835\n",
      "Epoch 3661 : Total Loss 8040.406\n",
      "Epoch 3671 : Total Loss 8037.987\n",
      "Epoch 3681 : Total Loss 8035.578\n",
      "Epoch 3691 : Total Loss 8033.179\n",
      "Epoch 3701 : Total Loss 8030.790\n",
      "Epoch 3711 : Total Loss 8028.409\n",
      "Epoch 3721 : Total Loss 8026.039\n",
      "Epoch 3731 : Total Loss 8023.677\n",
      "Epoch 3741 : Total Loss 8021.324\n",
      "Epoch 3751 : Total Loss 8018.980\n",
      "Epoch 3761 : Total Loss 8016.645\n",
      "Epoch 3771 : Total Loss 8014.318\n",
      "Epoch 3781 : Total Loss 8011.999\n",
      "Epoch 3791 : Total Loss 8009.689\n",
      "Epoch 3801 : Total Loss 8007.387\n",
      "Epoch 3811 : Total Loss 8005.092\n",
      "Epoch 3821 : Total Loss 8002.806\n",
      "Epoch 3831 : Total Loss 8000.527\n",
      "Epoch 3841 : Total Loss 7998.256\n",
      "Epoch 3851 : Total Loss 7995.992\n",
      "Epoch 3861 : Total Loss 7993.735\n",
      "Epoch 3871 : Total Loss 7991.486\n",
      "Epoch 3881 : Total Loss 7989.244\n",
      "Epoch 3891 : Total Loss 7987.008\n",
      "Epoch 3901 : Total Loss 7984.780\n",
      "Epoch 3911 : Total Loss 7982.558\n",
      "Epoch 3921 : Total Loss 7980.343\n",
      "Epoch 3931 : Total Loss 7978.134\n",
      "Epoch 3941 : Total Loss 7975.932\n",
      "Epoch 3951 : Total Loss 7973.736\n",
      "Epoch 3961 : Total Loss 7971.547\n",
      "Epoch 3971 : Total Loss 7969.363\n",
      "Epoch 3981 : Total Loss 7967.186\n",
      "Epoch 3991 : Total Loss 7965.014\n",
      "Epoch 4001 : Total Loss 7962.849\n",
      "Epoch 4011 : Total Loss 7960.689\n",
      "Epoch 4021 : Total Loss 7958.535\n",
      "Epoch 4031 : Total Loss 7956.386\n",
      "Epoch 4041 : Total Loss 7954.243\n",
      "Epoch 4051 : Total Loss 7952.106\n",
      "Epoch 4061 : Total Loss 7949.974\n",
      "Epoch 4071 : Total Loss 7947.848\n",
      "Epoch 4081 : Total Loss 7945.726\n",
      "Epoch 4091 : Total Loss 7943.611\n",
      "Epoch 4101 : Total Loss 7941.500\n",
      "Epoch 4111 : Total Loss 7939.394\n",
      "Epoch 4121 : Total Loss 7937.294\n",
      "Epoch 4131 : Total Loss 7935.198\n",
      "Epoch 4141 : Total Loss 7933.108\n",
      "Epoch 4151 : Total Loss 7931.022\n",
      "Epoch 4161 : Total Loss 7928.942\n",
      "Epoch 4171 : Total Loss 7926.866\n",
      "Epoch 4181 : Total Loss 7924.796\n",
      "Epoch 4191 : Total Loss 7922.730\n",
      "Epoch 4201 : Total Loss 7920.669\n",
      "Epoch 4211 : Total Loss 7918.612\n",
      "Epoch 4221 : Total Loss 7916.561\n",
      "Epoch 4231 : Total Loss 7914.514\n",
      "Epoch 4241 : Total Loss 7912.472\n",
      "Epoch 4251 : Total Loss 7910.435\n",
      "Epoch 4261 : Total Loss 7908.402\n",
      "Epoch 4271 : Total Loss 7906.374\n",
      "Epoch 4281 : Total Loss 7904.351\n",
      "Epoch 4291 : Total Loss 7902.332\n",
      "Epoch 4301 : Total Loss 7900.318\n",
      "Epoch 4311 : Total Loss 7898.309\n",
      "Epoch 4321 : Total Loss 7896.305\n",
      "Epoch 4331 : Total Loss 7894.305\n",
      "Epoch 4341 : Total Loss 7892.310\n",
      "Epoch 4351 : Total Loss 7890.320\n",
      "Epoch 4361 : Total Loss 7888.334\n",
      "Epoch 4371 : Total Loss 7886.353\n",
      "Epoch 4381 : Total Loss 7884.377\n",
      "Epoch 4391 : Total Loss 7882.405\n",
      "Epoch 4401 : Total Loss 7880.439\n",
      "Epoch 4411 : Total Loss 7878.477\n",
      "Epoch 4421 : Total Loss 7876.520\n",
      "Epoch 4431 : Total Loss 7874.568\n",
      "Epoch 4441 : Total Loss 7872.621\n",
      "Epoch 4451 : Total Loss 7870.679\n",
      "Epoch 4461 : Total Loss 7868.741\n",
      "Epoch 4471 : Total Loss 7866.809\n",
      "Epoch 4481 : Total Loss 7864.882\n",
      "Epoch 4491 : Total Loss 7862.959\n",
      "Epoch 4501 : Total Loss 7861.042\n",
      "Epoch 4511 : Total Loss 7859.130\n",
      "Epoch 4521 : Total Loss 7857.223\n",
      "Epoch 4531 : Total Loss 7855.321\n",
      "Epoch 4541 : Total Loss 7853.425\n",
      "Epoch 4551 : Total Loss 7851.533\n",
      "Epoch 4561 : Total Loss 7849.647\n",
      "Epoch 4571 : Total Loss 7847.766\n",
      "Epoch 4581 : Total Loss 7845.891\n",
      "Epoch 4591 : Total Loss 7844.021\n",
      "Epoch 4601 : Total Loss 7842.157\n",
      "Epoch 4611 : Total Loss 7840.298\n",
      "Epoch 4621 : Total Loss 7838.444\n",
      "Epoch 4631 : Total Loss 7836.596\n",
      "Epoch 4641 : Total Loss 7834.754\n",
      "Epoch 4651 : Total Loss 7832.917\n",
      "Epoch 4661 : Total Loss 7831.086\n",
      "Epoch 4671 : Total Loss 7829.260\n",
      "Epoch 4681 : Total Loss 7827.441\n",
      "Epoch 4691 : Total Loss 7825.627\n",
      "Epoch 4701 : Total Loss 7823.819\n",
      "Epoch 4711 : Total Loss 7822.017\n",
      "Epoch 4721 : Total Loss 7820.220\n",
      "Epoch 4731 : Total Loss 7818.430\n",
      "Epoch 4741 : Total Loss 7816.645\n",
      "Epoch 4751 : Total Loss 7814.867\n",
      "Epoch 4761 : Total Loss 7813.094\n",
      "Epoch 4771 : Total Loss 7811.328\n",
      "Epoch 4781 : Total Loss 7809.567\n",
      "Epoch 4791 : Total Loss 7807.813\n",
      "Epoch 4801 : Total Loss 7806.065\n",
      "Epoch 4811 : Total Loss 7804.323\n",
      "Epoch 4821 : Total Loss 7802.587\n",
      "Epoch 4831 : Total Loss 7800.857\n",
      "Epoch 4841 : Total Loss 7799.134\n",
      "Epoch 4851 : Total Loss 7797.416\n",
      "Epoch 4861 : Total Loss 7795.705\n",
      "Epoch 4871 : Total Loss 7794.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4881 : Total Loss 7792.302\n",
      "Epoch 4891 : Total Loss 7790.610\n",
      "Epoch 4901 : Total Loss 7788.924\n",
      "Epoch 4911 : Total Loss 7787.244\n",
      "Epoch 4921 : Total Loss 7785.571\n",
      "Epoch 4931 : Total Loss 7783.904\n",
      "Epoch 4941 : Total Loss 7782.244\n",
      "Epoch 4951 : Total Loss 7780.589\n",
      "Epoch 4961 : Total Loss 7778.942\n",
      "Epoch 4971 : Total Loss 7777.300\n",
      "Epoch 4981 : Total Loss 7775.665\n",
      "Epoch 4991 : Total Loss 7774.037\n",
      "The Mean Squared Error is 0.8449173567571847.\n",
      "The Root Mean Squared Error is 0.9191938624453412.\n",
      "\n",
      "The Runtime of this algorithm of this 1000 x 1000, 99.166% sparse matrix, is 1070.5518378000706 seconds.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "\n",
    "m, n = arr_1k.shape[0], arr_1k.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "U_4 = np.random.rand(m, user_features)\n",
    "V_4 = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U_4)\n",
    "print('V is:\\n', V_4)\n",
    "\n",
    "dict_1k_sparse = evaluation_pipeline(arr_1k, U_4, V_4, 5000, 2, matrix_factorization_sparse)\n",
    "\n",
    "call_eval_pipeline(dict_1k_sparse, flag_output = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382656d",
   "metadata": {},
   "source": [
    "## Improvements to Sparse Matrix Factorization\n",
    "\n",
    "We will make some improvements to our sparse matrix factorization. The main improvement we can make in our code is that we can try to vectorize as many computations as possible, and take out as many computations out of the loops as we can.\n",
    "\n",
    "### Matrix Factorization 3: Vectorize Sparse Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65dffe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is different from the previous one because we do not need to traverse each array separately\n",
    "# we isolate and extract all of the nonzero entries from the matrix in linear time (at worst O(n) for ground truth mxn)\n",
    "# and then we only loop through epochs and the number of latent features\n",
    "\n",
    "# UPDATE: This will also be vectorized, we will make notes of the changes in comments below\n",
    "\n",
    "# ARGUMENTS\n",
    "# A - ground truth matrix (sparse numpy array)\n",
    "# U - product 1\n",
    "# V - product 2\n",
    "# number of features\n",
    "# epochs - number of epochs to train\n",
    "# lambda - learning rate\n",
    "# beta - regularization parameter\n",
    "\n",
    "def matrix_factorization_sparse_vectorized(A, U, V, k, steps, lmbda=0.0002, beta = 0.02, print_divs=10):\n",
    "    # transpose V to make matrix multiplication work\n",
    "    V = V.T\n",
    "    \n",
    "    # convert A, a sparse numpy array, into a sparse scipy matrix object\n",
    "    sparse_A = csr_matrix(A)\n",
    "    \n",
    "    # start epochs\n",
    "    for epoch in range(steps):\n",
    "        # initialize total loss\n",
    "        e = 0\n",
    "        # get matrix of losses ahead of time and only look at the nonzero entries of A\n",
    "        A_hat = np.matmul(U,V)\n",
    "        err = A - A_hat\n",
    "        \n",
    "        # parse through nonzero indices of A\n",
    "        for i, j in zip(sparse_A.nonzero()[0], sparse_A.nonzero()[1]):\n",
    "            # component-wise error\n",
    "            eij = err[i,j]\n",
    "            # total error\n",
    "            e += eij**2\n",
    "                        \n",
    "            for K in range(k):\n",
    "                # gradient descent step for each entry, with regularization term, vectorized\n",
    "                U[i,K], V[K,j] = U[i,K] + lmbda * (2 * eij * V[K,j] - beta*U[i,K]), V[K,j] + lmbda * (2 * eij * U[i,K] - beta*V[K,j])\n",
    "                e += 0.5 * beta * (U[i][K]**2 + V[K][j]**2)\n",
    "        \n",
    "        # give print output of training (print only every 10 epochs)\n",
    "        if epoch % print_steps == 0:\n",
    "            print(f'Epoch {epoch + 1} : Total Loss {e:.3f}')\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return U, V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1463f4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      " [[0.51729788 0.9469626 ]\n",
      " [0.76545976 0.28239584]\n",
      " [0.22104536 0.68622209]\n",
      " ...\n",
      " [0.81725319 0.69902473]\n",
      " [0.63405118 0.1848986 ]\n",
      " [0.65351892 0.97255327]]\n",
      "V is:\n",
      " [[0.17041509 0.77499716]\n",
      " [0.89300173 0.66987814]\n",
      " [0.960109   0.99091208]\n",
      " ...\n",
      " [0.87679857 0.42425804]\n",
      " [0.12411982 0.29636888]\n",
      " [0.73663472 0.99557962]]\n",
      "Epoch 1 : Total Loss 71020.908\n",
      "Epoch 11 : Total Loss 66894.562\n",
      "Epoch 21 : Total Loss 62403.561\n",
      "Epoch 31 : Total Loss 57626.281\n",
      "Epoch 41 : Total Loss 52676.891\n",
      "Epoch 51 : Total Loss 47697.317\n",
      "Epoch 61 : Total Loss 42842.240\n",
      "Epoch 71 : Total Loss 38259.675\n",
      "Epoch 81 : Total Loss 34071.817\n",
      "Epoch 91 : Total Loss 30361.167\n",
      "Epoch 101 : Total Loss 27165.197\n",
      "Epoch 111 : Total Loss 24479.792\n",
      "Epoch 121 : Total Loss 22268.942\n",
      "Epoch 131 : Total Loss 20476.971\n",
      "Epoch 141 : Total Loss 19039.958\n",
      "Epoch 151 : Total Loss 17894.423\n",
      "Epoch 161 : Total Loss 16982.711\n",
      "Epoch 171 : Total Loss 16255.546\n",
      "Epoch 181 : Total Loss 15672.558\n",
      "Epoch 191 : Total Loss 15201.633\n",
      "Epoch 201 : Total Loss 14817.727\n",
      "Epoch 211 : Total Loss 14501.545\n",
      "Epoch 221 : Total Loss 14238.314\n",
      "Epoch 231 : Total Loss 14016.744\n",
      "Epoch 241 : Total Loss 13828.188\n",
      "Epoch 251 : Total Loss 13665.996\n",
      "Epoch 261 : Total Loss 13525.023\n",
      "Epoch 271 : Total Loss 13401.254\n",
      "Epoch 281 : Total Loss 13291.536\n",
      "Epoch 291 : Total Loss 13193.369\n",
      "Epoch 301 : Total Loss 13104.754\n",
      "Epoch 311 : Total Loss 13024.082\n",
      "Epoch 321 : Total Loss 12950.045\n",
      "Epoch 331 : Total Loss 12881.577\n",
      "Epoch 341 : Total Loss 12817.795\n",
      "Epoch 351 : Total Loss 12757.971\n",
      "Epoch 361 : Total Loss 12701.498\n",
      "Epoch 371 : Total Loss 12647.867\n",
      "Epoch 381 : Total Loss 12596.647\n",
      "Epoch 391 : Total Loss 12547.477\n",
      "Epoch 401 : Total Loss 12500.049\n",
      "Epoch 411 : Total Loss 12454.100\n",
      "Epoch 421 : Total Loss 12409.407\n",
      "Epoch 431 : Total Loss 12365.776\n",
      "Epoch 441 : Total Loss 12323.043\n",
      "Epoch 451 : Total Loss 12281.067\n",
      "Epoch 461 : Total Loss 12239.726\n",
      "Epoch 471 : Total Loss 12198.914\n",
      "Epoch 481 : Total Loss 12158.543\n",
      "Epoch 491 : Total Loss 12118.535\n",
      "Epoch 501 : Total Loss 12078.824\n",
      "Epoch 511 : Total Loss 12039.355\n",
      "Epoch 521 : Total Loss 12000.081\n",
      "Epoch 531 : Total Loss 11960.961\n",
      "Epoch 541 : Total Loss 11921.963\n",
      "Epoch 551 : Total Loss 11883.061\n",
      "Epoch 561 : Total Loss 11844.233\n",
      "Epoch 571 : Total Loss 11805.462\n",
      "Epoch 581 : Total Loss 11766.737\n",
      "Epoch 591 : Total Loss 11728.049\n",
      "Epoch 601 : Total Loss 11689.393\n",
      "Epoch 611 : Total Loss 11650.767\n",
      "Epoch 621 : Total Loss 11612.173\n",
      "Epoch 631 : Total Loss 11573.613\n",
      "Epoch 641 : Total Loss 11535.093\n",
      "Epoch 651 : Total Loss 11496.620\n",
      "Epoch 661 : Total Loss 11458.203\n",
      "Epoch 671 : Total Loss 11419.853\n",
      "Epoch 681 : Total Loss 11381.581\n",
      "Epoch 691 : Total Loss 11343.401\n",
      "Epoch 701 : Total Loss 11305.325\n",
      "Epoch 711 : Total Loss 11267.369\n",
      "Epoch 721 : Total Loss 11229.546\n",
      "Epoch 731 : Total Loss 11191.873\n",
      "Epoch 741 : Total Loss 11154.364\n",
      "Epoch 751 : Total Loss 11117.035\n",
      "Epoch 761 : Total Loss 11079.903\n",
      "Epoch 771 : Total Loss 11042.982\n",
      "Epoch 781 : Total Loss 11006.288\n",
      "Epoch 791 : Total Loss 10969.837\n",
      "Epoch 801 : Total Loss 10933.642\n",
      "Epoch 811 : Total Loss 10897.718\n",
      "Epoch 821 : Total Loss 10862.080\n",
      "Epoch 831 : Total Loss 10826.739\n",
      "Epoch 841 : Total Loss 10791.710\n",
      "Epoch 851 : Total Loss 10757.003\n",
      "Epoch 861 : Total Loss 10722.630\n",
      "Epoch 871 : Total Loss 10688.602\n",
      "Epoch 881 : Total Loss 10654.928\n",
      "Epoch 891 : Total Loss 10621.617\n",
      "Epoch 901 : Total Loss 10588.678\n",
      "Epoch 911 : Total Loss 10556.117\n",
      "Epoch 921 : Total Loss 10523.941\n",
      "Epoch 931 : Total Loss 10492.156\n",
      "Epoch 941 : Total Loss 10460.767\n",
      "Epoch 951 : Total Loss 10429.778\n",
      "Epoch 961 : Total Loss 10399.192\n",
      "Epoch 971 : Total Loss 10369.011\n",
      "Epoch 981 : Total Loss 10339.239\n",
      "Epoch 991 : Total Loss 10309.876\n",
      "Epoch 1001 : Total Loss 10280.922\n",
      "Epoch 1011 : Total Loss 10252.377\n",
      "Epoch 1021 : Total Loss 10224.241\n",
      "Epoch 1031 : Total Loss 10196.512\n",
      "Epoch 1041 : Total Loss 10169.188\n",
      "Epoch 1051 : Total Loss 10142.268\n",
      "Epoch 1061 : Total Loss 10115.747\n",
      "Epoch 1071 : Total Loss 10089.624\n",
      "Epoch 1081 : Total Loss 10063.893\n",
      "Epoch 1091 : Total Loss 10038.552\n",
      "Epoch 1101 : Total Loss 10013.596\n",
      "Epoch 1111 : Total Loss 9989.019\n",
      "Epoch 1121 : Total Loss 9964.818\n",
      "Epoch 1131 : Total Loss 9940.987\n",
      "Epoch 1141 : Total Loss 9917.520\n",
      "Epoch 1151 : Total Loss 9894.413\n",
      "Epoch 1161 : Total Loss 9871.659\n",
      "Epoch 1171 : Total Loss 9849.253\n",
      "Epoch 1181 : Total Loss 9827.188\n",
      "Epoch 1191 : Total Loss 9805.459\n",
      "Epoch 1201 : Total Loss 9784.060\n",
      "Epoch 1211 : Total Loss 9762.985\n",
      "Epoch 1221 : Total Loss 9742.228\n",
      "Epoch 1231 : Total Loss 9721.782\n",
      "Epoch 1241 : Total Loss 9701.643\n",
      "Epoch 1251 : Total Loss 9681.803\n",
      "Epoch 1261 : Total Loss 9662.256\n",
      "Epoch 1271 : Total Loss 9642.998\n",
      "Epoch 1281 : Total Loss 9624.022\n",
      "Epoch 1291 : Total Loss 9605.323\n",
      "Epoch 1301 : Total Loss 9586.894\n",
      "Epoch 1311 : Total Loss 9568.731\n",
      "Epoch 1321 : Total Loss 9550.828\n",
      "Epoch 1331 : Total Loss 9533.179\n",
      "Epoch 1341 : Total Loss 9515.780\n",
      "Epoch 1351 : Total Loss 9498.625\n",
      "Epoch 1361 : Total Loss 9481.709\n",
      "Epoch 1371 : Total Loss 9465.028\n",
      "Epoch 1381 : Total Loss 9448.576\n",
      "Epoch 1391 : Total Loss 9432.349\n",
      "Epoch 1401 : Total Loss 9416.343\n",
      "Epoch 1411 : Total Loss 9400.552\n",
      "Epoch 1421 : Total Loss 9384.974\n",
      "Epoch 1431 : Total Loss 9369.602\n",
      "Epoch 1441 : Total Loss 9354.435\n",
      "Epoch 1451 : Total Loss 9339.466\n",
      "Epoch 1461 : Total Loss 9324.693\n",
      "Epoch 1471 : Total Loss 9310.111\n",
      "Epoch 1481 : Total Loss 9295.718\n",
      "Epoch 1491 : Total Loss 9281.509\n",
      "Epoch 1501 : Total Loss 9267.481\n",
      "Epoch 1511 : Total Loss 9253.630\n",
      "Epoch 1521 : Total Loss 9239.953\n",
      "Epoch 1531 : Total Loss 9226.448\n",
      "Epoch 1541 : Total Loss 9213.110\n",
      "Epoch 1551 : Total Loss 9199.937\n",
      "Epoch 1561 : Total Loss 9186.926\n",
      "Epoch 1571 : Total Loss 9174.074\n",
      "Epoch 1581 : Total Loss 9161.377\n",
      "Epoch 1591 : Total Loss 9148.834\n",
      "Epoch 1601 : Total Loss 9136.442\n",
      "Epoch 1611 : Total Loss 9124.198\n",
      "Epoch 1621 : Total Loss 9112.099\n",
      "Epoch 1631 : Total Loss 9100.143\n",
      "Epoch 1641 : Total Loss 9088.328\n",
      "Epoch 1651 : Total Loss 9076.651\n",
      "Epoch 1661 : Total Loss 9065.109\n",
      "Epoch 1671 : Total Loss 9053.702\n",
      "Epoch 1681 : Total Loss 9042.425\n",
      "Epoch 1691 : Total Loss 9031.278\n",
      "Epoch 1701 : Total Loss 9020.258\n",
      "Epoch 1711 : Total Loss 9009.363\n",
      "Epoch 1721 : Total Loss 8998.591\n",
      "Epoch 1731 : Total Loss 8987.941\n",
      "Epoch 1741 : Total Loss 8977.409\n",
      "Epoch 1751 : Total Loss 8966.994\n",
      "Epoch 1761 : Total Loss 8956.695\n",
      "Epoch 1771 : Total Loss 8946.510\n",
      "Epoch 1781 : Total Loss 8936.437\n",
      "Epoch 1791 : Total Loss 8926.473\n",
      "Epoch 1801 : Total Loss 8916.618\n",
      "Epoch 1811 : Total Loss 8906.870\n",
      "Epoch 1821 : Total Loss 8897.227\n",
      "Epoch 1831 : Total Loss 8887.687\n",
      "Epoch 1841 : Total Loss 8878.250\n",
      "Epoch 1851 : Total Loss 8868.913\n",
      "Epoch 1861 : Total Loss 8859.675\n",
      "Epoch 1871 : Total Loss 8850.534\n",
      "Epoch 1881 : Total Loss 8841.490\n",
      "Epoch 1891 : Total Loss 8832.540\n",
      "Epoch 1901 : Total Loss 8823.684\n",
      "Epoch 1911 : Total Loss 8814.919\n",
      "Epoch 1921 : Total Loss 8806.245\n",
      "Epoch 1931 : Total Loss 8797.661\n",
      "Epoch 1941 : Total Loss 8789.164\n",
      "Epoch 1951 : Total Loss 8780.755\n",
      "Epoch 1961 : Total Loss 8772.430\n",
      "Epoch 1971 : Total Loss 8764.191\n",
      "Epoch 1981 : Total Loss 8756.034\n",
      "Epoch 1991 : Total Loss 8747.960\n",
      "Epoch 2001 : Total Loss 8739.966\n",
      "Epoch 2011 : Total Loss 8732.052\n",
      "Epoch 2021 : Total Loss 8724.217\n",
      "Epoch 2031 : Total Loss 8716.460\n",
      "Epoch 2041 : Total Loss 8708.779\n",
      "Epoch 2051 : Total Loss 8701.173\n",
      "Epoch 2061 : Total Loss 8693.642\n",
      "Epoch 2071 : Total Loss 8686.185\n",
      "Epoch 2081 : Total Loss 8678.799\n",
      "Epoch 2091 : Total Loss 8671.486\n",
      "Epoch 2101 : Total Loss 8664.243\n",
      "Epoch 2111 : Total Loss 8657.069\n",
      "Epoch 2121 : Total Loss 8649.964\n",
      "Epoch 2131 : Total Loss 8642.927\n",
      "Epoch 2141 : Total Loss 8635.957\n",
      "Epoch 2151 : Total Loss 8629.053\n",
      "Epoch 2161 : Total Loss 8622.214\n",
      "Epoch 2171 : Total Loss 8615.438\n",
      "Epoch 2181 : Total Loss 8608.727\n",
      "Epoch 2191 : Total Loss 8602.077\n",
      "Epoch 2201 : Total Loss 8595.490\n",
      "Epoch 2211 : Total Loss 8588.963\n",
      "Epoch 2221 : Total Loss 8582.496\n",
      "Epoch 2231 : Total Loss 8576.089\n",
      "Epoch 2241 : Total Loss 8569.740\n",
      "Epoch 2251 : Total Loss 8563.448\n",
      "Epoch 2261 : Total Loss 8557.214\n",
      "Epoch 2271 : Total Loss 8551.035\n",
      "Epoch 2281 : Total Loss 8544.912\n",
      "Epoch 2291 : Total Loss 8538.844\n",
      "Epoch 2301 : Total Loss 8532.829\n",
      "Epoch 2311 : Total Loss 8526.868\n",
      "Epoch 2321 : Total Loss 8520.959\n",
      "Epoch 2331 : Total Loss 8515.101\n",
      "Epoch 2341 : Total Loss 8509.295\n",
      "Epoch 2351 : Total Loss 8503.539\n",
      "Epoch 2361 : Total Loss 8497.832\n",
      "Epoch 2371 : Total Loss 8492.175\n",
      "Epoch 2381 : Total Loss 8486.566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2391 : Total Loss 8481.004\n",
      "Epoch 2401 : Total Loss 8475.490\n",
      "Epoch 2411 : Total Loss 8470.022\n",
      "Epoch 2421 : Total Loss 8464.599\n",
      "Epoch 2431 : Total Loss 8459.221\n",
      "Epoch 2441 : Total Loss 8453.888\n",
      "Epoch 2451 : Total Loss 8448.599\n",
      "Epoch 2461 : Total Loss 8443.353\n",
      "Epoch 2471 : Total Loss 8438.150\n",
      "Epoch 2481 : Total Loss 8432.989\n",
      "Epoch 2491 : Total Loss 8427.869\n",
      "Epoch 2501 : Total Loss 8422.790\n",
      "Epoch 2511 : Total Loss 8417.751\n",
      "Epoch 2521 : Total Loss 8412.753\n",
      "Epoch 2531 : Total Loss 8407.793\n",
      "Epoch 2541 : Total Loss 8402.873\n",
      "Epoch 2551 : Total Loss 8397.990\n",
      "Epoch 2561 : Total Loss 8393.146\n",
      "Epoch 2571 : Total Loss 8388.338\n",
      "Epoch 2581 : Total Loss 8383.567\n",
      "Epoch 2591 : Total Loss 8378.833\n",
      "Epoch 2601 : Total Loss 8374.134\n",
      "Epoch 2611 : Total Loss 8369.471\n",
      "Epoch 2621 : Total Loss 8364.843\n",
      "Epoch 2631 : Total Loss 8360.249\n",
      "Epoch 2641 : Total Loss 8355.689\n",
      "Epoch 2651 : Total Loss 8351.162\n",
      "Epoch 2661 : Total Loss 8346.669\n",
      "Epoch 2671 : Total Loss 8342.209\n",
      "Epoch 2681 : Total Loss 8337.781\n",
      "Epoch 2691 : Total Loss 8333.385\n",
      "Epoch 2701 : Total Loss 8329.020\n",
      "Epoch 2711 : Total Loss 8324.687\n",
      "Epoch 2721 : Total Loss 8320.385\n",
      "Epoch 2731 : Total Loss 8316.113\n",
      "Epoch 2741 : Total Loss 8311.872\n",
      "Epoch 2751 : Total Loss 8307.661\n",
      "Epoch 2761 : Total Loss 8303.479\n",
      "Epoch 2771 : Total Loss 8299.326\n",
      "Epoch 2781 : Total Loss 8295.203\n",
      "Epoch 2791 : Total Loss 8291.108\n",
      "Epoch 2801 : Total Loss 8287.041\n",
      "Epoch 2811 : Total Loss 8283.003\n",
      "Epoch 2821 : Total Loss 8278.992\n",
      "Epoch 2831 : Total Loss 8275.009\n",
      "Epoch 2841 : Total Loss 8271.053\n",
      "Epoch 2851 : Total Loss 8267.125\n",
      "Epoch 2861 : Total Loss 8263.223\n",
      "Epoch 2871 : Total Loss 8259.348\n",
      "Epoch 2881 : Total Loss 8255.499\n",
      "Epoch 2891 : Total Loss 8251.676\n",
      "Epoch 2901 : Total Loss 8247.879\n",
      "Epoch 2911 : Total Loss 8244.107\n",
      "Epoch 2921 : Total Loss 8240.361\n",
      "Epoch 2931 : Total Loss 8236.641\n",
      "Epoch 2941 : Total Loss 8232.945\n",
      "Epoch 2951 : Total Loss 8229.274\n",
      "Epoch 2961 : Total Loss 8225.628\n",
      "Epoch 2971 : Total Loss 8222.006\n",
      "Epoch 2981 : Total Loss 8218.408\n",
      "Epoch 2991 : Total Loss 8214.834\n",
      "Epoch 3001 : Total Loss 8211.284\n",
      "Epoch 3011 : Total Loss 8207.758\n",
      "Epoch 3021 : Total Loss 8204.254\n",
      "Epoch 3031 : Total Loss 8200.775\n",
      "Epoch 3041 : Total Loss 8197.318\n",
      "Epoch 3051 : Total Loss 8193.884\n",
      "Epoch 3061 : Total Loss 8190.472\n",
      "Epoch 3071 : Total Loss 8187.083\n",
      "Epoch 3081 : Total Loss 8183.716\n",
      "Epoch 3091 : Total Loss 8180.371\n",
      "Epoch 3101 : Total Loss 8177.048\n",
      "Epoch 3111 : Total Loss 8173.747\n",
      "Epoch 3121 : Total Loss 8170.467\n",
      "Epoch 3131 : Total Loss 8167.208\n",
      "Epoch 3141 : Total Loss 8163.971\n",
      "Epoch 3151 : Total Loss 8160.754\n",
      "Epoch 3161 : Total Loss 8157.558\n",
      "Epoch 3171 : Total Loss 8154.382\n",
      "Epoch 3181 : Total Loss 8151.227\n",
      "Epoch 3191 : Total Loss 8148.091\n",
      "Epoch 3201 : Total Loss 8144.976\n",
      "Epoch 3211 : Total Loss 8141.880\n",
      "Epoch 3221 : Total Loss 8138.804\n",
      "Epoch 3231 : Total Loss 8135.746\n",
      "Epoch 3241 : Total Loss 8132.708\n",
      "Epoch 3251 : Total Loss 8129.689\n",
      "Epoch 3261 : Total Loss 8126.688\n",
      "Epoch 3271 : Total Loss 8123.705\n",
      "Epoch 3281 : Total Loss 8120.741\n",
      "Epoch 3291 : Total Loss 8117.794\n",
      "Epoch 3301 : Total Loss 8114.866\n",
      "Epoch 3311 : Total Loss 8111.955\n",
      "Epoch 3321 : Total Loss 8109.061\n",
      "Epoch 3331 : Total Loss 8106.184\n",
      "Epoch 3341 : Total Loss 8103.324\n",
      "Epoch 3351 : Total Loss 8100.481\n",
      "Epoch 3361 : Total Loss 8097.654\n",
      "Epoch 3371 : Total Loss 8094.843\n",
      "Epoch 3381 : Total Loss 8092.049\n",
      "Epoch 3391 : Total Loss 8089.270\n",
      "Epoch 3401 : Total Loss 8086.507\n",
      "Epoch 3411 : Total Loss 8083.759\n",
      "Epoch 3421 : Total Loss 8081.026\n",
      "Epoch 3431 : Total Loss 8078.308\n",
      "Epoch 3441 : Total Loss 8075.604\n",
      "Epoch 3451 : Total Loss 8072.916\n",
      "Epoch 3461 : Total Loss 8070.241\n",
      "Epoch 3471 : Total Loss 8067.581\n",
      "Epoch 3481 : Total Loss 8064.934\n",
      "Epoch 3491 : Total Loss 8062.301\n",
      "Epoch 3501 : Total Loss 8059.681\n",
      "Epoch 3511 : Total Loss 8057.075\n",
      "Epoch 3521 : Total Loss 8054.481\n",
      "Epoch 3531 : Total Loss 8051.901\n",
      "Epoch 3541 : Total Loss 8049.333\n",
      "Epoch 3551 : Total Loss 8046.777\n",
      "Epoch 3561 : Total Loss 8044.234\n",
      "Epoch 3571 : Total Loss 8041.702\n",
      "Epoch 3581 : Total Loss 8039.183\n",
      "Epoch 3591 : Total Loss 8036.675\n",
      "Epoch 3601 : Total Loss 8034.178\n",
      "Epoch 3611 : Total Loss 8031.693\n",
      "Epoch 3621 : Total Loss 8029.218\n",
      "Epoch 3631 : Total Loss 8026.755\n",
      "Epoch 3641 : Total Loss 8024.302\n",
      "Epoch 3651 : Total Loss 8021.860\n",
      "Epoch 3661 : Total Loss 8019.428\n",
      "Epoch 3671 : Total Loss 8017.006\n",
      "Epoch 3681 : Total Loss 8014.594\n",
      "Epoch 3691 : Total Loss 8012.192\n",
      "Epoch 3701 : Total Loss 8009.800\n",
      "Epoch 3711 : Total Loss 8007.417\n",
      "Epoch 3721 : Total Loss 8005.043\n",
      "Epoch 3731 : Total Loss 8002.679\n",
      "Epoch 3741 : Total Loss 8000.323\n",
      "Epoch 3751 : Total Loss 7997.977\n",
      "Epoch 3761 : Total Loss 7995.639\n",
      "Epoch 3771 : Total Loss 7993.310\n",
      "Epoch 3781 : Total Loss 7990.989\n",
      "Epoch 3791 : Total Loss 7988.676\n",
      "Epoch 3801 : Total Loss 7986.372\n",
      "Epoch 3811 : Total Loss 7984.075\n",
      "Epoch 3821 : Total Loss 7981.786\n",
      "Epoch 3831 : Total Loss 7979.505\n",
      "Epoch 3841 : Total Loss 7977.232\n",
      "Epoch 3851 : Total Loss 7974.966\n",
      "Epoch 3861 : Total Loss 7972.708\n",
      "Epoch 3871 : Total Loss 7970.456\n",
      "Epoch 3881 : Total Loss 7968.212\n",
      "Epoch 3891 : Total Loss 7965.975\n",
      "Epoch 3901 : Total Loss 7963.745\n",
      "Epoch 3911 : Total Loss 7961.521\n",
      "Epoch 3921 : Total Loss 7959.304\n",
      "Epoch 3931 : Total Loss 7957.094\n",
      "Epoch 3941 : Total Loss 7954.890\n",
      "Epoch 3951 : Total Loss 7952.692\n",
      "Epoch 3961 : Total Loss 7950.501\n",
      "Epoch 3971 : Total Loss 7948.316\n",
      "Epoch 3981 : Total Loss 7946.137\n",
      "Epoch 3991 : Total Loss 7943.964\n",
      "Epoch 4001 : Total Loss 7941.797\n",
      "Epoch 4011 : Total Loss 7939.636\n",
      "Epoch 4021 : Total Loss 7937.481\n",
      "Epoch 4031 : Total Loss 7935.331\n",
      "Epoch 4041 : Total Loss 7933.187\n",
      "Epoch 4051 : Total Loss 7931.048\n",
      "Epoch 4061 : Total Loss 7928.915\n",
      "Epoch 4071 : Total Loss 7926.787\n",
      "Epoch 4081 : Total Loss 7924.665\n",
      "Epoch 4091 : Total Loss 7922.548\n",
      "Epoch 4101 : Total Loss 7920.436\n",
      "Epoch 4111 : Total Loss 7918.330\n",
      "Epoch 4121 : Total Loss 7916.228\n",
      "Epoch 4131 : Total Loss 7914.132\n",
      "Epoch 4141 : Total Loss 7912.041\n",
      "Epoch 4151 : Total Loss 7909.954\n",
      "Epoch 4161 : Total Loss 7907.873\n",
      "Epoch 4171 : Total Loss 7905.797\n",
      "Epoch 4181 : Total Loss 7903.725\n",
      "Epoch 4191 : Total Loss 7901.659\n",
      "Epoch 4201 : Total Loss 7899.597\n",
      "Epoch 4211 : Total Loss 7897.540\n",
      "Epoch 4221 : Total Loss 7895.488\n",
      "Epoch 4231 : Total Loss 7893.441\n",
      "Epoch 4241 : Total Loss 7891.398\n",
      "Epoch 4251 : Total Loss 7889.360\n",
      "Epoch 4261 : Total Loss 7887.327\n",
      "Epoch 4271 : Total Loss 7885.299\n",
      "Epoch 4281 : Total Loss 7883.275\n",
      "Epoch 4291 : Total Loss 7881.257\n",
      "Epoch 4301 : Total Loss 7879.242\n",
      "Epoch 4311 : Total Loss 7877.233\n",
      "Epoch 4321 : Total Loss 7875.228\n",
      "Epoch 4331 : Total Loss 7873.228\n",
      "Epoch 4341 : Total Loss 7871.233\n",
      "Epoch 4351 : Total Loss 7869.243\n",
      "Epoch 4361 : Total Loss 7867.257\n",
      "Epoch 4371 : Total Loss 7865.276\n",
      "Epoch 4381 : Total Loss 7863.300\n",
      "Epoch 4391 : Total Loss 7861.328\n",
      "Epoch 4401 : Total Loss 7859.362\n",
      "Epoch 4411 : Total Loss 7857.400\n",
      "Epoch 4421 : Total Loss 7855.444\n",
      "Epoch 4431 : Total Loss 7853.492\n",
      "Epoch 4441 : Total Loss 7851.545\n",
      "Epoch 4451 : Total Loss 7849.603\n",
      "Epoch 4461 : Total Loss 7847.666\n",
      "Epoch 4471 : Total Loss 7845.734\n",
      "Epoch 4481 : Total Loss 7843.807\n",
      "Epoch 4491 : Total Loss 7841.885\n",
      "Epoch 4501 : Total Loss 7839.968\n",
      "Epoch 4511 : Total Loss 7838.056\n",
      "Epoch 4521 : Total Loss 7836.150\n",
      "Epoch 4531 : Total Loss 7834.248\n",
      "Epoch 4541 : Total Loss 7832.352\n",
      "Epoch 4551 : Total Loss 7830.462\n",
      "Epoch 4561 : Total Loss 7828.576\n",
      "Epoch 4571 : Total Loss 7826.696\n",
      "Epoch 4581 : Total Loss 7824.821\n",
      "Epoch 4591 : Total Loss 7822.952\n",
      "Epoch 4601 : Total Loss 7821.088\n",
      "Epoch 4611 : Total Loss 7819.230\n",
      "Epoch 4621 : Total Loss 7817.377\n",
      "Epoch 4631 : Total Loss 7815.530\n",
      "Epoch 4641 : Total Loss 7813.689\n",
      "Epoch 4651 : Total Loss 7811.853\n",
      "Epoch 4661 : Total Loss 7810.022\n",
      "Epoch 4671 : Total Loss 7808.198\n",
      "Epoch 4681 : Total Loss 7806.379\n",
      "Epoch 4691 : Total Loss 7804.566\n",
      "Epoch 4701 : Total Loss 7802.759\n",
      "Epoch 4711 : Total Loss 7800.958\n",
      "Epoch 4721 : Total Loss 7799.163\n",
      "Epoch 4731 : Total Loss 7797.373\n",
      "Epoch 4741 : Total Loss 7795.590\n",
      "Epoch 4751 : Total Loss 7793.812\n",
      "Epoch 4761 : Total Loss 7792.041\n",
      "Epoch 4771 : Total Loss 7790.276\n",
      "Epoch 4781 : Total Loss 7788.516\n",
      "Epoch 4791 : Total Loss 7786.763\n",
      "Epoch 4801 : Total Loss 7785.016\n",
      "Epoch 4811 : Total Loss 7783.275\n",
      "Epoch 4821 : Total Loss 7781.540\n",
      "Epoch 4831 : Total Loss 7779.812\n",
      "Epoch 4841 : Total Loss 7778.090\n",
      "Epoch 4851 : Total Loss 7776.373\n",
      "Epoch 4861 : Total Loss 7774.664\n",
      "Epoch 4871 : Total Loss 7772.960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4881 : Total Loss 7771.263\n",
      "Epoch 4891 : Total Loss 7769.572\n",
      "Epoch 4901 : Total Loss 7767.887\n",
      "Epoch 4911 : Total Loss 7766.209\n",
      "Epoch 4921 : Total Loss 7764.537\n",
      "Epoch 4931 : Total Loss 7762.871\n",
      "Epoch 4941 : Total Loss 7761.212\n",
      "Epoch 4951 : Total Loss 7759.559\n",
      "Epoch 4961 : Total Loss 7757.912\n",
      "Epoch 4971 : Total Loss 7756.272\n",
      "Epoch 4981 : Total Loss 7754.638\n",
      "Epoch 4991 : Total Loss 7753.011\n",
      "The Mean Squared Error is 0.845015220602158.\n",
      "The Root Mean Squared Error is 0.9192470944213846.\n",
      "\n",
      "The Runtime of this algorithm of this 1000 x 1000, 99.166% sparse matrix, is 922.500383000006 seconds.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "\n",
    "m, n = arr_1k.shape[0], arr_1k.shape[1]\n",
    "\n",
    "user_features = 2\n",
    "\n",
    "U_4 = np.random.rand(m, user_features)\n",
    "V_4 = np.random.rand(n, user_features)\n",
    "\n",
    "# check entries of matrices\n",
    "\n",
    "print('U is:\\n', U_4)\n",
    "print('V is:\\n', V_4)\n",
    "\n",
    "dict_1k_sparse = evaluation_pipeline(arr_1k, U_4, V_4, 5000, 2, matrix_factorization_sparse_vectorized, print_divs=100)\n",
    "\n",
    "call_eval_pipeline(dict_1k_sparse, flag_output = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de941c31",
   "metadata": {},
   "source": [
    "**Note: The runtime of the algorithm is very inconsistent, this particular algorithm has clocked as low as 5 minutes to train and evaluate fully. However, our vectorized operation is, for a fact, much quicker than the unvectorized version, nearly 2.5 minutes quicker, in fact.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52843964",
   "metadata": {},
   "source": [
    "### Complexity Analysis\n",
    "\n",
    "The time complexity of the matrix factorization methods are not entirely clear as it depends on the cases that we are looking at.\n",
    "\n",
    "Let us lay down some definitions:\n",
    "\n",
    "**Sparsity (s)**:\n",
    "\n",
    "$s$ is a parameter denoting the proportion of 0's in a matrix compared to the total number of entries. For an $m\\times n$ interaction matrix, $A$, we see that the **sparsity of $A$** is:\n",
    "\n",
    "$$s = \\frac{\\mathrm{Number \\; of \\; 0's}}{nm}$$\n",
    "\n",
    "**Density (d)**:\n",
    "\n",
    "Likewise, $d$ is a parameter denoting the proportion of nonzero's compared to the total number of entries. For $A$ indicated above, the **density of $A$** is:\n",
    "\n",
    "$$d = \\frac{\\mathrm{Number \\;of\\;nonzero's}}{nm}$$\n",
    "\n",
    "Sparsity and density are related by the following expression:\n",
    "\n",
    "$$s = 1 - d$$\n",
    "\n",
    "(rearrange to see density in terms of sparsity)\n",
    "\n",
    "Now onto our analysis:\n",
    "\n",
    "1) **Case 1: Dense Matrices**\n",
    "\n",
    "There is no clear definition, but for our purposes, we consider a **dense matrix**, $A$ (assume $m\\times n$) as one in which there is a **density** of at least $\\frac{nm}{2}$, where the nonzero entries are distributed with little bias across all rows and columns.\n",
    "\n",
    "Let $k$ denote the **number of latent features**, $n$ denote the **number of columns**, and $m$ the **number of rows**. Assume that our number of latent features is *fixed*, and is far smaller than the dimensions of our interaction table.  Assume that our training loop persists for $e$ **epochs** and that $e$ is *fixed* (in practice, the number of epochs can get large, but, in principle, it is fixed before we run our training loop).\n",
    "\n",
    "Let $N := \\max(m,n)$. For a general $m\\times n$ ground truth, interaction matrix, $A$, the *regular matrix factorization* implementation has a big $\\mathcal{O}$ time complexity of $\\mathcal{O}(ekmn)$. In our case, since the latent features and epochs are fixed in size (the dimensions of the matrix are features of our data and model), we can consider $k << N$, $e << N$, and $m$ and $n$ to be arbitrarily large, and we see that the worst-case runtime is actually just $\\mathcal{O}(N^{2})$.\n",
    "\n",
    "Now look at the *sparse matrix factorization* implementation, note that this is no different than the *regular matrix factorization* as when we isolate the indices corresponding to nonzero entries, we, essentially, must traverse at least $0.5nm$ entries in $A$. Therefore, the time complexity of the sparse MF is just $\\mathcal{O}(N^{2})$.\n",
    "\n",
    "2) **Case 2: Sparse Matrices**\n",
    "\n",
    "We consider a **sparse matrix** as one in which the **sparsity** is **at least** $1 - 1/\\min(n,m)$ (i.e. the number of nonzero entries is approximately the number of rows or the number of columns). With notation above, we see that the worst-case runtime for the *sparse matrix factorization* is $\\mathcal{O}(N)$, and the worst-case runtime for the *regular matrix factorization* is $\\mathcal{O}(N^{2})$, and to see this, just recognize that $e$ and $k$ are fixed as above. \n",
    "\n",
    "In our *sparse matrix factorization*, since **we only traverse nonzero entries of the interaction table**, we are only running our training loop on at most $\\max(n,m)$ entries. Therefore, since $k$ and $e$ are fixed, we get a time complexity of $\\sim \\mathcal{O}(N)$.\n",
    "\n",
    "In our *regular matrix factorization*, since **we must traverse ALL entries of the interaction table**, we are running our training loop for $nm$ entries as we **must traverse the zero entries in order to check that they are zero**. Therefore, we have a time complexity of $\\sim \\mathcal{O}(N^{2})$.\n",
    "\n",
    "### Why is Sparsity Important?\n",
    "\n",
    "Sparsity is an important assumption about our input data that is actually true for the business problem. Oftentimes, when we are trying to analyze interactions, we will have many entries missing. Most people either do not leave ratings or the ratings are often not present due to some preprocessing method (like one-hot encoding). A sparse matrix is a very realistic and conservative assumption to make when we are trying to generate recommendations.\n",
    "\n",
    "Not only does sparsity optimize our method above, but it is an assumption that often holds in the business problem where matrix factorization is relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommenders",
   "language": "python",
   "name": "recommenders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
